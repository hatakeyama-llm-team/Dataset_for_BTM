{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mecabで分かち書きしながら、tokenizeする\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mecabによる分かち書き"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#コーパスの読み込み\n",
    "target_path=\"/data/hatakeyama/python/llm_corpus/corpus_scale_200.jsonl\"\n",
    "lines=[]\n",
    "with open(target_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        lines.append(json.loads(line)[\"text\"])\n",
    "\n",
    "#一時ファイルに書き込み\n",
    "with open(\"../data/corpus_text.txt\",\"w\") as f:\n",
    "    f.write(\"\")\n",
    "with open(\"../data/corpus_text.txt\",\"a\") as f:\n",
    "    for line in lines:\n",
    "        f.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n",
      "input-buffer overflow. The line is split. use -b #SIZE option.\n"
     ]
    }
   ],
   "source": [
    "#わかちがき\n",
    "!mecab -F\"%M||||\" -E\"\\n\" -b 100000 < ../data/corpus_text.txt  > ../data/corpus_text.txt.tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1216549"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "import sentencepiece as spm\n",
    "import sys\n",
    "import yaml\n",
    "import os\n",
    "from special_token_list import *\n",
    "\n",
    "\n",
    "def yaml_to_namespace(yaml_path):\n",
    "    with open(yaml_path, 'r') as file:\n",
    "        # YAMLファイルを辞書として読み込む\n",
    "        data = yaml.safe_load(file)\n",
    "        # 辞書をSimpleNamespaceに変換\n",
    "        return recursive_namespace(data)\n",
    "\n",
    "\n",
    "def recursive_namespace(data):\n",
    "    if isinstance(data, dict):\n",
    "        # 再帰的に辞書の各要素をSimpleNamespaceに変換\n",
    "        return SimpleNamespace(**{k: recursive_namespace(v) for k, v in data.items()})\n",
    "    elif isinstance(data, list):\n",
    "        # リストの要素も変換\n",
    "        return [recursive_namespace(v) for v in data]\n",
    "    else:\n",
    "        # その他のデータ型はそのまま返す\n",
    "        return data\n",
    "\n",
    "\n",
    "args = yaml_to_namespace('../30tokenize/config.yaml')\n",
    "\n",
    "args.input=\"../data/corpus_text.txt.tok\"\n",
    "#args.vocab_size=3000\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin tokenizing...\n"
     ]
    }
   ],
   "source": [
    "print(\"begin tokenizing...\")\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "        input=args.input,\n",
    "        model_prefix=args.model_prefix,\n",
    "        vocab_size=args.vocab_size,\n",
    "        character_coverage=args.character_coverage,\n",
    "        model_type=args.model_type,\n",
    "        num_threads=args.num_threads,\n",
    "        train_extremely_large_corpus=args.train_extremely_large_corpus,\n",
    "        user_defined_symbols=[\n",
    "            BOS_TOKEN,\n",
    "            EOS_TOKEN,\n",
    "            PAD_TOKEN,\n",
    "            CLS_TOKEN,\n",
    "            SEP_TOKEN,\n",
    "            EOD_TOKEN,\n",
    "            MASK_TOKEN,\n",
    "            NEWLINE_TOKEN,\n",
    "            EXTRA_TOKEN1,\n",
    "            EXTRA_TOKEN2,\n",
    "            EXTRA_TOKEN3,\n",
    "            EXTRA_TOKEN4,\n",
    "            EXTRA_TOKEN5,\n",
    "            EXTRA_TOKEN6,\n",
    "            EXTRA_TOKEN7,\n",
    "            EXTRA_TOKEN8,\n",
    "        ],  # Note: `NEWLINE_TOKEN` is needed in `user_defined_symbols`.\n",
    "        byte_fallback=True,\n",
    "        split_digits=True,\n",
    "        allow_whitespace_only_pieces=True,\n",
    "        remove_extra_whitespaces=False,\n",
    "        pretokenization_delimiter=\"||||\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sentencepiece as spm\n",
    "#test\n",
    "model_path=\"../data/tokenizers/tokenizer_scale200.model\"\n",
    "sp = spm.SentencePieceProcessor(model_file=model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['▁', '吾', '輩', 'は', '猫', 'である', '。', '\\n', '名前', 'はまだ', 'ない'],\n",
       " [272, 6324, 1713, 276, 1642, 375, 270, 8, 1132, 2315, 326])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text=\" 価格詳細, 日本の正式代理店で購入する必要がある\"\n",
    "text=\"he is a good man\"\n",
    "text=\"import pandas as pd  from tqdm import tqdm\"\n",
    "text=\"\"\"\n",
    "def yaml_to_namespace(yaml_path):\n",
    "    with open(yaml_path, 'r') as file:\n",
    "        # YAMLファイルを辞書として読み込む\n",
    "        data = yaml.safe_load(file)\n",
    "        # 辞書をSimpleNamespaceに変換\n",
    "        return recursive_namespace(data)\n",
    "\"\"\"\n",
    "\n",
    "text=\"これはテストです。\"\n",
    "text=\"吾輩は猫である｡名前はまだない\"\n",
    "text=\"吾輩は猫である｡\\n名前はまだない\"\n",
    "(sp.encode(text, out_type=str)),(sp.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ああ\\nああ', [8722, 8, 2721], 'ああ\\nああ')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=\"ああ\\nああ\"\n",
    "tokens=sp.encode(t)\n",
    "t,tokens,sp.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
