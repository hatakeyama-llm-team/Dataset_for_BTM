{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.cleaner.auto_cleaner import clean_text,ml_clean_text\n",
    "import glob\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaningの検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中山トンネルは、上越新幹線高崎駅-上毛高原駅間にある総延長14,857 mの複線鉄道トンネルであり高崎方面から進行すると榛名トンネルの次、二番目に通過するトンネルである。建設中に2回の大出水事故を起こして難工事を極め、2回の経路変更によりようやく完成したが、経路変更のためにトンネル内に半径1,500 mの曲線ができてしまい、営業速度240 km/hの新幹線がトンネル内の曲線部分を通過するときには160 km/hに減速せざるをえなくなった。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.cleaner.hojichar_filter import hoji_filter, prob_hoji_filter, prob_filter\n",
    "#クリーニングする処理\n",
    "text=\"衝撃!!|中山トンネルは、上越新幹線高崎駅 - 上毛高原駅間にある総延長14,857 mの複線鉄道トンネルであり高崎方面から進行すると榛名トンネルの次、二番目に通過するトンネルである。建設中に2回の大出水事故を起こして難工事を極め、2回の経路変更によりようやく完成したが、経路変更のためにトンネル内に半径1,500 mの曲線ができてしまい、営業速度240 km/hの新幹線がトンネル内の曲線部分を通過するときには160 km/hに減速せざるをえなくなった。\"\n",
    "#text=\"求人情報求人情報求人情報求人情報求人情報は､､aaaaaaaaaaaaaaaaa\"\n",
    "#text=\"中山トンネルは、上越新幹線高崎駅 - 上毛高原駅間にある総延長14,857 mの複線鉄道トンネルであり高崎方面から進行すると榛名トンネルの次、二番目に通過するトンネルである。建設中に2回の大出水事故を起こして難工事を極め、2回の経路変更によりようやく完成したが、経路変更のためにトンネル内に半径1,500 mの曲線ができてしまい、営業速度240 km/hの新幹線がトンネル内の曲線部分を通過するときには160 km/hに減速せざるをえなくなった。\"\n",
    "print(clean_text(text))\n",
    "#print(prob_filter(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下､クラスタリング後のjsonlファイルを読み込む例｡\n",
    "#datasetsライブラリから､適当にmc4-jaなどを読み込んで問題ナシ｡\n",
    "path_list=glob.glob(\"../data/dedup_categorized/**/*.jsonl\",recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10989"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T17:40:29.367909Z",
     "start_time": "2024-05-15T17:40:27.798851Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/model/entity_vector/entity_vector.model.bin'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclassify\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mText2Vec\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Text2Vec, texts2classes\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfasttext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_facebook_model\n\u001B[0;32m----> 3\u001B[0m t2v \u001B[38;5;241m=\u001B[39m Text2Vec(\u001B[43mload_facebook_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../data/model/entity_vector/entity_vector.model.bin\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/.asdf/installs/python/3.9.12/lib/python3.9/site-packages/gensim/models/fasttext.py:728\u001B[0m, in \u001B[0;36mload_facebook_model\u001B[0;34m(path, encoding)\u001B[0m\n\u001B[1;32m    666\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_facebook_model\u001B[39m(path, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    667\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load the model from Facebook's native fasttext `.bin` output file.\u001B[39;00m\n\u001B[1;32m    668\u001B[0m \n\u001B[1;32m    669\u001B[0m \u001B[38;5;124;03m    Notes\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    726\u001B[0m \n\u001B[1;32m    727\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 728\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load_fasttext_format\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfull_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.asdf/installs/python/3.9.12/lib/python3.9/site-packages/gensim/models/fasttext.py:807\u001B[0m, in \u001B[0;36m_load_fasttext_format\u001B[0;34m(model_file, encoding, full_model)\u001B[0m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_load_fasttext_format\u001B[39m(model_file, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m, full_model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m    789\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load the input-hidden weight matrix from Facebook's native fasttext `.bin` output files.\u001B[39;00m\n\u001B[1;32m    790\u001B[0m \n\u001B[1;32m    791\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    805\u001B[0m \n\u001B[1;32m    806\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 807\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m fin:\n\u001B[1;32m    808\u001B[0m         m \u001B[38;5;241m=\u001B[39m gensim\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39m_fasttext_bin\u001B[38;5;241m.\u001B[39mload(fin, encoding\u001B[38;5;241m=\u001B[39mencoding, full_model\u001B[38;5;241m=\u001B[39mfull_model)\n\u001B[1;32m    810\u001B[0m     model \u001B[38;5;241m=\u001B[39m FastText(\n\u001B[1;32m    811\u001B[0m         vector_size\u001B[38;5;241m=\u001B[39mm\u001B[38;5;241m.\u001B[39mdim,\n\u001B[1;32m    812\u001B[0m         window\u001B[38;5;241m=\u001B[39mm\u001B[38;5;241m.\u001B[39mws,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    821\u001B[0m         max_n\u001B[38;5;241m=\u001B[39mm\u001B[38;5;241m.\u001B[39mmaxn,\n\u001B[1;32m    822\u001B[0m     )\n",
      "File \u001B[0;32m~/.asdf/installs/python/3.9.12/lib/python3.9/site-packages/smart_open/smart_open_lib.py:177\u001B[0m, in \u001B[0;36mopen\u001B[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001B[0m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transport_params \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    175\u001B[0m     transport_params \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m--> 177\u001B[0m fobj \u001B[38;5;241m=\u001B[39m \u001B[43m_shortcut_open\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[43muri\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbuffering\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbuffering\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnewline\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fobj\n",
      "File \u001B[0;32m~/.asdf/installs/python/3.9.12/lib/python3.9/site-packages/smart_open/smart_open_lib.py:363\u001B[0m, in \u001B[0;36m_shortcut_open\u001B[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001B[0m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m    361\u001B[0m     open_kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124merrors\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m errors\n\u001B[0;32m--> 363\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_builtin_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocal_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffering\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbuffering\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mopen_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../data/model/entity_vector/entity_vector.model.bin'"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.classify.Text2Vec import Text2Vec, texts2classes\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "t2v = Text2Vec(load_facebook_model('../data/model/entity_vector/entity_vector.model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_dir = '../data/model/entity_vector/entity_vector.model.bin'\n",
    "model = KeyedVectors.load_word2vec_format(model_dir, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3196374 ,  1.8061428 , -1.6604292 , -0.52288646, -1.0512975 ,\n",
       "        1.5391749 , -1.1566727 , -0.82461774, -0.35898477,  1.2454671 ,\n",
       "       -1.3672171 ,  2.7784317 ,  1.9350111 , -1.0831355 , -0.68428457,\n",
       "       -2.2910554 ,  1.3848027 , -2.5809252 ,  1.7140344 , -0.02286446,\n",
       "        0.48743463, -3.0924594 ,  1.6131709 , -2.3386939 ,  0.9431879 ,\n",
       "       -1.3243551 , -1.5037524 , -0.58501345, -0.59000933, -1.2175318 ,\n",
       "        0.5918976 ,  0.8936033 ,  2.92767   ,  2.5808516 ,  0.25240132,\n",
       "        1.3072065 , -0.99929696, -0.65497893,  0.7952702 ,  0.5303069 ,\n",
       "        0.7305566 , -1.6341536 ,  3.4560654 ,  0.02341504,  1.2947271 ,\n",
       "        0.8061331 ,  0.1369439 , -1.1958709 ,  0.5880644 ,  0.9518098 ,\n",
       "       -1.5347742 , -0.03704506,  1.9196514 , -0.49741378,  0.18996468,\n",
       "        2.0485742 , -1.4030429 , -0.7410698 ,  0.91882706,  2.25174   ,\n",
       "       -1.2315228 ,  1.6947681 , -1.0058278 , -2.6799057 , -0.7104148 ,\n",
       "        3.435787  , -1.4046692 ,  0.45291036,  1.0565063 , -0.8461948 ,\n",
       "       -3.1640196 ,  0.64888203, -1.399048  ,  0.47241396, -0.01752668,\n",
       "        1.2104096 ,  2.3901832 ,  0.81863236, -1.8399897 ,  1.3883724 ,\n",
       "       -1.9731888 ,  0.78735584, -1.9198691 , -0.25979242, -1.9386506 ,\n",
       "       -0.3288854 ,  2.0727334 ,  1.3017743 ,  1.1594691 ,  0.5559459 ,\n",
       "       -0.3290553 ,  1.3914404 , -1.2373985 ,  1.5121813 ,  1.1932248 ,\n",
       "       -1.9059974 , -3.0493915 ,  0.28742906, -3.4606688 ,  1.3452346 ,\n",
       "       -2.6772466 , -4.28033   , -3.6886249 , -3.338902  ,  0.11547095,\n",
       "        1.5204529 ,  2.6498957 ,  1.1934309 , -1.0250934 ,  0.02170203,\n",
       "        0.5887626 , -1.0997807 , -0.1602445 ,  1.812049  ,  1.8318881 ,\n",
       "       -1.6615653 , -0.09486692,  0.23741394,  0.35145164, -0.0733832 ,\n",
       "        2.0587277 ,  0.98026925,  0.41389623,  0.268494  ,  2.737538  ,\n",
       "       -1.0286772 ,  1.3104395 ,  3.6910791 ,  1.5212071 , -0.18581529,\n",
       "        0.7727285 , -0.4689151 , -0.97593147, -2.0912967 ,  0.92327565,\n",
       "        0.467994  , -1.0310591 ,  2.0066736 , -3.3062592 ,  0.32139772,\n",
       "       -0.66105825,  1.124768  ,  0.91663307,  1.5039533 , -2.7332196 ,\n",
       "        0.6700071 , -0.46023822,  0.39064747, -3.7987678 , -0.1135751 ,\n",
       "        0.5383529 , -2.5147386 ,  2.0784366 , -1.2725413 , -3.323065  ,\n",
       "        0.15421394,  0.31819844,  3.5352285 ,  0.9742537 , -1.14151   ,\n",
       "       -0.61672837, -2.4171398 ,  1.4185447 , -1.301237  ,  0.44783747,\n",
       "       -0.55978906, -3.5609589 , -1.2413286 , -0.85452825, -2.2994266 ,\n",
       "        1.2500238 ,  0.6914206 ,  0.5022976 , -2.319108  , -2.378949  ,\n",
       "        0.03109485,  0.7031198 ,  0.39885262,  0.9031111 , -2.0582314 ,\n",
       "       -0.09988285,  0.3381644 , -1.2242286 ,  1.2451372 , -0.26091272,\n",
       "       -1.7639197 ,  1.8839642 , -0.30513576, -1.2544785 ,  1.6887231 ,\n",
       "        0.03191289,  0.70662624, -2.1520326 ,  0.0368898 ,  0.29871565,\n",
       "        1.3575997 ,  0.4155956 ,  0.5251682 ,  1.0581384 ,  1.2753831 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001B[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001B[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001B[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    }
   ],
   "source": [
    "model.get_vector(\"あ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "仲間割れをしたアベンジャーズたちの空港でのバトルが続いている。キャプテン・アメリカとホークアイ対、スパイダーマンとヴィジョンの対決だ。スーパージャンパーを使ってヒーローたちを空高く飛ばしてトラックの火を消したり、オイル缶を爆発させてキャプテン・アメリカの攻撃を止めよう。スパイダーマンのウェブブラストで、バイクに乗って突進してくるホークアイから身を守れ。\n"
     ]
    }
   ],
   "source": [
    "line_list=[]\n",
    "path=random.choice(path_list)\n",
    "with open(path,\"r\") as f:\n",
    "    #1行ずつ読み込む\n",
    "    for line in f:\n",
    "        text=json.loads(line)[\"text\"]\n",
    "        cleaned=clean_text(text)\n",
    "        print(cleaned)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=[]\n",
    "cnt=0\n",
    "\n",
    "while True:\n",
    "    path=random.choice(path_list)\n",
    "    with open(path,\"r\") as f:\n",
    "        #1行ずつ読み込む\n",
    "        for line in f:\n",
    "            cnt+=1\n",
    "            text=json.loads(line)[\"text\"]\n",
    "            text_list.append(text)\n",
    "    if cnt>100:\n",
    "        break\n",
    "random.shuffle(text_list)\n",
    "text_list=text_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 233.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9955269694328308\n",
      "0.9958707690238953\n",
      "0.7076561450958252\n",
      "0.6594078540802002\n",
      "0.5173245668411255\n",
      "0.6453263759613037\n",
      "0.983125627040863\n",
      "0.6897611021995544\n",
      "0.991079568862915\n",
      "0.546472430229187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '似顔絵のコツを知りたいという方。それからやさしい雰囲気の似顔絵を描きたいという方の役に立てる内容にしていきたいと思います。BIRDFULの似顔絵は「やさしさ」をテーマにして描いています。',\n",
       " '森田健作ではなかったの!',\n",
       " 'プーケット行きの乗り継ぎチケットも羽田で発券済なので、飛行機から降りたらまずtransfer counterを目指して進みます。transfer counterはWESTとEASTの両サイドにあって迷いましたが、スワンナプーム空港の国内線はコンコースA,BのあるEASTへ進みます。\\n入国審査を通過する際にシールを渡され胸元に貼るように指示されます。これはプーケットに着いた際に手荷物の受け取り場所が国内線用と国際線用の2つに分かれているので間違えないよう注意するシールのようです。日本から預けた私たちの荷物は国際線用の手荷物カウンターに出てくるので、プーケットに着いたらこのシールと同じ表記の場所で受け取ります。おそらくこれをつけていると間違えそうになった場合現地のスタッフが案内をしてくれるのだと思います。\\nそれにしてもバンコクの空港はエアコンもよく効いていて寒いです。コートは置いてきたとはいえ冬服のセーターを着ているのに寒いと感じるのには少しビックリでした。次行くことがあればブランケットや羽織るものがあるとよさそうです。',\n",
       " 'シェークスピアの史劇「ヘンリー5世」にちなみ、ボドキン(矢じりの古語)の名を冠した気鋭のマイクロ・ワイナリー。醸造家のクリストファー・クリステンセンはスタンフォード大学を卒業後、20代の若さでメドロック・エイムズにて醸造家として活躍する傍ら、2011年より自らのブランドで繊細な食事にもマッチするエレガントなスタイルのソーヴィニヨン・ブランを造り始めた。探究心の強い彼はソーヴィニヨン・ミュスケ単独のワインや赤ワインのように果皮を付けたまま発酵させる古のスタイルなどにも挑戦、個性的で素晴らしい味わいに仕上げている。またシャルマ方式のスパークリングなどにもチャレンジ、その個性的な味わいで最も人気のある一本となっている。繊細な風味を愛する彼は独特の食文化を持つ日本にも深い興味を持っており日本への進出を喜んでいる。\\n価格が安く質の良いぶどうを手に入れることに長けたクリステンセンはノース・コースト各地のクオリティの高いブドウ栽培者達と契約し、パワフルではなく、爽やかさが薫るエレガントなスタイルに仕上げる。ヴィクターズ・スポイルズは\"戦利品\"の意。\\n高いコストパフォーマンスを誇り長年人気を博しているクラインのジンファンデルがリニューアル。内陸部の伝統産地ロダイに残る1948年、1962年、1968年に植樹された株仕立ての3つの畑を中心としたブレンド。\\nパイスとカリニャンを使ったチリ伝統のワイン “ピペーニョ\"。早飲みタイプで冷やしても美味しく飲める。重厚なワインが多い昨今、本来の気軽な美味しさを教えてくれる。アウパとはデイヴィッドの故郷バスクの言葉で“Viva\" (万歳)の意。\\nクラインのユニークな気候風土を持つ複数の畑に植えられる様々な品種をブレンドした、クラインの弟分的なブランド。多くの品種のブレンドによりもたらされる複雑な風味と素晴らしいバランス。コストパフォーマンス抜群のワイン。レッドはスパイシーで重厚さと複雑さを持ち、長い余韻を楽しめる本格派。\\n大人気の“ カリフォルニアシラー\" がグレードアップ。冷涼なソノマコーストを中心にブドウのクオリティをさらに上げ、クラインらしい高いコストパフォーマンスを表している。重すぎず、食欲をそそるスパイシーな味わい。\\nコロンビアヴァレーはフランスのボルドー地方とほぼ同緯度にあるが、カスケード山脈によって海から隔てられ内陸性の気候を持つ。そのためブドウの生育期の昼は十分に暖かく夜は急激に冷え込む。この気候パターンが豊かな果実味と飲み飽きない酸味を併せ持つスタイルを育む。完熟した黒いベリーの風味と黒コショウのフレーバーが食欲をそそる。\\n温暖で陽光豊かなパソ・ロブルスは果実味に溢れ重厚感のあるコストパフォーマンス抜群のカベルネの産地として注目を集める。このエリアは夜は冷え込むため適度な酸味が保たれ、食事とともに愉しむスタイルとなっている。\\nジンファンデルの「聖地」ソノマ、ドライ・クリーク・ヴァレーで優良な栽培者達と密な関係を保ち、古い樹齢にこだわり良質な果汁を得る。マイク・ダッシュは「ジンファンデルは本来濃厚なブドウではない。」と主張しエレガントなスタイルに仕上げる。\\n温暖な内陸のセントラルヴァレーの中でも北部に位置するジンファンデルの名醸地ロダイやアマドールのブドウをレイク・カウンティ、メンドシーノとブレンド。アメリカンオークの風味がほどよく利いた高品質なZIN。\\n安くて美味しいピノ・ノワールを探すのはどこの国でも困難だ。豊かな陽光に恵まれたリゾート地としても高い人気を誇るサンタバーバラは熟度の高い健全なブドウが育つ上、太平洋の寒流がもたらす冷たい潮風と霧により、バランスのよい酸味とエレガントさ、複雑な風味をブドウにもたらし、ピノ・ノワールの新たな聖地として注目を集めている。\\nカリニャンを主体にパイス、カベルネ・フランをブレンドしたワイン。軽やかさの中にスパイシーで野性味のある深い味わいが感じられる。\\n現在もっとも注目されている高級ピノノワールの銘醸地モントレーはモントレー湾から上ってくる冷たい霧が通る谷に沿って高品質なピノを生み出す畑が連なっている。豊かな果実味とハーブやスパイスの風味が食欲をそそるコストパフォーマンス抜群の1本。',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'もうすぐ彼や旦那さんの誕生日。やっぱり身につけて欲しいからお財布をプレゼントしたいな。男性なら日常生活の半数を占めるスーツ姿。バッチリ似合うお財布をここで教えちゃいます!']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "cleaned_list=[ml_clean_text(text) for text in tqdm(text_list)]\n",
    "cleaned_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "似顔絵のコツを知りたいという方。それからやさしい雰囲気の似顔\n",
      "RDFULの似顔絵は「やさしさ」をテーマにして描いています。\n",
      "森田健作ではなかったの!\n",
      "森田健作ではなかったの!\n",
      "プーケット行きの乗り継ぎチケットも羽田で発券済なので、飛行機\n",
      "くことがあればブランケットや羽織るものがあるとよさそうです。\n",
      "シェークスピアの史劇「ヘンリー5世」にちなみ、ボドキン(矢じ\n",
      "スパイスの風味が食欲をそそるコストパフォーマンス抜群の1本。\n",
      "もうすぐ彼や旦那さんの誕生日。やっぱり身につけて欲しいからお\n",
      "占めるスーツ姿。バッチリ似合うお財布をここで教えちゃいます!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for line in cleaned_list:\n",
    "    \n",
    "    t=line[:30]\n",
    "    if t!=\"\":\n",
    "        print(t[:30])\n",
    "    t=line[-30:]\n",
    "    if t!=\"\":\n",
    "        print(t[-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 88.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['夢喰いメリー第12話「夢魘」最終回と思っていたら実はまだもう1話残っていたんですねっ汗そして、本当にどこまで行っても夢もキボーも見えてこないシリアスさですっ\\n一緒に戻ってきた千鶴が突然牢獄のデイドリームへいざなう。そして彼女の夢魔レオンが現れる。二人はミストルティンについて「レオンの銃でなければ、ミストルティンは倒せない」と語り始める。先生斬ればよくね?最終回だと勝手に思ってたけど実は違った第12話。転校生とその夢魔から、ミストルティンを倒すには彼らが持ってる銃が必要と言われますが、それがまた、夢もキボーもありゃしませんよねっ汗弾を作るためにあと一人、犠牲にしなくちゃいけない、それは嫌でも、そうするとミストルティンによってどんどん犠牲者が増えてしまう・・・大を活かすために小を殺すか、小を助けるために大を晒すか、はもしかしたら永遠に全員が納得する答えが出ない身近な選択かもしれませんね。しかもその小が知り合いならなおさら・・・まったく、どこまで行っても夢もキボーも未だに見出せず、むしろますます絶望の淵へ、困難の淵へと落とされていきますね・・・なんとシリアス過ぎる作品なんだろう・・・・・・ってことで、戦っても勝てない、このままじゃ勇魚以前に光凪さんのキボーも破壊されてしまうので、じぶんは夢路がそのソードでミストルティンではなく飯島を斬り捨て殺っちまうことを提言しますwそうすれば器を失ったミストルティンは・・・あれ?器が死んだらどうなるんだろう?ま、まぁ、とりあえず真っ向勝負よりは現段階、最も効果がありそうなので、あと、アイツウザイのでとっとと斬り捨ててくれると嬉しいんですけどもね☆次回最終回・・・ですね。\\n夢喰いメリー第12話「夢魘」\\nミストルティンに完全敗北だったメリーたち。\\n勇魚の夢を消すまでの猶予期間をもらったが...逆転の手はあるのか?\\n千鶴の夢魔・レオンが夢路たちをデイドリームに招き。\\n一人の人の夢を守るため、自らの命をさし出してでも叶えたい夢がある。夢を守ることが夢。そんな夢でいいのか?|所詮、すべては戯言なんだよ|2011/04/03 01:04|夢喰いメリー第12話「夢魘」感想\\nミストルティンとのバトルからあけて光凪医院に戻ってきた夢路たち。\\n一緒に戻ってきた千鶴が突然牢獄のデイドリームへいざなう。\\nそして彼女の夢魔レオンが現れる。\\n二人はミストルティンについて「...|ひえんきゃく|2011/04/03 08:26|夢喰いメリー第12話「夢魘」感想',\n",
       " '「EELSを使用してデバイス構造内の界面ボロン層を検出するには、FE-TEMが必要ですか?」\\n以上は、分析電子顕微鏡の利用者をたびたび悩ませている質問です。これらの質問に対しては実験的に確認する他方法が無いため、TEMラボで多くの時間が空費されてきました。それがEELS Advisorシミュレーションソフトウェアです。\\nEELS Advisorソフトウェアは、電子エネルギー損失スペクトル(下記の参考資料を参照)の最先端の理論モデリングをベースにしています。EELS Advisorは、EELS技術の複雑さをカプセル化し、わかりやすく親しみやすいインターフェイス(下記のUIパレットのスクリーンショットを参照)を備えており、EELS実験に関する一般的な疑問点を提起し、その回答を得ることができます。\\nSimulate:EELSスペクトルのシミュレーション\\nスペクトルをシミュレートするには、上の図のように、EELS Advisorのオプションダイアログの[Specimen(試料)]パネルと[Experiment(実験)]パネルに基本情報を入力するだけです。GMSの各ツールを使用すると、下の図のように、測定されたEELSスペクトルと同じように、シミュレートされたスペクトルを確認して分析できます。\\nこのようなケースのカーボンKエッジは実質的に検出不可能です。このシミュレーションにより、この材料のTEM試料を製作してその炭素成分のEELS測定を行う価値がないことを、即座に効率よく結論付けることができます。\\nOptiEELS:EELS取得の最適な実験パラメータを確立\\n上記のようなシミュレーションを実行した後でも、提案された測定が可能かどうかの疑問が残るかもしれません。測定対象の信号量を最大化するための実験条件を最適に選択した時のみ実現可能だとしてもです。このような疑問への回答を得るには、OptiEELSが最適です。\\nこれらの結果は、次のように、注釈付きの画像ドキュメントに出力されます。\\nOptiMAP:EFTEM元素マッピングの最適条件を決定\\nこれまでに研究してきた材料内のクロムの元素マップを取得するのは有効であるとあなたが判断したとします。マップは本当に可能でしょうか?可能なら、このようなマップについて、エネルギーフィルタ像を取得する最適な条件は何でしょうか?このような疑問点への回答を得るには、OptiMAP機能を使用してください。OptiMAPは、ユーザーが入力した試料および実験条件を利用し、元素マップを得るための最適なスリット幅およびエネルギー位置をわずか数秒で計算します。また、マップする信号のSN比も予測するので、マップデータを生成する価値があるかどうかを判断できます。OptiMAPでは、OptiEELSの結果と似た画像ドキュメントが出力されます。次に例を示します。',\n",
       " 'こちらは、空気で紡いだ太番手の空紡糸を2本と1本を交互で編みたることで横段状の凹凸をつけたオリジナルビボーダー素材を使用したカットソーです。\\n空紡糸は外側の撚りが強く内側に空気を多く含む為、ドライでハリのある着心地となっています。\\n裏面はフラットに編み上げられた肌触りも心地よい一枚、着用シーズンの幅も広い一枚です。\\nトップスにインナーに、スタイリングの幅も広く、フラットな裾でニットやスウェットとのレイヤードも楽しんでいただけます。',\n",
       " 'よい結果が出ることをお祈りいたします。\\n(とくに前回も受けたという方は、お気づきの点があればぜひ教えてください)\\nすっかり放置して申し訳ないです。\\n充電した後、必ずや復活するつもりです。\\njaman先生に励ましのお便りを(?)\\njamanさん、お久しぶりです♪ 本日C級に2度目のチャレンジしてきました。手ごたえは・・やっぱり難しかったですぅ・・(><)。\\n長文問題は、「ウォーキングについて」で、\"ウォーキングは健康いいよ\"って内容がずらずら書いてて、これは内容もわかりやすく解きかったのですが、後半の文法が手ごわかったです。恒例の仲間はずれ問題はありませんでした。問題文がインドネシア語で出題されているのが数問ありました。なんといっても、前回と一番の違いは、ヒアリングのテープの声がバタオネさんじゃなく、女性だったのです!これにはびっくりしました。結果は、2ヶ月後。いいご報告をしたいのですが、C級の壁は厚そうです・・。\\n昨日C級を受けてまいりました。\\n初めて受けたのですが、過去問の構成とは少し異なり、10問ずつの固まりが四つ(+リスニング)で、長文読解の後は文法や作文、日本語訳の選択が中心でした。よくある仲間はずれを選べ風の問いはありませんでした。\\n以下は順不同ですが、\\n・karyawanなどwanの付く単語の中から、人のsifatを表すものを選べというので、消去法で知らない単語のdermawanを選びました。後で辞書で調べて慈善家と分かりましたが、たぶん正解だと思います。\\n・menantikan \"angin\"の「風」が意味するところを聞く問題があり、迷いましたがkesempatanにしました。\\n・membuat jadiの意味のme-iを選ぶ問題が出てmenyakiti hati orangのmenyakitiを選びました。(他の選択肢はmenikmati等)\\n・このほか、PRやPuskesmasは何の省略か、またことわざではmenggarami air lautの意味を問うもの(sia-siaな行為をすることという趣旨の選択肢を選びました)がありました。\\n・リスニングは女性の声で比較的聞きやすく、例えばセラミックの博物館で見るもの(皿やお椀)を聞いたり、洋服屋によりました、何を買うため?といった問題がありました。(あったように聞こえました。)\\n記憶違いがあり得ますが、とりあえず御礼の意をこめて、ご報告いたします。\\njamanさんこんにちは。\\n私もC級受けてきました。\\n500単語のお陰で、長文読解はわかりやすかったです。\\nでも質問がインドネシア語で単語がわからなくて想像力の世界でした。\\nリスニングが女の人だったので前よりかは聞きやすかったのですが、\\n運営も変化のきざしがあるようで、何よりです。\\nwebでしか存じ上げない方もいらっしゃいますが、お三方ともC級の実力は十分だと思いますので、きっといい結果が出ることでしょう。\\n>恒例の仲間はずれ問題はありませんでした。\\nそれは意表をつかれました。私も以前「仲間はずれは評判が良くない」とか書いた気がするので、基本的にはいい傾向だと思いますが、事前に言ってほしいですね。\\nもっとも、本当に「評判が良くない」のかはさだかではありませんが。\\nD級・E級はどうだったのでしょうか?\\n情報をお持ちの方がいらっしゃいましたら、教えてください。\\n>長文読解はわかりやすかったです。\\n昨年のC級の長文問題はえらく難しくて、ほとんどB級と変わらない感じでしたが、C級の適正水準に戻しつつあるのかもしれません。\\njamanさま、はじめまして。てるてると申します。\\nE級の情報を。とのことですので\\nC級レベルの皆さんに混じってご報告するのは気が引けますがご参考までに。\\n『姉が入院し、妹が姉にも面識のある友達を誘ってお見舞いに行く』\\n過去問2000年D級第2回の題材と似ているという印象を受けました。\\nE級では仲間はずれ問題が出ました。\\n曖昧な記憶ですが、このようなかんじの選択肢です。\\nリスニングは私にとってはあまり良くありませんでした。\\n声質がやわらかすぎるのかとても聞き取りづらい読み上げをして\\n少しストレスを感じました。\\n最初のアナウンスもインドネシア語なのですから\\n同じかたが同じ環境で読み上げて欲しかったです。\\n本題の音質が劣るようではテストを兼ねたアナウンスの意味がないのにな\\n末筆ですが、今後一層のご活躍をお祈りしております。\\nbyてるてる(2007-07-05 13:55)\\nE級の傾向はあまり変わっていないようですね。\\nE級の長文は毎回、日常のよくある話で、過去問をやっておけば大丈夫です。\\n仲間はずれは、そんな感じの名詞4択なら、まあ問題ないでしょう。\\nリスニングは改善されたとおっしゃる方もありますが、なにしろ元がひどすぎたので、英検等の他の検定試験に比べたら、まだまだ及第点にほど遠いかもしれません。\\n相変わらず、スタジオではなく、適当にINJか誰かの自宅でラジカセで録音してるのでしょうかね。\\n(少なくとも、私が知っている時代の)リスニングの音質では、満点とれる人などいるはずもないので、6～7点を目標にすればいいでしょう。\\n何年か前にC級を取得し、現在B級に挑戦中です。\\n先日B級を受けてきました。B級受験は1年ぶりです。\\n皆さんと同じくB級もリスニングは女性の声になっていました。検定前に聞き取りを鍛える為にバタオネ先生のカセットを聞いて耳を馴らしていたので、ちょっとガッカリしましたが、スピードは以前に比べてゆっくりだった気がします。すぐにわかるものと、語順を変えて選びにくくしたもの等。今回は数字を含んだ問題は出ませんでした。\\n長文はいつもの様にコンパスから、「若者たちの成長過程とそれを取り囲む環境」のような内容でした。(自分の語彙不足で、それすら違っていたらすみません。)\\nことわざは1問。お陰様で即答できました。\\n翻訳問題は、時事問題に加えてファッション等、5問中3問が「・・・・・・adalah・・・・」形式でした。(だったと思います、少なくとも2問は)今回医療・法律関係の文はありませんでした。\\n年齢的に、近い過去の事は忘れるのが早いので、この位しか今思い出せず、申し訳ありません。問題用紙を持ち帰らせてくれると本当に嬉しいのですが。\\nBerkat Jaman様,少しは前に進めた気がします。有難うございました。今後とも宜しくお願い致します。\\n>アルティメットさん\\n貴重なB級情報、ありがとうございました。\\nバタオネ先生の帰国により、多くの方がリスニングの改善に期待されていたのですが、期待はずれでしたね。\\nこの一ヶ月ほとんど更新していないにもかかわらず、毎日多くのヒット数をいただき、ありがたいです。\\n最近ここへいらっしゃって、熱心にご覧いただいている方も多いのだと思います。\\n過去の問題へのつっこみ・疑問等は大歓迎ですので、何かありましたらお気軽にどうぞ。\\n特にことわざ集、イディオム、略語編、単語等など結局すべてなのですが、7月の検定前にもう一度きっちり見直し勉強させていただきましたところ、やっとB級一次合格できました。ホントに有難うございます。C級合格から何と7年目にしてです。2,3回は受験しなかった事もありますが・・・。二次試験合格まで又どれだけの歳月かかるかわかりませんが折角ここまで来たので頑張ります。本当に有難うございました。また今後もよろしくお願いいたします。\\n>anggrekさん\\n7年ですか、本当に頭が下がります。\\nふだんインドネシア語を使う機会があるかどうか存じませんが、これだけ努力されたのですから、落ち着いて臨めば、2次の面接に通るだけの実力は十分でしょう。\\n知らない単語があっても、あるいはeの発音がエかウかわからなくても、大きな声を自信を持って受けてください。\\njamanさん、ありがとうございます。普段インドネシア語を使う機会はあまり無いのですが、音読と聴き取りの練習をしてみて、内容によっては二次試験の方が通過しやすいような気がしてきました。初めてのことで面接試験ってどんなのかドキドキしますが、今では一次試験に7年もかかった分を取り戻すべく何とかスグ合格できるよう頑張りたいと思っています。好きな分野が出題されることを願うばかりです。色々アドバイス頂き本当に有難うございました。\\nC級やっぱりダメでした。\\nあと三歩位かな?\\n来年1月又受験します。\\nanggrekさん合格してるといいですね。\\n単語力はだいぶ上がったとのことでしたので、後はテクニックを身につければだいじょうぶです。\\n単語とリスニングはどうせみんなできないので、5割で十分です。\\n大問1の長文読解と大問4(3?)の文法で、8割取ることを目指しましょう。\\nジャカルタにある日系企業の工場の現地従業員とお話をするために\\nD級でも取ろうかと思っています。\\n先日二次試験の不合格通知が来ました。落ち着いて受ける事が出来たのですが、肝心の問題文の意味や質問内容を大分取り違えて答えてしまったようです。5問目も的外れの答えだった、と総評に書かれていました。一次試験合格で舞い上がってしまいチョッと二次試験を甘く見ていたと反省然りです。自分の弱点もとてもよくわかりました。又次回受けようと思っています。\\nlumba-lumbaさんにもお励まし頂き嬉しかったです。有難うございました。\\n私も「きっとだいじょうぶでしょう」などと、いささか無責任なことを言って、かえって申し訳なかったかもしれません。\\n2次試験は、その文章が得意分野かどうかで決まってしまう部分があるので、運不運がありますね。\\nまあこれは、1次の長文もけっきょく同じですけど。\\nありがとうございます。勉強にな..\\njaman様こんにちは。..\\nexblogインドネシア語の中庭ノート11/24NEW',\n",
       " '',\n",
       " '',\n",
       " 'みんなのレビュー\\nB4サイズでこの価格は安いと思い購入しました。\\n実際手にしてみると作業台として手やひじを乗せやすい\\nこのサイズを選んで良かったです。\\nデザインは機能的だと思います。\\n発光面以外に周辺を少し照らすので\\n暗い部屋でも部屋を使い勝手はいいです。\\nしかし、ネジの出っ張りは気になります。\\nもう少しフラットな物に変更できれば\\n書類も傷まないのにと思い、星四つです。\\n良い買い物ができました!サイズが最高です。フィルムの閲覧に使用します。\\nぽるぽーぬさん\\n思ったより大きくて使い心地が良いです。\\nちゃんと使えて良かったです(^_^)\\nこの価格でこの質なので大満足です!\\nあいあい1234さん\\n軽いし、電池でも使えるのでどこでも描ける!',\n",
       " '6月2日の「ダビング10」放送開始,予定が確定に変わるのはいつか\\nタイムリミットまで6週間となり,チューナー内蔵録画機のメーカーから,「もう準備が間に合わない」という悲鳴が上がっている。6月2日という期日はあくまで予定であり,いつまでたっても「確定」にならないからだ。\\nメーカーは「ダビング10と補償金は本来別の議論であり,早く移行日を確定すべき」と主張する。しかし放送業界に詳しい関係者によると,「コンテンツ側が納得しない限り,放送業界はダビング10の放送に踏み切れない」と指摘する。文化庁の小委員会は2008年5月8日以降の会合で,補償金制度に関する議論をひとまず収拾するための暫定的な結論を模索することになっている。ここでコンテンツ業界が納得できない結論が出ると,ダビング10の移行が宙に浮く危険性すらある。\\nこうした状況では,録画機メーカーはダビング10への対応に慎重にならざるを得ない。ダビング10への切り替え日より前に同方式の対応製品を出荷しユーザーが録画すると,その製品は有料のBSデジタル放送であるWOWOWやスター・チャンネルの番組をダビング10の番組として処理してしまうからだ。このため,6月2日前に出荷する製品についてタイマーを設定し動作を切り替えるという手法はとれない。6月2日以降の出荷品についても当面は「切り替え以降に,放送波を使ったダウンロードによる受信機ソフトウエアの書き換え」という手段をとらざるを得ない状況である。\\n録画機メーカーなどが間に合わないと主張する理由は,流通を抱えるからである。製品の配送だけではなく,パンフレットや説明書の準備,さらにはユーザーへの周知など,録画機にとってダビング10は最重要機能であるだけに事前の作業は膨大である。例えば録画機のソフトウエアをバージョンアップするためには,新しいソフトウエアが送られる時間に電源を入れておく必要がある。この工程を得ないまま録画すると,ダビング10にならずユーザーの苦情が殺到する。このため,1カ月弱は告知期間として十分とは言いがたい。\\n技術的な準備は粛々と\\nデジタル放送推進協会(Dpa)の関係者はこうした状況を認識しつつも,「6月2日の旗は降ろさない」という。この期日を維持することで,関係者が早期の解決を進めることを期待しているようだ。技術的な準備も粛々と進めている。\\n地上デジタル放送やBSデジタル放送などの受信機メーカーは,自社製品がダビング10とコピーワンスの両方の信号に反応するかの動作試験を進めている。動作試験の結果は,Dpaに報告される。Dpa関係者は,「実験結果に問題がなければ今週(2008年4月20～26日)中に,技術的にはダビング10への切り替えに問題がないことを確定できる」という。\\nこうした厳しい状況のなかで,情報通信審議会情報通信政策部会の「デジタル・コンテンツの流通の促進等に関する検討委員会」は第35回会合(2008年4月11日開催)で,新ワーキンググループ(WG)の設置を決めた。このWGは,情通審のデジタルコンテンツの流通促進に関する中間答申(2007年8月公表)に盛り込まれた課題の確認とフォローアップを目的とする。\\nこの中間答申では,地上デジタル放送のコンテンツ保護技術をコピーワンスからダビング10に変更すべきと提言した。さらには「クリエーターに適切な対価を還元するための制度やルールの在り方について検討を進め,可能な限り早期に具体策がまとめられることを期待する」という見解も示した。Dpaの関係者は,「実力者が集まるこのWGで議論が前進するのではないか」といちるの望みを託している。',\n",
       " 'ご存じの通り神待ち少女と出会えるのは早いもの勝ちですので、好みの子がいましたらスグに連絡しましょう。\\n色麻町にもかわいい神待ち少女は沢山いますので積極的に誘ってみて下さい。今は特に家出の多いシーズンですので簡単に神待ち少女が見つかるはずです。',\n",
       " '手技による顔面攻撃を認めたルールで試合を行うなど、武道としての空手を追求してきた極真館の型を解説したハウツー中巻。これから空手を学ぶ初心者や自分の型を見直したい人たちに向け、盧山初雄館長と岡崎寛人副本部長が正しい型を徹底指導。\\n曲目Disc1:1.北の国から愛を込めて/2.サタデーナイトビリーバー/3.北の国から愛を込めて(instrumental)/4.サタデーナイトビリーバー(instrumental)/Disc2:1.北の国から愛を込めて(Music Video)\\n【中古】【古本】菜の花食堂のささやかな事件簿/碧野圭【文庫大和書房】の通販はWowma!\\n無地チェスターコート男の子コートアウタージャケットピーコートお出かけフォーマル110 120 130 617081の通販はWowma!']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_list=[clean_text(text) for text in tqdm(text_list)]\n",
    "cleaned_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path=\"../data/test_articles.txt\"\n",
    "with open(out_path,\"w\") as f:\n",
    "    f.write(\"\")\n",
    "for path in path_list:\n",
    "    with open(path,\"r\") as f:\n",
    "        #1行ずつ読み込む\n",
    "        cnt=0\n",
    "        for line in f:\n",
    "            cleaned=json.loads(line)[\"text\"]\n",
    "            cleaned=cleaned.replace(\"\\n\",\"\")\n",
    "            if len(cleaned)<100:\n",
    "                with open(out_path,\"a\") as f:\n",
    "                    f.write(cleaned+\"\\n\")\n",
    "            else:\n",
    "                with open(out_path,\"a\") as f:\n",
    "                    f.write(cleaned[:100]+\"\\n\")\n",
    "                with open(out_path,\"a\") as f:\n",
    "                    f.write(cleaned[-100:]+\"\\n\")\n",
    "            cnt+=1\n",
    "            if cnt>1:\n",
    "                break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ処理の作業練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# %%\n",
    "\n",
    "pq_list=glob.glob(\"/data/hatakeyama/python/llm_corpus_original/**/*.parquet\",recursive=True)\n",
    "\n",
    "pq_list=[i for i in pq_list if i.endswith('.parquet')]\n",
    "print(len(pq_list), \" files found\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "for database_path in pq_list:\n",
    "    #database_path=\"/data/hatakeyama/python/llm_corpus_original/CC-MAIN-2023-50_v2/text/batch10/CC-MAIN-2023-50-batch10-iter10404.parquet\"\n",
    "    df=pd.read_parquet(database_path)\n",
    "    if df.shape[0]==0:\n",
    "        continue\n",
    "    lines=df['text'].tolist()\n",
    "    break\n",
    "\n",
    "lines\n",
    "#lines=[]\n",
    "#for article in read_gzip_json_file(database_path):\n",
    "#    text = article.get('text', '')  # 'text'キーからテキストデータを取得\n",
    "#    lines.append(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 2, 2, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import numpy as np\n",
    "from src.classify.TfidfClassifier import prepare_pileline\n",
    "\n",
    "# テキストデータセット (仮のもの)\n",
    "texts = [\n",
    "    \"猫は可愛い\",\n",
    "    \"犬も可愛い\",\n",
    "    \"猫は動物\",\n",
    "    \"犬は友達\",\n",
    "    \"動物園には猫がいない\",\n",
    "    \"動物園には犬がいない\",\n",
    "    \"可愛い犬と遊ぶ\",\n",
    "    \"可愛い猫と遊ぶ\"\n",
    "]\n",
    "\n",
    "pipe=prepare_pileline()\n",
    "pipe.fit(texts)\n",
    "clusters = pipe.named_steps['cluster'].labels_\n",
    "\n",
    "\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;truncator&#x27;, TextTruncator()),\n",
       "                (&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(norm=None,\n",
       "                                 tokenizer=&lt;function extract_nouns at 0x14c393de4400&gt;)),\n",
       "                (&#x27;cluster&#x27;, MiniBatchKMeans(n_clusters=20, random_state=0))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;truncator&#x27;, TextTruncator()),\n",
       "                (&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(norm=None,\n",
       "                                 tokenizer=&lt;function extract_nouns at 0x14c393de4400&gt;)),\n",
       "                (&#x27;cluster&#x27;, MiniBatchKMeans(n_clusters=20, random_state=0))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">TextTruncator</label><div class=\"sk-toggleable__content fitted\"><pre>TextTruncator()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(norm=None, tokenizer=&lt;function extract_nouns at 0x14c393de4400&gt;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MiniBatchKMeans<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.cluster.MiniBatchKMeans.html\">?<span>Documentation for MiniBatchKMeans</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MiniBatchKMeans(n_clusters=20, random_state=0)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('truncator', TextTruncator()),\n",
       "                ('tfidf',\n",
       "                 TfidfVectorizer(norm=None,\n",
       "                                 tokenizer=<function extract_nouns at 0x14c393de4400>)),\n",
       "                ('cluster', MiniBatchKMeans(n_clusters=20, random_state=0))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001B[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001B[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001B[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "kmeans = joblib.load(\"../data/model/kmeans.pkl\")\n",
    "kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63755  files found\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'database_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[114], line 83\u001B[0m\n\u001B[1;32m     79\u001B[0m             docs \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 83\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[114], line 63\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     60\u001B[0m docs \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     62\u001B[0m \u001B[38;5;66;03m# gzの場合\u001B[39;00m\n\u001B[0;32m---> 63\u001B[0m lines \u001B[38;5;241m=\u001B[39m load_gzip_or_parquet(\u001B[43mdatabase_path\u001B[49m)\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m lines:\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m do_ml_clean:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'database_path' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "# %%\n",
    "with open(\"temp/gz_list.txt\", \"r\") as f:\n",
    "    gz_list = f.read().splitlines()\n",
    "\n",
    "# gz_list = [i for i in gz_list if i.endswith('.gz')]\n",
    "print(len(gz_list), \" files found\")\n",
    "\n",
    "random.shuffle(gz_list)\n",
    "\n",
    "\n",
    "# %%\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from src.classify.Text2Vec import Text2Vec, texts2classes\n",
    "from src.cleaner.auto_cleaner import clean_text, ml_clean_text\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "import joblib\n",
    "from src.load_gz import read_gzip_json_file, load_gzip_or_parquet\n",
    "from src.distribute_jsonl import process_lines, make_dir\n",
    "import pandas as pd\n",
    "\n",
    "streaming = True\n",
    "base_dir = \"../data/categorized\"\n",
    "length_threshold = 30  # 短い記事は捨てる\n",
    "check_length = 200  # はじめのlengthだけで分類する\n",
    "\n",
    "# 機械学習で記事を選別する\n",
    "do_ml_clean = True\n",
    "\n",
    "# load models\n",
    "#t2v = Text2Vec(load_facebook_model('../data/model/cc.ja.300.bin'))\n",
    "t2v = Text2Vec(load_facebook_model('../data/model/cc.ja.300.bin'))\n",
    "kmeans = joblib.load(\"../data/model/kmeans.pkl\")\n",
    "\n",
    "\n",
    "make_dir(base_dir)\n",
    "\n",
    "\n",
    "def proc(docs, base_dir, database_path,\n",
    "         check_length=check_length):\n",
    "    return process_lines(docs, t2v, kmeans, base_dir, database_path, check_length=check_length)\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    docs = []\n",
    "\n",
    "    # gzの場合\n",
    "    lines = load_gzip_or_parquet(database_path)\n",
    "\n",
    "    for text in lines:\n",
    "        if do_ml_clean:\n",
    "            text = ml_clean_text(text)\n",
    "        else:\n",
    "            text = clean_text(text)\n",
    "        if len(text) < length_threshold:\n",
    "            continue\n",
    "\n",
    "        docs.append(text)\n",
    "        if len(docs) == batch_size:\n",
    "            proc(docs, base_dir, database_path,\n",
    "                 check_length=check_length)\n",
    "\n",
    "            # docsをリセット\n",
    "            docs = []\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gzの場合\n",
    "database_path=\"../data/original_dump/jap2010/jap2010_0001_0.jsonl.gz\"\n",
    "lines = load_gzip_or_parquet(database_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[117], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m lines:\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m do_ml_clean:\n\u001B[0;32m----> 7\u001B[0m         text \u001B[38;5;241m=\u001B[39m \u001B[43mml_clean_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m      9\u001B[0m         text \u001B[38;5;241m=\u001B[39m clean_text(text)\n",
      "File \u001B[0;32m/media/hatakeyama/python/Dataset_for_BTM/01web_codes/src/cleaner/auto_cleaner.py:92\u001B[0m, in \u001B[0;36mml_clean_text\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m     90\u001B[0m text \u001B[38;5;241m=\u001B[39m classifier\u001B[38;5;241m.\u001B[39mclean(text)\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 92\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[43mclean_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhoji\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m text\n",
      "File \u001B[0;32m/media/hatakeyama/python/Dataset_for_BTM/01web_codes/src/cleaner/auto_cleaner.py:77\u001B[0m, in \u001B[0;36mclean_text\u001B[0;34m(text, hoji)\u001B[0m\n\u001B[1;32m     74\u001B[0m     text \u001B[38;5;241m=\u001B[39m hoji_filter(text)\n\u001B[1;32m     75\u001B[0m     text \u001B[38;5;241m=\u001B[39m prob_hoji_filter(text)\n\u001B[0;32m---> 77\u001B[0m paragraphs \u001B[38;5;241m=\u001B[39m \u001B[43mtext_to_cleaned_paragraphs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;66;03m# print(\"aa\", original_text)\u001B[39;00m\n\u001B[1;32m     79\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(paragraphs)\n",
      "File \u001B[0;32m/media/hatakeyama/python/Dataset_for_BTM/01web_codes/src/cleaner/auto_cleaner.py:28\u001B[0m, in \u001B[0;36mtext_to_cleaned_paragraphs\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m     24\u001B[0m text \u001B[38;5;241m=\u001B[39m repeated_phrase\u001B[38;5;241m.\u001B[39mis_repetitive_japanese(\n\u001B[1;32m     25\u001B[0m     text)  \u001B[38;5;66;03m# n-gramの計算(計算量が多そうな場合、削る)\u001B[39;00m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# パラグラフと文章に分割\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m paragraphs \u001B[38;5;241m=\u001B[39m \u001B[43mtext_to_paragraph_sentences\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m new_paragraphs \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m paragraph \u001B[38;5;129;01min\u001B[39;00m paragraphs:\n",
      "File \u001B[0;32m/media/hatakeyama/python/Dataset_for_BTM/01web_codes/src/cleaner/splitter.py:20\u001B[0m, in \u001B[0;36mtext_to_paragraph_sentences\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124;03mSplit a text into paragraphs and sentences.\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03m    List[List[str]]: The paragraphs and sentences.\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     19\u001B[0m paragraphs \u001B[38;5;241m=\u001B[39m paragraph_split(text)\n\u001B[0;32m---> 20\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43msentence_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparagraph\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m paragraph \u001B[38;5;129;01min\u001B[39;00m paragraphs]\n",
      "File \u001B[0;32m/media/hatakeyama/python/Dataset_for_BTM/01web_codes/src/cleaner/splitter.py:53\u001B[0m, in \u001B[0;36msentence_split\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msentence_split\u001B[39m(text: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m     44\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;124;03m    Split a text into sentences.\u001B[39;00m\n\u001B[1;32m     46\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;124;03m        List[str]: The sentences.\u001B[39;00m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 53\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msegmenter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/textprocess/lib/python3.12/site-packages/ja_sentence_segmenter/split/simple_splitter.py:137\u001B[0m, in \u001B[0;36msplit_punctuation\u001B[0;34m(arg, punctuations, split_between_quote, split_between_parens)\u001B[0m\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m __split_punctuation_iter(\u001B[38;5;28miter\u001B[39m(arg), punctuations, split_between_quote, split_between_parens)\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arg, Iterator):\n\u001B[0;32m--> 137\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m __split_punctuation_iter(arg, punctuations, split_between_quote, split_between_parens)\n",
      "File \u001B[0;32m~/miniconda3/envs/textprocess/lib/python3.12/site-packages/ja_sentence_segmenter/split/simple_splitter.py:54\u001B[0m, in \u001B[0;36m__split_punctuation_iter\u001B[0;34m(texts, punctuations, split_between_quote, split_between_parens)\u001B[0m\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arg, Iterator):\n\u001B[1;32m     51\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m __split_newline_iter(arg)\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__split_punctuation_iter\u001B[39m(texts: Iterator[\u001B[38;5;28mstr\u001B[39m], punctuations: \u001B[38;5;28mstr\u001B[39m, split_between_quote: \u001B[38;5;28mbool\u001B[39m, split_between_parens: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Generator[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m]:\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mescape_between_punctuation\u001B[39m(match: Match[\u001B[38;5;28mstr\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m     56\u001B[0m         text \u001B[38;5;241m=\u001B[39m match\u001B[38;5;241m.\u001B[39mgroup()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "docs=[]\n",
    "# gzの場合\n",
    "lines = load_gzip_or_parquet(database_path)\n",
    "\n",
    "for text in lines:\n",
    "    if do_ml_clean:\n",
    "        text = ml_clean_text(text)\n",
    "    else:\n",
    "        text = clean_text(text)\n",
    "    if len(text) < length_threshold:\n",
    "        continue\n",
    "\n",
    "    docs.append(text)\n",
    "    if len(docs) == batch_size:\n",
    "        proc(docs, base_dir, database_path,\n",
    "                check_length=check_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=200\n",
    "import numpy as np\n",
    "target_texts =docs\n",
    "target_texts = [i[:length] for i in target_texts]\n",
    "vec = np.array([np.array(t2v.text2vec(i), dtype=\"float32\") for i in target_texts])\n",
    "classes = kmeans.predict(vec.astype(\"float64\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2841, 1990, 6089, 5703, 2597, 4077,   60, 2501, 8525, 7998, 6457,\n",
       "       4461, 9651,  869, 1932,  108, 8631], dtype=int32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in vec:\n",
    "    if v.dtype is not vec[1].dtype:\n",
    "        print(\"dtype error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
