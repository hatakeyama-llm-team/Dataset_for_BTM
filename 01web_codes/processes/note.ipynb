{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目標\n",
    "* 「html構造」→「trafilatura」→「できるだけ分解した処理群」\n",
    "* trafilatura\n",
    "* 処理群の分解\n",
    "\n",
    "* Mecabのエラーについて：辞書のインストールが必要になるが、[この記事](https://analytics-note.xyz/programming/python-mecabrc-error/)と全く同じ現象が起きていた。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -illow (/data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: warcio in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (1.7.4)\n",
      "Requirement already satisfied: six in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from warcio) (1.16.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -illow (/data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: beautifulsoup4 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -illow (/data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: trafilatura in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (1.8.1)\n",
      "Requirement already satisfied: htmldate>=1.8.0 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from trafilatura) (1.8.1)\n",
      "Requirement already satisfied: courlan>=1.0.0 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from trafilatura) (1.0.0)\n",
      "Requirement already satisfied: justext>=3.0.0 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from trafilatura) (3.0.0)\n",
      "Requirement already satisfied: lxml<5.2.0,>=4.9.4 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from trafilatura) (5.1.0)\n",
      "Requirement already satisfied: certifi in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from trafilatura) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from trafilatura) (2.0.7)\n",
      "Requirement already satisfied: charset-normalizer>=3.2.0 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from trafilatura) (3.3.2)\n",
      "Requirement already satisfied: langcodes>=3.3.0 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from courlan>=1.0.0->trafilatura) (3.3.0)\n",
      "Requirement already satisfied: tld>=0.13 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from courlan>=1.0.0->trafilatura) (0.13)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from htmldate>=1.8.0->trafilatura) (2.8.2)\n",
      "Requirement already satisfied: dateparser>=1.1.2 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from htmldate>=1.8.0->trafilatura) (1.2.0)\n",
      "Requirement already satisfied: pytz in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from dateparser>=1.1.2->htmldate>=1.8.0->trafilatura) (2023.3.post1)\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from dateparser>=1.1.2->htmldate>=1.8.0->trafilatura) (2023.10.3)\n",
      "Requirement already satisfied: tzlocal in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from dateparser>=1.1.2->htmldate>=1.8.0->trafilatura) (5.2)\n",
      "Requirement already satisfied: six>=1.5 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->htmldate>=1.8.0->trafilatura) (1.16.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -illow (/data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: mecab-python3 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (1.0.8)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -illow (/data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: unidic in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.22.0 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from unidic) (2.31.0)\n",
      "Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from unidic) (0.10.1)\n",
      "Requirement already satisfied: plac<2.0.0,>=1.1.3 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from unidic) (1.4.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.41.1 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from unidic) (4.66.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.22.0->unidic) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.22.0->unidic) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.22.0->unidic) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.22.0->unidic) (3.3.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -illow (/data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: unidic-lite in /data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages (1.0.8)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -illow (/data/steve_ssd2/takumi/experiment/cramming/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install warcio\n",
    "!pip install beautifulsoup4\n",
    "!pip install trafilatura\n",
    "!pip install mecab-python3\n",
    "!pip install unidic\n",
    "!pip install unidic-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私\t代名詞,,,,,,ワタクシ,私-代名詞,私,ワタクシ,私,ワタクシ,和,\"\",\"\",\"\",\"\",\"\",\"\",体,ワタクシ,ワタクシ,ワタクシ,ワタクシ,\"0\",\"\",\"\",11345327978324480,41274\n",
      "は\t助詞,係助詞,,,,,ハ,は,は,ワ,は,ワ,和,\"\",\"\",\"\",\"\",\"\",\"\",係助,ハ,ハ,ハ,ハ,\"\",\"動詞%F2@0,名詞%F1,形容詞%F2@-1\",\"\",8059703733133824,29321\n",
      "Yahoo\t名詞,固有名詞,一般,,,,ヤフー,ヤフー,Ｙａｈｏｏ,ヤフー,Ｙａｈｏｏ,ヤフー,固,\"\",\"\",\"\",\"\",\"\",\"\",固有名,ヤフー,ヤフー,ヤフー,ヤフー,\"2,1\",\"\",\"\",11994031248777728,43634\n",
      "プレミアム\t名詞,普通名詞,一般,,,,プレミアム,プレミアム-premium,プレミアム,プレミアム,プレミアム,プレミアム,外,\"\",\"\",\"\",\"\",\"\",\"\",体,プレミアム,プレミアム,プレミアム,プレミアム,\"0,2,3\",\"C2\",\"\",9278787250430464,33756\n",
      "会員\t名詞,普通名詞,一般,,,,カイイン,会員,会員,カイイン,会員,カイイン,漢,\"\",\"\",\"\",\"\",\"\",\"\",体,カイイン,カイイン,カイイン,カイイン,\"0\",\"C2\",\"\",1549220507165184,5636\n",
      "に\t助詞,格助詞,,,,,ニ,に,に,ニ,に,ニ,和,\"\",\"\",\"\",\"\",\"\",\"\",格助,ニ,ニ,ニ,ニ,\"\",\"名詞%F1\",\"\",7745518285496832,28178\n",
      "なり\t動詞,非自立可能,,,五段-ラ行,連用形-一般,ナル,成る,なり,ナリ,なる,ナル,和,\"\",\"\",\"\",\"\",\"\",\"\",用,ナリ,ナル,ナリ,ナル,\"1\",\"C1\",\"\",7713357570384513,28061\n",
      "まし\t助動詞,,,,助動詞-マス,連用形-一般,マス,ます,まし,マシ,ます,マス,和,\"\",\"\",\"\",\"\",\"\",\"\",助動,マシ,マス,マシ,マス,\"\",\"動詞%F4@1\",\"\",9812325267808897,35697\n",
      "た\t助動詞,,,,助動詞-タ,終止形-一般,タ,た,た,タ,た,タ,和,\"\",\"\",\"\",\"\",\"\",\"\",助動,タ,タ,タ,タ,\"\",\"動詞%F2@1,形容詞%F4@-2\",\"\",5948916285711019,21642\n",
      "。\t補助記号,句点,,,,,,。,。,,。,,記号,\"\",\"\",\"\",\"\",\"\",\"\",補助,,,,,\"\",\"\",\"\",6880571302400,25\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "mecab = MeCab.Tagger()\n",
    "print(mecab.parse(\"私はYahooプレミアム会員になりました。\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://feathercloud.blks.jp/faq/04/01/</td>\n",
       "      <td>Feathercloud [よくある質問-FAQ]</td>\n",
       "      <td>2023-11-28T10:19:01Z</td>\n",
       "      <td>b'&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://fline.biz/list/detail_gourmet/post-108.php</td>\n",
       "      <td>ホームライフサポート｜一関市　フリーペーパー【FREE LINE】地域情報が満載！</td>\n",
       "      <td>2023-11-28T11:18:14Z</td>\n",
       "      <td>b'&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://gagon.com/videos/xha/xha023/xhs023.html</td>\n",
       "      <td>Fetish Factory Videos Hair Scene019</td>\n",
       "      <td>2023-11-28T10:21:42Z</td>\n",
       "      <td>b'&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://ikechang.com/news/2008/news0809j5.html</td>\n",
       "      <td>天童のニュース（平成20年9月・第5週） - 天童市の観光ガイド</td>\n",
       "      <td>2023-11-28T10:52:42Z</td>\n",
       "      <td>b'&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://ks-com.co.jp/</td>\n",
       "      <td>K'S COM GROUP</td>\n",
       "      <td>2023-11-28T10:07:26Z</td>\n",
       "      <td>b'&lt;?xml version=\"1.0\" encoding=\"Shift_JIS\"?&gt;\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0             http://feathercloud.blks.jp/faq/04/01/   \n",
       "1  http://fline.biz/list/detail_gourmet/post-108.php   \n",
       "2     http://gagon.com/videos/xha/xha023/xhs023.html   \n",
       "3      http://ikechang.com/news/2008/news0809j5.html   \n",
       "4                               http://ks-com.co.jp/   \n",
       "\n",
       "                                       title             timestamp  \\\n",
       "0                  Feathercloud [よくある質問-FAQ]  2023-11-28T10:19:01Z   \n",
       "1  ホームライフサポート｜一関市　フリーペーパー【FREE LINE】地域情報が満載！  2023-11-28T11:18:14Z   \n",
       "2        Fetish Factory Videos Hair Scene019  2023-11-28T10:21:42Z   \n",
       "3           天童のニュース（平成20年9月・第5週） - 天童市の観光ガイド  2023-11-28T10:52:42Z   \n",
       "4                              K'S COM GROUP  2023-11-28T10:07:26Z   \n",
       "\n",
       "                                             content  \n",
       "0  b'<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01...  \n",
       "1  b'<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0...  \n",
       "2  b'<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0...  \n",
       "3  b'<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01...  \n",
       "4  b'<?xml version=\"1.0\" encoding=\"Shift_JIS\"?>\\n...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=\"/data/fairy_hdd4/takumi/matsuo_llm/Dataset_for_BTM/01web_codes/processes/CC-MAIN-2023-50_text_batch0_CC-MAIN-2023-50-batch0-iter0.parquet\"\n",
    "\n",
    "df=pd.read_parquet(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trafilatura import fetch_url, extract\n",
    "df=df[[\"content\",\"url\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=df['content'].iloc[1].decode(\"UTF-8\")\n",
    "text1 = extract(text1, include_tables=False,target_lang='ja',favour_precision=True) #trafilaturaでテキスト抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "UTF-8\n",
      "1\n",
      "UTF-8\n",
      "2\n",
      "UTF-8\n",
      "3\n",
      "EUC-JP\n",
      "4\n",
      "Shift_JIS\n",
      "5\n",
      "UTF-8\n",
      "6\n",
      "no\n",
      "7\n",
      "Shift_JIS\n",
      "8\n",
      "UTF-8\n",
      "9\n",
      "Shift_JIS\n",
      "10\n",
      "UTF-8\n",
      "11\n",
      "UTF-8\n",
      "12\n",
      "UTF-8\n",
      "13\n",
      "Shift_JIS\n",
      "14\n",
      "UTF-8\n",
      "15\n",
      "Shift_JIS\n",
      "16\n",
      "UTF-8\n",
      "17\n",
      "UTF-8\n",
      "18\n",
      "Shift_JIS\n",
      "19\n",
      "UTF-8\n",
      "20\n",
      "no\n",
      "21\n",
      "UTF-8\n",
      "22\n",
      "UTF-8\n",
      "23\n",
      "Shift_JIS\n",
      "24\n",
      "UTF-8\n",
      "25\n",
      "Shift_JIS\n",
      "26\n",
      "Shift_JIS\n",
      "27\n",
      "UTF-8\n",
      "28\n",
      "Shift_JIS\n",
      "29\n",
      "EUC-JP\n",
      "30\n",
      "UTF-8\n",
      "31\n",
      "UTF-8\n",
      "32\n",
      "UTF-8\n",
      "33\n",
      "UTF-8\n",
      "34\n",
      "UTF-8\n",
      "35\n",
      "Shift_JIS\n",
      "36\n",
      "UTF-8\n",
      "37\n",
      "UTF-8\n",
      "38\n",
      "UTF-8\n",
      "39\n",
      "UTF-8\n",
      "40\n",
      "UTF-8\n",
      "41\n",
      "UTF-8\n",
      "42\n",
      "UTF-8\n",
      "43\n",
      "UTF-8\n",
      "44\n",
      "UTF-8\n",
      "45\n",
      "UTF-8\n",
      "46\n",
      "UTF-8\n",
      "47\n",
      "UTF-8\n",
      "48\n",
      "UTF-8\n",
      "49\n",
      "UTF-8\n",
      "50\n",
      "UTF-8\n",
      "51\n",
      "UTF-8\n",
      "52\n",
      "UTF-8\n",
      "53\n",
      "UTF-8\n",
      "54\n",
      "Shift_JIS\n",
      "55\n",
      "UTF-8\n",
      "56\n",
      "Shift_JIS\n",
      "57\n",
      "UTF-8\n",
      "58\n",
      "UTF-8\n",
      "59\n",
      "UTF-8\n",
      "60\n",
      "UTF-8\n",
      "61\n",
      "UTF-8\n",
      "62\n",
      "UTF-8\n",
      "63\n",
      "UTF-8\n",
      "64\n",
      "UTF-8\n",
      "65\n",
      "UTF-8\n",
      "66\n",
      "UTF-8\n",
      "67\n",
      "UTF-8\n",
      "68\n",
      "UTF-8\n",
      "69\n",
      "UTF-8\n",
      "70\n",
      "UTF-8\n",
      "71\n",
      "UTF-8\n",
      "72\n",
      "UTF-8\n",
      "73\n",
      "EUC-JP\n",
      "74\n",
      "Shift_JIS\n",
      "75\n",
      "UTF-8\n",
      "76\n",
      "UTF-8\n",
      "77\n",
      "UTF-8\n",
      "78\n",
      "UTF-8\n",
      "79\n",
      "UTF-8\n",
      "80\n",
      "UTF-8\n",
      "81\n",
      "UTF-8\n",
      "82\n",
      "UTF-8\n",
      "83\n",
      "UTF-8\n",
      "84\n",
      "UTF-8\n",
      "85\n",
      "UTF-8\n",
      "86\n",
      "UTF-8\n",
      "87\n",
      "UTF-8\n",
      "88\n",
      "UTF-8\n",
      "89\n",
      "UTF-8\n",
      "90\n",
      "Shift_JIS\n",
      "91\n",
      "UTF-8\n",
      "92\n",
      "Shift_JIS\n",
      "93\n",
      "Shift_JIS\n",
      "94\n",
      "UTF-8\n",
      "95\n",
      "UTF-8\n",
      "96\n",
      "UTF-8\n",
      "97\n",
      "UTF-8\n",
      "98\n",
      "no\n",
      "99\n",
      "UTF-8\n",
      "100\n",
      "UTF-8\n",
      "101\n",
      "UTF-8\n",
      "102\n",
      "Shift_JIS\n",
      "103\n",
      "UTF-8\n"
     ]
    }
   ],
   "source": [
    "#変換\n",
    "texts=[]\n",
    "euc_jp_list=[3,29,73]\n",
    "utf_list=[0,1,2,5,8,10,11,12,14,16,17,19,21,22,24,27,30,31,32,33,34,36,37,38,\n",
    "          39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,55,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,91,94,95,96,97,99,100,101,103]\n",
    "shift_jis_list=[4,7,9,13,15,18,23,25,26,28,35,54,56,74,90,92,93,102] #6,20,98\n",
    "for i in range(len(df)):\n",
    "    print(i)\n",
    "    if i in euc_jp_list:\n",
    "        text=df['content'].iloc[i].decode(\"EUC-JP\")\n",
    "        print(\"EUC-JP\")\n",
    "        texts.append(text)\n",
    "    elif i in shift_jis_list:\n",
    "        text=df['content'].iloc[i].decode(\"Shift_JIS\")\n",
    "        print(\"Shift_JIS\")\n",
    "        texts.append(text)\n",
    "    elif i in [6,20,98]:\n",
    "        #text=df['content'].iloc[i].decode(\"shift_jis\")\n",
    "        print(\"no\")\n",
    "    else:# i in utf_list:\n",
    "        text=df['content'].iloc[i].decode(\"UTF-8\")\n",
    "        print(\"UTF-8\")\n",
    "        texts.append(text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パイプライン\n",
    "1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text checker\n",
    "def char_is_hiragana(c):\n",
    "    return u'\\u3040' <= c <= u'\\u309f'\n",
    "\n",
    "def contains_hiragana(s):\n",
    "    return any(char_is_hiragana(c) for c in s)\n",
    "\n",
    "def check(s):\n",
    "    if not contains_hiragana(s):\n",
    "        return \"\"\n",
    "\n",
    "    else:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hojichar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hojichar\n",
    "from hojichar import Compose, document_filters\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "sys.path.append(\"/data/fairy_hdd4/takumi/matsuo_llm/Dataset_for_BTM/01web_codes/src\")\n",
    "\n",
    "base_path = \"/data/fairy_hdd4/takumi/matsuo_llm/Dataset_for_BTM/01web_codes/src/cleaner/hoji_dict/\"\n",
    "cleaner = Compose([\n",
    "    document_filters.JSONLoader(key=\"text\"),\n",
    "    document_filters.AcceptJapanese(),\n",
    "    document_filters.DocumentLengthFilter(min_doc_len=10, max_doc_len=50000),\n",
    "    document_filters.MaskPersonalInformation(),\n",
    "    document_filters.JSONDumper(),\n",
    "])\n",
    "\n",
    "with open(base_path + \"adult_keywords_ja.txt\") as f:\n",
    "    adult_keywords_ja = f.read().splitlines()\n",
    "with open(base_path + \"adult_keywords_en.txt\") as f:\n",
    "    adult_keywords_en = f.read().splitlines()\n",
    "with open(base_path + \"advertisement_keywords_ja.txt\") as f:\n",
    "    advertisement_keywords_ja = f.read().splitlines()\n",
    "\n",
    "noise_keywords = adult_keywords_ja + adult_keywords_en + advertisement_keywords_ja\n",
    "noise_keywords = list(set(noise_keywords))\n",
    "noise_keywords = [k for k in noise_keywords if k != \"\"]\n",
    "\n",
    "prob_cleaner = Compose([\n",
    "    document_filters.JSONLoader(key=\"text\"),\n",
    "    # document_filters.DiscardRareKuten(),  # 日本語以外を消す\n",
    "    document_filters.DiscardAdultContentJa(\n",
    "        base_path + \"adult_keywords_ja.txt\"),\n",
    "    document_filters.DiscardAdultContentEn(\n",
    "        base_path + \"adult_keywords_en.txt\"\n",
    "    ),\n",
    "    # document_filters.DiscardDiscriminationContentJa(\n",
    "    #    base_path + \"discrimination_keywords_ja.txt\"\n",
    "    # ),\n",
    "    # document_filters.DiscardViolenceContentJa(\n",
    "    #    base_path + \"violence_keywords_ja.txt\"\n",
    "    # ),\n",
    "    document_filters.DiscardBBSComments(),\n",
    "    document_filters.DiscardAds(\n",
    "        base_path + \"advertisement_keywords_ja.txt\",\n",
    "        max_allowed_num=10,\n",
    "    ),\n",
    "    document_filters.JSONDumper(),\n",
    "])\n",
    "\n",
    "def hoji_filter(text):\n",
    "    d = {\"text\": text}\n",
    "    parsed = cleaner(json.dumps(d))\n",
    "    if parsed == \"\":\n",
    "        return \"\"\n",
    "    text = json.loads(parsed)[\"text\"]\n",
    "    return text\n",
    "\n",
    "\n",
    "# 確率的にフィルタリングするが､ちょっとイマイチ\n",
    "def prob_hoji_filter(text, survive_ratio=0.5):\n",
    "    # ngワード類については､確率的に処理する\n",
    "    if random.random() < survive_ratio:\n",
    "        return text\n",
    "\n",
    "    d = {\"text\": text}\n",
    "    parsed = prob_cleaner(json.dumps(d))\n",
    "    if parsed == \"\":\n",
    "        return \"\"\n",
    "    text = json.loads(parsed)[\"text\"]\n",
    "    return text\n",
    "\n",
    "\n",
    "def prob_filter(text, ratio=5):\n",
    "    # 遅い\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # テキスト全体の長さ\n",
    "    total_length = len(text)\n",
    "\n",
    "    # noise_keywordsの各単語がテキストに含まれる回数の合計を計算\n",
    "    noise_length = sum(text.count(word) for word in noise_keywords)\n",
    "\n",
    "    # noise_keywordsがテキストに占める割合を計算\n",
    "    noise_ratio = noise_length / total_length\n",
    "\n",
    "    # print(noise_ratio*ratio)\n",
    "    # 割合が閾値以上であれば、確率的に\"\"を返す\n",
    "    if noise_ratio*ratio > random.random():\n",
    "        return \"\"\n",
    "    else:\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 追加処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 91011.16it/s]\n"
     ]
    }
   ],
   "source": [
    "##追加処理\n",
    "\n",
    "# %%\n",
    "# クリーニングスクリプトに入れ忘れた、日付系を消す処理\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "# 冒頭の数字だけの行を削除\n",
    "\n",
    "def remove_num_lines(record):\n",
    "    lines = record[\"text\"].split(\"\\n\")\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        check_line = line[:20]\n",
    "        count = sum(c.isdigit() for c in check_line)\n",
    "        # num_ratio=count/len(check_line)\n",
    "        num_ratio = count\n",
    "        # print(num_ratio)\n",
    "        ratio = 5\n",
    "        if num_ratio > ratio and check_line.find(\":\") > 0:\n",
    "            continue\n",
    "        if num_ratio > ratio and check_line.find(\"日\") > 0:\n",
    "            continue\n",
    "        if num_ratio > ratio and check_line.find(\"年\") > 0:\n",
    "            continue\n",
    "        if num_ratio > ratio and check_line.find(\"-\") > 0:\n",
    "            continue\n",
    "        if num_ratio > ratio and check_line.find(\"/\") > 0:\n",
    "            continue\n",
    "        if num_ratio > ratio and check_line.find(\"／\") > 0:\n",
    "            continue\n",
    "        if num_ratio > ratio and check_line.find(\"月\") > 0:\n",
    "            continue\n",
    "\n",
    "        new_lines.append(line)\n",
    "    record[\"text\"] = \"\\n\".join(new_lines)\n",
    "    return record\n",
    "\n",
    "\n",
    "cluster_ids = list(range(10000))\n",
    "random.shuffle(cluster_ids)\n",
    "for cat_id in tqdm(cluster_ids):\n",
    "    jsonl_list = glob.glob(\n",
    "        f\"../data/categorized/{cat_id}/*.jsonl\", recursive=True)\n",
    "    for path in (jsonl_list):\n",
    "        # ファイルの最終更新時間とサイズをチェック\n",
    "        try:\n",
    "            stats = os.stat(path)\n",
    "        except:\n",
    "            continue\n",
    "        last_modified = datetime.fromtimestamp(stats.st_mtime)\n",
    "        if (datetime.now() - last_modified) > timedelta(minutes=5):\n",
    "            # if True:\n",
    "\n",
    "            record_list = []\n",
    "            with open(path, \"r\") as f:\n",
    "                for record in f:\n",
    "                    data = json.loads(record)\n",
    "                    record_list.append(data)\n",
    "\n",
    "            cleaned_record_list = []\n",
    "            for record in record_list:\n",
    "                cleaned = remove_num_lines(record)\n",
    "                if len(cleaned[\"text\"]) > 0:\n",
    "                    cleaned_record_list.append(cleaned)\n",
    "\n",
    "            with open(path, \"w\") as f:\n",
    "                for record in cleaned_record_list:\n",
    "                    f.write(json.dumps(record, ensure_ascii=False)+\"\\n\")\n",
    "            # break\n",
    "            # if path==\"../data/categorized/39/00A2Zpve0kEr.jsonl\":\n",
    "            #    break\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正則化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/lighttransport/japanese-llama-experiment/blob/main/02_normalize/text_normalizer.py\n",
    "\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "# Added some modification for Japanese text normalization.\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "#\n",
    "# NOTE: 実際のところ, 01_normalize では PUNCT 変換は行わない.\n",
    "# dedup 時には行う(PUNCT 除去)\n",
    "# TODO: dedup 時, 日本語の場合は句読点を含んだままのほうが良いような気もするがどうか.\n",
    "#\n",
    "\n",
    "UNICODE_PUNCT = {\n",
    "    # 日本語の場合は句読点は、。のままがよいでしょう\n",
    "    \"。\": \"。\",\n",
    "    \"、\": \"、\",\n",
    "    \"，\": \"、\",\n",
    "\n",
    "    \"„\": '\"',\n",
    "    \"”\": '\"',\n",
    "    \"“\": '\"',\n",
    "    \"«\": '\"',\n",
    "    \"»\": '\"',\n",
    "    \"１\": '\"',\n",
    "    \"」\": '\"',\n",
    "    \"「\": '\"',\n",
    "    \"《\": '\"',\n",
    "    \"》\": '\"',\n",
    "    \"´\": \"'\",\n",
    "    \"∶\": \":\",\n",
    "    \"：\": \":\",\n",
    "    \"？\": \"?\",\n",
    "    \"！\": \"!\",\n",
    "    \"（\": \"(\",\n",
    "    \"）\": \")\",\n",
    "    \"；\": \";\",\n",
    "    \"–\": \"-\",\n",
    "    \"—\": \" - \",\n",
    "    \"．\": \". \",\n",
    "    \"～\": \"~\",\n",
    "    \"’\": \"'\",\n",
    "    \"…\": \"...\",\n",
    "    \"━\": \"-\",\n",
    "    \"〈\": \"<\",\n",
    "    \"〉\": \">\",\n",
    "    \"【\": \"[\",\n",
    "    \"】\": \"]\",\n",
    "    \"％\": \"%\",\n",
    "    \"►\": \"-\",\n",
    "}\n",
    "\n",
    "UNICODE_PUNCT_RE = re.compile(f\"[{''.join(UNICODE_PUNCT.keys())}]\")\n",
    "\n",
    "\n",
    "def replace_unicode_punct(text: str) -> str:\n",
    "    return \"\".join((UNICODE_PUNCT.get(c, c) for c in text))\n",
    "\n",
    "\n",
    "def remove_unicode_punct(text: str) -> str:\n",
    "    \"\"\"More aggressive version of replace_unicode_punct but also faster.\"\"\"\n",
    "    return UNICODE_PUNCT_RE.sub(\"\", text)\n",
    "\n",
    "\n",
    "# Reuse `strip_accents` for CJK text. Use NFKC\n",
    "def strip_accents(line: str) -> str:\n",
    "    \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "    # nfd = unicodedata.normalize(\"NFD\", line)\n",
    "    nkfc = unicodedata.normalize(\"NFKC\", line)\n",
    "    output = [c for c in nkfc if unicodedata.category(c) != \"Mn\"]\n",
    "    if len(output) == line:\n",
    "        return line\n",
    "    return \"\".join(output)\n",
    "\n",
    "\n",
    "# Build a regex matching all control characters.\n",
    "# newline(LF, 10) has meaningful infor in CJK text, so do not remove it.\n",
    "NON_PRINTING_CHARS_RE = re.compile(\n",
    "    f\"[{''.join(map(chr, list(range(0,10)) + list(range(11, 32)) + list(range(127,160))))}]\"\n",
    ")\n",
    "DIGIT_RE = re.compile(r\"\\d\")\n",
    "PUNCT_OR_NON_PRINTING_CHARS_RE = re.compile(\n",
    "    (UNICODE_PUNCT_RE.pattern +\n",
    "     NON_PRINTING_CHARS_RE.pattern).replace(\"][\", \"\")\n",
    ")\n",
    "\n",
    "\n",
    "def remove_non_printing_char(text: str) -> str:\n",
    "    return NON_PRINTING_CHARS_RE.sub(\"\", text)\n",
    "\n",
    "\n",
    "def normalize_spacing_for_tok(text: str, language: str = \"en\") -> str:\n",
    "    res = (\n",
    "        text.replace(\"\\r\", \"\")\n",
    "        # remove extra spaces\n",
    "        .replace(\"(\", \" (\")\n",
    "        .replace(\")\", \") \")\n",
    "        .replace(\" +\", \" \")\n",
    "    )\n",
    "    res = re.sub(r\"\\) ([\\.\\!\\:\\?\\;\\,])\", r\"\\)\\1\", res)\n",
    "    res = res.replace(\"( \", \"(\").replace(\" )\", \")\")\n",
    "    res = re.sub(r\"(\\d) \\%\", r\"\\1\\%\", res)\n",
    "    res = res.replace(\" :\", \":\").replace(\" ;\", \";\")\n",
    "    res = res.replace(\"`\", \"'\").replace(\"''\", ' \" ')\n",
    "\n",
    "    res = (\n",
    "        res.replace(\"„\", '\"')\n",
    "        .replace(\"“\", '\"')\n",
    "        .replace(\"”\", '\"')\n",
    "        .replace(\"–\", \"-\")\n",
    "        .replace(\"—\", \" - \")\n",
    "        .replace(\" +\", \" \")\n",
    "        .replace(\"´\", \"'\")\n",
    "        .replace(\"([a-z])‘([a-z])\", r\"\\1'\\2/\")\n",
    "        .replace(\"([a-z])’([a-z])\", r\"\\1'\\2/\")\n",
    "        .replace(\"‘\", '\"')\n",
    "        .replace(\"‚\", '\"')\n",
    "        .replace(\"’\", '\"')\n",
    "        .replace(\"''\", '\"')\n",
    "        .replace(\"´´\", '\"')\n",
    "        .replace(\"…\", \"...\")\n",
    "        # French quotes\n",
    "        .replace(\" « \", ' \"')\n",
    "        .replace(\"« \", '\"')\n",
    "        .replace(\"«\", '\"')\n",
    "        .replace(\" » \", '\" ')\n",
    "        .replace(\" »\", '\"')\n",
    "        .replace(\"»\", '\"')\n",
    "        # handle pseudo-spaces\n",
    "        .replace(\" %\", \"%\")\n",
    "        .replace(\"nº \", \"nº \")\n",
    "        .replace(\" :\", \":\")\n",
    "        .replace(\" ºC\", \" ºC\")\n",
    "        .replace(\" cm\", \" cm\")\n",
    "        .replace(\" ?\", \"?\")\n",
    "        .replace(\" !\", \"!\")\n",
    "        .replace(\" ;\", \";\")\n",
    "        .replace(\", \", \", \")\n",
    "        .replace(\" +\", \" \")\n",
    "        .replace(\"．\", \". \")\n",
    "    )\n",
    "    # English \"quotation,\" followed by comma, style\n",
    "    if language == \"en\":\n",
    "        res = re.sub(r\"\\\"([,\\.]+)\", r\"\\1\\\"\", res)\n",
    "    # Czech is confused\n",
    "    elif language == \"cs\" or language == \"cz\":\n",
    "        pass\n",
    "    # German/Spanish/French \"quotation\", followed by comma, style\n",
    "    else:\n",
    "        res = res.replace(',\"', '\",')\n",
    "        res = re.sub(\n",
    "            r\"(\\.+)\\\"(\\s*[^<])\", r\"\\\"\\1\\2\", res\n",
    "        )  # don't fix period at end of sentence\n",
    "\n",
    "    if (\n",
    "        language == \"de\"\n",
    "        or language == \"es\"\n",
    "        or language == \"cz\"\n",
    "        or language == \"cs\"\n",
    "        or language == \"fr\"\n",
    "    ):\n",
    "        res = re.sub(r\"(\\d) (\\d)\", r\"\\1,\\2\", res)\n",
    "    else:\n",
    "        res = re.sub(r\"(\\d) (\\d)\", r\"\\1.\\2\", res)\n",
    "    return res\n",
    "\n",
    "\n",
    "# NOTE accent=True will do NFKC normalization\n",
    "# NOTE: set punct=0(no zenkaku->hankaku conversion) hby default for Japanese dataset\n",
    "def normalize(line: str, accent=True, case=False, numbers=False, punct=0) -> str:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return line\n",
    "    if case:\n",
    "        line = line.lower()\n",
    "\n",
    "    # FIXME: Always apply NKFC normalization for CJK text.\n",
    "    if accent:\n",
    "        line = strip_accents(line)\n",
    "    if numbers:\n",
    "        line = DIGIT_RE.sub(\"0\", line)\n",
    "    if punct == 1:\n",
    "        line = replace_unicode_punct(line)\n",
    "    elif punct == 2:\n",
    "        line = remove_unicode_punct(line)\n",
    "    line = remove_non_printing_char(line)\n",
    "    return line\n",
    "\n",
    "\n",
    "def slow_normalize_for_dedup(line: str) -> str:\n",
    "    return normalize(line, accent=False, case=True, numbers=True, punct=2)\n",
    "\n",
    "\n",
    "def normalize_for_dedup(line: str) -> str:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return line\n",
    "    # case\n",
    "    line = line.lower()\n",
    "    # numbers\n",
    "    line = DIGIT_RE.sub(\"0\", line)\n",
    "    line = PUNCT_OR_NON_PRINTING_CHARS_RE.sub(\"\", line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1787164060.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    export MECABBC /data/satori_hdd4/takumi/steve_ssd2/experiment/cramming/venv/lib/python3.10/site-packages/MeCab\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "export MECABBC /data/satori_hdd4/takumi/steve_ssd2/experiment/cramming/venv/lib/python3.10/site-packages/MeCab\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "wakati.parse(\"pythonが大好きです\").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\n----------------------------------------------------------\n\nFailed initializing MeCab. Please see the README for possible solutions:\n\n    https://github.com/SamuraiT/mecab-python3#common-issues\n\nIf you are still having trouble, please file an issue here, and include the\nERROR DETAILS below:\n\n    https://github.com/SamuraiT/mecab-python3/issues\n\nissueを英語で書く必要はありません。\n\n------------------- ERROR DETAILS ------------------------\narguments: \ndefault dictionary path: /data/satori_hdd4/takumi/steve_ssd2/experiment/cramming/venv/lib/python3.10/site-packages/unidic/dicdir\n[ifs] no such file or directory: /data/satori_hdd4/takumi/steve_ssd2/experiment/cramming/venv/lib/python3.10/site-packages/unidic/dicdir/mecabrc\n----------------------------------------------------------\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/steve_ssd2/experiment/cramming/venv/lib/python3.10/site-packages/MeCab/__init__.py:137\u001b[0m, in \u001b[0;36mTagger.__init__\u001b[0;34m(self, rawargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ee:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tagger \u001b[38;5;241m=\u001b[39m \u001b[43mMeCab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tagger\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpythonが大好きです\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/steve_ssd2/experiment/cramming/venv/lib/python3.10/site-packages/MeCab/__init__.py:139\u001b[0m, in \u001b[0;36mTagger.__init__\u001b[0;34m(self, rawargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Tagger, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(args)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ee:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_info(rawargs)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mee\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \n----------------------------------------------------------\n\nFailed initializing MeCab. Please see the README for possible solutions:\n\n    https://github.com/SamuraiT/mecab-python3#common-issues\n\nIf you are still having trouble, please file an issue here, and include the\nERROR DETAILS below:\n\n    https://github.com/SamuraiT/mecab-python3/issues\n\nissueを英語で書く必要はありません。\n\n------------------- ERROR DETAILS ------------------------\narguments: \ndefault dictionary path: /data/satori_hdd4/takumi/steve_ssd2/experiment/cramming/venv/lib/python3.10/site-packages/unidic/dicdir\n[ifs] no such file or directory: /data/satori_hdd4/takumi/steve_ssd2/experiment/cramming/venv/lib/python3.10/site-packages/unidic/dicdir/mecabrc\n----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tagger = MeCab.Tagger()\n",
    "print(tagger.parse(\"pythonが大好きです\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parts filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\n----------------------------------------------------------\n\nFailed initializing MeCab. Please see the README for possible solutions:\n\n    https://github.com/SamuraiT/mecab-python3#common-issues\n\nIf you are still having trouble, please file an issue here, and include the\nERROR DETAILS below:\n\n    https://github.com/SamuraiT/mecab-python3/issues\n\nissueを英語で書く必要はありません。\n\n------------------- ERROR DETAILS ------------------------\narguments: -Owakati\ndefault dictionary path: /data/satori_hdd4/takumi/steve_ssd2/experiment/cramming/venv/lib/python3.10/site-packages/unidic/dicdir\n[ifs] no such file or directory: /data/satori_hdd4/takumi/steve_ssd2/experiment/cramming/venv/lib/python3.10/site-packages/unidic/dicdir/mecabrc\n----------------------------------------------------------\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/steve_ssd2/experiment/cramming/venv/lib/python3.10/site-packages/MeCab/__init__.py:137\u001b[0m, in \u001b[0;36mTagger.__init__\u001b[0;34m(self, rawargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ee:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mMeCab\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m----> 3\u001b[0m wakati \u001b[38;5;241m=\u001b[39m \u001b[43mMeCab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-Owakati\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# テキスト\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tagger \u001b[38;5;241m=\u001b[39m MeCab\u001b[38;5;241m.\u001b[39mTagger()\n",
      "File \u001b[0;32m~/steve_ssd2/experiment/cramming/venv/lib/python3.10/site-packages/MeCab/__init__.py:139\u001b[0m, in \u001b[0;36mTagger.__init__\u001b[0;34m(self, rawargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Tagger, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(args)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ee:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_info(rawargs)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mee\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \n----------------------------------------------------------\n\nFailed initializing MeCab. Please see the README for possible solutions:\n\n    https://github.com/SamuraiT/mecab-python3#common-issues\n\nIf you are still having trouble, please file an issue here, and include the\nERROR DETAILS below:\n\n    https://github.com/SamuraiT/mecab-python3/issues\n\nissueを英語で書く必要はありません。\n\n------------------- ERROR DETAILS ------------------------\narguments: -Owakati\ndefault dictionary path: /data/satori_hdd4/takumi/steve_ssd2/experiment/cramming/venv/lib/python3.10/site-packages/unidic/dicdir\n[ifs] no such file or directory: /data/satori_hdd4/takumi/steve_ssd2/experiment/cramming/venv/lib/python3.10/site-packages/unidic/dicdir/mecabrc\n----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "from collections import Counter\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "# テキスト\n",
    "tagger = MeCab.Tagger()\n",
    "\n",
    "def parts_count(text, return_word_count=False):\n",
    "    # テキストを解析\n",
    "    parsed = tagger.parse(text)\n",
    "\n",
    "    # 品詞をカウントするためのCounterオブジェクト\n",
    "    pos_counter = Counter()\n",
    "    word_counter = Counter()\n",
    "\n",
    "    # 解析結果を行ごとに処理\n",
    "    all_counts = 0\n",
    "    for line in parsed.split('\\n'):\n",
    "        # EOSまたは空行の場合はスキップ\n",
    "        if line == 'EOS' or line == '':\n",
    "            continue\n",
    "        # タブで分割し、形態素情報を取得\n",
    "        pos_info = line.split('\\t')\n",
    "        # print(pos_info)\n",
    "        pos = pos_info[1]\n",
    "        pos = pos.split(\",\")[0]\n",
    "\n",
    "        if return_word_count:\n",
    "            word = pos_info[0]\n",
    "            word_counter[(word, pos)] += 1\n",
    "\n",
    "        # 品詞をカウント\n",
    "        pos_counter[pos] += 1\n",
    "        all_counts += 1\n",
    "\n",
    "    if return_word_count:\n",
    "        return pos_counter, all_counts, word_counter\n",
    "    else:\n",
    "        return pos_counter, all_counts\n",
    "\n",
    "\n",
    "def filter(text, threshold=0.9, min_length=10):\n",
    "    \"\"\"\n",
    "    名詞の羅列の文章は無効と判定する\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    if text == \"\":\n",
    "        return None\n",
    "    pos_counter, all_counts = parts_count(text)\n",
    "    # print(pos_counter, all_counts)\n",
    "    # print(pos_counter)\n",
    "    meishi_and_symbol_counts = pos_counter['名詞'] + \\\n",
    "        pos_counter['記号']+pos_counter['補助記号']+pos_counter['接頭詞']\n",
    "\n",
    "    if all_counts == 0:\n",
    "        return None\n",
    "    ratio = meishi_and_symbol_counts/all_counts\n",
    "    # print(ratio, pos_counter)\n",
    "    if ratio > threshold and len(text) > min_length:\n",
    "        return None\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "def n_gram(words, n):\n",
    "    return [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "# +(n-gramによって重複の有無を調べる)\n",
    "\n",
    "\n",
    "def filter2(text, threshold=0.9, min_length=10,  ngram_threshold_2gram=0.20, ngram_threshold_3gram=0.20, ngram_threshold_4gram=0.20):\n",
    "    if text is None:\n",
    "        return None\n",
    "\n",
    "    pos_counter, words, word_counter = parts_count(\n",
    "        text, return_word_count=True)\n",
    "    meishi_and_symbol_counts = pos_counter['名詞'] + \\\n",
    "        pos_counter['記号'] + pos_counter['補助記号']\n",
    "    ratio = meishi_and_symbol_counts / len(words)\n",
    "\n",
    "    if ratio > threshold and len(text) > min_length:\n",
    "        return None\n",
    "\n",
    "    # 2-gramの処理\n",
    "    ngram_counts_2gram = Counter(n_gram(words, 2))\n",
    "    total_2grams = sum(ngram_counts_2gram.values())\n",
    "    most_common_2gram_count = ngram_counts_2gram.most_common(\n",
    "        1)[0][1] if ngram_counts_2gram else 0\n",
    "    if total_2grams > 0 and most_common_2gram_count / total_2grams > ngram_threshold_2gram:\n",
    "        return None\n",
    "\n",
    "    # 3-gramの処理\n",
    "    ngram_counts_3gram = Counter(n_gram(words, 3))\n",
    "    total_3grams = sum(ngram_counts_3gram.values())\n",
    "    repeated_3grams = sum(\n",
    "        count for count in ngram_counts_3gram.values() if count > 1)\n",
    "    if total_3grams > 0 and repeated_3grams / total_3grams > ngram_threshold_3gram:\n",
    "        return None\n",
    "\n",
    "    # 4-gramの処理\n",
    "    ngram_counts_4gram = Counter(n_gram(words, 4))\n",
    "    total_4grams = sum(ngram_counts_4gram.values())\n",
    "    repeated_4grams = sum(\n",
    "        count for count in ngram_counts_4gram.values() if count > 1)\n",
    "    if total_4grams > 0 and repeated_4grams / total_4grams > ngram_threshold_4gram:\n",
    "        return None\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repetitipn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 単一文字の繰り返し200----------------------------------------\n",
    "\n",
    "\n",
    "def repeated_id(text, threshold_ratio=0.3):\n",
    "    # # 文字の繰り返しをチェックする辞書\n",
    "    char_count = {}\n",
    "    # 各文字の繰り返し回数をカウント\n",
    "    for char in text:\n",
    "        if char in char_count:\n",
    "            char_count[char] += 1\n",
    "        else:\n",
    "            char_count[char] = 1\n",
    "    # ratio%以上繰り返される文字があるかチェック\n",
    "    repeated_chars = [char for char, count in char_count.items(\n",
    "    ) if count >= threshold_ratio*len(text)]\n",
    "    # 繰り返される文字がなければmatrix_tempに追加\n",
    "    if not repeated_chars:\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def is_repetitive_japanese(text, thresholds={\n",
    "    'line_dup': 0.30,\n",
    "    'paragraph_dup': 0.30,\n",
    "    'char_in_line_dup': 0.20,\n",
    "    'char_in_paragraph_dup': 0.20,\n",
    "    '2-gram': 0.20,\n",
    "    '3-gram': 0.18,\n",
    "    '4-gram': 0.16,\n",
    "    '5-gram': 0.15,\n",
    "    '6-gram': 0.14,\n",
    "    '7-gram': 0.13,\n",
    "    '8-gram': 0.12,\n",
    "    '9-gram': 0.11,\n",
    "        '10-gram': 0.10}):\n",
    "    if text == \"\":\n",
    "        return \"\"\n",
    "    # 段落と行に分割\n",
    "    paragraphs = text.split('\\n')\n",
    "    lines = text.replace('\\n', ' ').split('。')\n",
    "\n",
    "    # 段落と行の重複率を計算\n",
    "    paragraph_dup_rate = calc_dup_rate(paragraphs)\n",
    "    line_dup_rate = calc_dup_rate(lines)\n",
    "\n",
    "    # 文字に含まれる重複の割合を計算\n",
    "    char_in_paragraph_dup_rate = calc_char_dup_rate(paragraphs, text)\n",
    "    char_in_line_dup_rate = calc_char_dup_rate(lines, text)\n",
    "\n",
    "    # n-gramの重複率を計算\n",
    "    ngram_dup_rates = {}\n",
    "    for n in range(2, 11):\n",
    "        ngrams = extract_ngrams(text.replace('\\n', ''), n)\n",
    "        if n < 5:\n",
    "            # 最頻出のn-gramの出現回数を計算\n",
    "            ngram_dup_rates[n] = calc_max_freq_rate(ngrams)\n",
    "        else:\n",
    "            # 2回以上出現するn-gramの総出現回数を計算\n",
    "            ngram_dup_rates[n] = calc_total_dup_freq_rate(ngrams)\n",
    "\n",
    "    # 各指標が閾値を超えているかチェック\n",
    "    if (line_dup_rate > thresholds['line_dup'] or\n",
    "        paragraph_dup_rate > thresholds['paragraph_dup'] or\n",
    "        char_in_line_dup_rate > thresholds['char_in_line_dup'] or\n",
    "        char_in_paragraph_dup_rate > thresholds['char_in_paragraph_dup'] or\n",
    "            any(ngram_dup_rates[n] > thresholds[f'{n}-gram'] for n in range(2, 11))):\n",
    "        # print(text)\n",
    "        return \"\"\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def calc_dup_rate(items):\n",
    "    counter = Counter(items)\n",
    "    total = len(items)\n",
    "    dup_count = sum(1 for count in counter.values() if count > 1)\n",
    "    return dup_count / total if total > 0 else 0\n",
    "\n",
    "\n",
    "def calc_char_dup_rate(items, text):\n",
    "    counter = Counter(items)\n",
    "    total_chars = len(text)\n",
    "    dup_chars = sum(len(item) * (count - 1)\n",
    "                    for item, count in counter.items() if count > 1)\n",
    "    return dup_chars / total_chars if total_chars > 0 else 0\n",
    "\n",
    "\n",
    "def extract_ngrams(text, n):\n",
    "    return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "\n",
    "\n",
    "def calc_max_freq_rate(ngrams):\n",
    "    counter = Counter(ngrams)\n",
    "    total = len(ngrams)\n",
    "    max_freq = max(counter.values())\n",
    "    return max_freq / total if total > 0 else 0\n",
    "\n",
    "\n",
    "def calc_total_dup_freq_rate(ngrams):\n",
    "    counter = Counter(ngrams)\n",
    "    total = len(ngrams)\n",
    "    dup_freq = sum(count - 1 for count in counter.values() if count > 1)\n",
    "    return dup_freq / total if total > 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rule base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .parts_filter import filter\n",
    "\n",
    "noise_ending_list = \"\"\"\n",
    "一覧ページ上部へ\n",
    "ページ上部へ\n",
    "上部へ\n",
    "上へ戻る\n",
    "\"\"\"\n",
    "noise_ending_list = noise_ending_list.split(\"\\n\")\n",
    "noise_ending_list = [x for x in noise_ending_list if len(x) > 0]\n",
    "\n",
    "\n",
    "def clean_endings(sent: str):\n",
    "    for noise_ending in noise_ending_list:\n",
    "        if sent.endswith(noise_ending):\n",
    "            return sent[:-len(noise_ending)]\n",
    "    return sent\n",
    "\n",
    "\n",
    "noise_header_list = \"\"\"\n",
    "トップページ>\n",
    "\"\"\"\n",
    "noise_header_list = noise_header_list.split(\"\\n\")\n",
    "noise_header_list = [x for x in noise_header_list if len(x) > 0]\n",
    "\n",
    "\n",
    "def clean_headers(text):\n",
    "    for noise_header in noise_header_list:\n",
    "        if text.startswith(noise_header):\n",
    "            return text[len(noise_header):]\n",
    "    return text\n",
    "\n",
    "\n",
    "sentence_endings = ['。', '！', '？', '.', '!', '?', \"．\", \"」\", \"。\"]\n",
    "\n",
    "# 文頭の見出しを消す\n",
    "\n",
    "\n",
    "def remove_header(txt, header_list, n_check=30):\n",
    "    for header in header_list:\n",
    "        if header in txt[:n_check]:\n",
    "            for delimiter in header_list:\n",
    "                txt = txt.split(delimiter)[1:]\n",
    "                txt = delimiter.join(txt)\n",
    "                break\n",
    "    return txt\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    text = remove_header(text, header_list=[\"|\", \"】\", \">\", \"]\",])\n",
    "    text = clean_endings(text)\n",
    "    text = clean_headers(text)\n",
    "    text = dedup_lines(text)\n",
    "\n",
    "    # 文章全体で名詞が多い場合は無効と判定\n",
    "    try:\n",
    "        if not filter(text, threshold=0.7):\n",
    "            return \"\"\n",
    "    except:\n",
    "        return \"\"\n",
    "    for ending in sentence_endings:\n",
    "        if text.find(ending) > 0:\n",
    "            return text\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def dedup_lines(data, check_length=10):\n",
    "\n",
    "    lines = data.split('\\n')\n",
    "    new_lines = []\n",
    "    old_line = \"\"\n",
    "    for line in lines:\n",
    "        set_a = set(line[:check_length])\n",
    "        set_b = set(old_line[:check_length])\n",
    "        if len(list(set_a-set_b)) < 2:\n",
    "            continue\n",
    "        old_line = line\n",
    "        new_lines.append(line)\n",
    "\n",
    "    result_text = '\\n'.join(new_lines)\n",
    "    return result_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rulebase line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/lighttransport/japanese-llama-experiment/blob/main/03_clean_step1/clean_text.py\n",
    "\n",
    "# TODO: hard codeしない\n",
    "import random\n",
    "import unicodedata\n",
    "import re\n",
    "broken_sentence_endings = \"\"\"\n",
    "...\n",
    "... \n",
    "...　\n",
    "\\\"\n",
    "'\n",
    "[…]\n",
    "詳細を見る\n",
    "すべて表示する\n",
    "するまとめ\n",
    "公式サイト\n",
    "にアクセス!\n",
    "お​問​い​合​わ​せ​\n",
    "アクセスマップ\n",
    "送料無料!\n",
    "コメント(0)\n",
    "トラックバックは利用できません。\n",
    "この記事へのコメントはありません。\n",
    "返信をキャンセルする。\n",
    "URL\n",
    "お問い合わせ\n",
    ",?\n",
    "周辺施設\n",
    "続きを表示\n",
    "音楽出版社求人\n",
    "Check\n",
    "マップMAP\n",
    "未入力項目があります\n",
    "SNS\n",
    "●facebook→\n",
    "●twitter→\n",
    "ABOUT US\n",
    "求人情報\n",
    "1件中1～1件を表示\n",
    "事業内容\n",
    "トップページ\n",
    "特集\n",
    "賃貸情報\n",
    "条件を選ぶ\n",
    "現在の選択エリア\n",
    "沿線から選ぶ\n",
    "エリアを選びなおす\n",
    "さらにエリアを絞る\n",
    "△ページTOP\n",
    "公式\n",
    "About\n",
    "search\n",
    "プロフィール\n",
    "更新情報をチェックする\n",
    "検索:\n",
    "関連記事\n",
    "ブックマークする\n",
    "友達に教える\n",
    "お問い合せ\n",
    "CLOSE\n",
    "My account\n",
    "Contact Us\n",
    "ご利用方法\n",
    "会員規約\n",
    "特定商取引法に基づく表記\n",
    "組織概要\n",
    "<前の20件1次の20件>\n",
    "官公庁臨時職員\n",
    "湘南国際村仕事\n",
    "タイアップ広告掲載\n",
    "広告を掲載しませんか?\n",
    "会社概要\n",
    "Pocket\n",
    "目次\n",
    "TOP\n",
    "から見つける\n",
    "プライバシーポリシー\n",
    "利用規約\n",
    "All Rights Reserved.\n",
    "滞在後投稿されたものです。\n",
    "法令順守と犯罪抑止のために\n",
    "メルカリあんしん・あんぜん宣言!\n",
    "偽ブランド品撲滅への取り組み\n",
    "スタッフブログ\n",
    "賃貸ホームページ\n",
    "物件カタログ\n",
    "ホーム\n",
    "メニュー\n",
    "仲介手数料最大無料\n",
    "口コミをもっと\n",
    "詳細情報\n",
    "ツイート\n",
    "メルマガ\n",
    "登録商標です。\n",
    "ようこそゲストさん\n",
    "→\n",
    "[...]\n",
    "の先頭へ\n",
    "個人情報の取り扱いについて\n",
    "中古マンション\n",
    "»\n",
    "パスワードをお忘れの方\n",
    "上部へ\n",
    "JavaScriptを有効にしてご利用下さい.\n",
    "ログインできない時は?\n",
    "運営会社\n",
    "企業情報\n",
    "問い合わせ\n",
    "一覧へ\n",
    "ヘルプ\n",
    "物件\n",
    "もっと見る\n",
    "を探す\n",
    "マンション\n",
    "検索\n",
    "店\n",
    "検索のヒント:\n",
    "新規の表示\n",
    "Vths\n",
    "Id\n",
    "見出し語\n",
    "Yomi\n",
    "大分類1\n",
    "小分類2\n",
    "Pos\n",
    "Actions\n",
    "メールアドレス\n",
    "パスワード\n",
    "次回から自動ログインをする\n",
    "パスワードを忘れた方はこちら\n",
    "不動産無料査定\n",
    "住まい紹介ブログ\n",
    "🗣naamal\n",
    "⋆。* ⋆。* ⋆\n",
    "122投稿\n",
    "サイトは?\n",
    "トップ\n",
    "ニュース\n",
    "日程\n",
    "著作権\n",
    "記事・写真・動画の利用申込\n",
    "採用情報\n",
    "サイト一覧\n",
    "関連情報\n",
    "の投稿\n",
    "ページトップへ\n",
    "記事を読む\n",
    "運営者情報\n",
    "Twitter公式\n",
    "もっとみる\n",
    "投稿する\n",
    "店舗TOP\n",
    "スタッフ・開発者募集\n",
    "ヘルプページ\n",
    "最近のコメント\n",
    "カレンダー\n",
    "の情報:\n",
    "の情報\n",
    "いいね\n",
    "キャンペーン\n",
    "|BOOKED\n",
    "HOME\n",
    "よくある質問\n",
    "BLOG\n",
    "...\n",
    "問い合わせください。\n",
    "休診日\n",
    "木曜日・日曜日・祝日\n",
    "all rights reserved.\n",
    "</tr>\n",
    "</div>\n",
    "浮気調査\n",
    "カテゴリー\n",
    "お知らせ\n",
    "新着アイテム\n",
    "アーカイブ\n",
    "月\n",
    "Search\n",
    "TOPへ\n",
    "TOPページへ\n",
    "Menu\n",
    "Return Top\n",
    "携帯版はこちら\n",
    "会社情報\n",
    "初めての方へ\n",
    "ダウンロード\n",
    "アクセス\n",
    "IR情報\n",
    "一覧\n",
    "採用サイト\n",
    "マイページ\n",
    "研究会会員\n",
    "専用ページ\n",
    "はこちら\n",
    "相談する\n",
    "相談\n",
    "探す\n",
    "事例を見る\n",
    "お客様からの声(成功事例)\n",
    "書籍\n",
    "メタ情報\n",
    "の詳細\n",
    "Home\n",
    "ログイン\n",
    "投稿フィード\n",
    "コメントフィード\n",
    "船井総研のサービス\n",
    "お客様からの声(成功事例)\n",
    "レポート\n",
    "サービス\n",
    "船井総研のサービス概要\n",
    "セミナー\n",
    "経営研究会\n",
    "コンサルティング\n",
    "業種・テーマ\n",
    "住宅・不動産\n",
    "リフォーム\n",
    "不動産\n",
    "賃貸\n",
    "建設\n",
    "病院・クリニック\n",
    "歯科医院\n",
    "特定商取引法\n",
    "はじめてお越しの方へ\n",
    "LINE\n",
    "menu\n",
    "免責事項\n",
    "閉じる\n",
    "読者になる\n",
    "リンク\n",
    "Tweet\n",
    "方法はコチラ\n",
    "トップへ戻る\n",
    "[ヘルプ]\n",
    "のプロフィール\n",
    "ランキング】\n",
    "更新しました\n",
    "アーカイヴ\n",
    "未分類\n",
    "お問合せ\n",
    "シェア\n",
    "[お問い合わせ]\n",
    "[利用規約]\n",
    "[個人情報保護方針]\n",
    "/RSS\n",
    "もっと読む\n",
    "の検索結果\n",
    "通販ショップ\n",
    "フォローする\n",
    "シェアする\n",
    "MENU\n",
    "このブログについて\n",
    "コメントを書く\n",
    "twitter\n",
    "facebook\n",
    "line\n",
    "コメント\n",
    "arrowup\n",
    "読まれている記事\n",
    "のビュー\n",
    "質問箱\n",
    "コメントをキャンセル\n",
    "down\n",
    "Website\n",
    "more\n",
    "Name\n",
    "Email\n",
    "home\n",
    "新着情報\n",
    "First\n",
    "のQ&A\n",
    "サイトへ\n",
    "Previous\n",
    "上へ\n",
    "更新\n",
    "Twitter\n",
    "拡大\n",
    "ランキング\n",
    "Next\n",
    "会員登録\n",
    "Last\n",
    "スタッフ紹介\n",
    "1階層ページ\n",
    "タイトル\n",
    "この記事を削除する\n",
    "2階層ページ\n",
    "サイド見出し\n",
    "送信する\n",
    "イメージナビ\n",
    "お客様の声・対策事例\n",
    "ページ次のページ »\n",
    "こちらから\n",
    "役立つ文例集\n",
    "お客様の声\n",
    "個人情報保護ポリシー\n",
    "特定商取引について\n",
    "必須お名前\n",
    "申込\n",
    "フォーム\n",
    "FAQ\n",
    "電話する\n",
    "ページ先頭へ\n",
    "送信する\n",
    "天気予報\n",
    "ロフィールを表示\n",
    "リンク集\n",
    "上部へ戻る\n",
    "へスキップ\n",
    "Category\n",
    "ご質問\n",
    "ヤミ金の電話番号\n",
    "最新記事\n",
    "インフォメーション\n",
    "コラム\n",
    "事務所案内\n",
    "サービス料金\n",
    "リストナビ\n",
    "掲示板BBS\n",
    "地図ページ\n",
    "メールフォーム\n",
    "ケータイサイト\n",
    "おすすめの注目情報を掲載。\n",
    "この記事をツイートする\n",
    "自動でツイートされます。\n",
    "コメントをするには、\n",
    "ログインして下さい。\n",
    "ページの先頭へ戻る。\n",
    "page\n",
    "の口コミ\n",
    "メニューにもどる。\n",
    "調べる▼\n",
    "検索結果\n",
    "とURLをコピーしました\n",
    "のご利用について\n",
    "通販サイト\n",
    "無料\n",
    "無料ブログはココログ!\n",
    "パスワードを忘れた方\n",
    "特定商取引法表示\n",
    "note(blog)\n",
    "niconicoへのご意見・ご要望\n",
    "ダウンロードはこちらからどうぞ。\n",
    "下記のお問い合わせフォームでご連絡ください。\n",
    "買い物かごカートに商品がありません。\n",
    "ニュースレターに登録して最新情報やセール情報をいち早くゲット。\n",
    "受信する受信しない\n",
    "JavaScriptを有効にしてご覧ください。\n",
    "このページの機能を利用するにはJavaScriptに対応したブラウザが必要です。\n",
    "分で読めます。\n",
    "無断転載を禁じます。\n",
    "検索条件を入力してください。\n",
    "コメント(0)トラックバックは利用できません。\n",
    "この記事へのコメントはありません。返信をキャンセルする。\n",
    "法令順守と犯罪抑止のためにメルカリあんしん・あんぜん宣言!Facebook\n",
    "Facebook\n",
    "はてなブログを使っています。\n",
    "見つかりました\n",
    "インプラント\n",
    "歯科\n",
    "VIPがお送りします\n",
    "見積\n",
    "ID:\n",
    "PR\n",
    "一覧\n",
    "出会い\n",
    "探せます\n",
    "商品はございません\n",
    "《前のページ|次のページ》\n",
    "RSS\n",
    "人気記事まだデータがありません。\n",
    "投稿が見つかりませんでした。\n",
    "投稿がありません。\n",
    "別のサイトにジャンプしようとしています。\n",
    "お問合せは24時間受け付けております。\n",
    "お気軽にご連絡ください。\n",
    "左の一覧からカテゴリーをお選びください。\n",
    "までを表示します。\n",
    "\n",
    "の人気クチコミ\n",
    "JavaScriptをONにしてください。\n",
    "サンプルです\n",
    "リンクに追加する\n",
    "サイドバー\n",
    "これはテストです\n",
    "のクチコミ\n",
    "サイトについて\n",
    "口コミ・感想!\n",
    "ホーム>\n",
    "Next Entry\n",
    "\"\"\"\n",
    "\n",
    "broken_ending_list = broken_sentence_endings.split(\"\\n\")\n",
    "broken_ending_list = [x for x in broken_ending_list if len(x) > 0]\n",
    "\n",
    "broken_ending_list += [\n",
    "    '続きを読む', '[続きを読む]', '(続きを読む)', '続きを見る', '続きをみる', '(続く)', '(続きを表示)', '(続きをみる)', '[続きをみる]', '[続きを見る]',\n",
    "]\n",
    "broken_ending_list += ['...(続きを表示)', '[ 続きを見る ]',\n",
    "                       '・・・続きを見る', '... 続きを読む',\n",
    "                       \"詳細はこちら »\",\n",
    "                       \"詳細>>>\",\n",
    "                       \"サイトマップ\",\n",
    "                       ]\n",
    "\n",
    "\n",
    "noise_mid_list = \"\"\"\n",
    "Copyright(C)\n",
    "デジタル広告ガイド\n",
    "Copyright\n",
    "HOME>\n",
    "新聞広告ガイド\n",
    "©\n",
    "無断転載を禁止します\n",
    "著作権\n",
    "記事・写真・動画の利用申込\n",
    "採用情報\n",
    "Copyright © \n",
    "Copyright \n",
    "\"\"\"\n",
    "noise_mid_list = noise_mid_list.split(\"\\n\")\n",
    "noise_mid_list = [x for x in noise_mid_list if len(x) > 0]\n",
    "\n",
    "\n",
    "random_mid_list = \"\"\"\n",
    "JavaScriptを\n",
    "ページの先頭\n",
    "登録・解除\n",
    "お問い合わせ等\n",
    "無料メールマガジン\n",
    "配信登録\n",
    "性癖\n",
    "Blog\n",
    "ブログ\n",
    "ここに説明文が入ります\n",
    "資料請求\n",
    "メールですすめる\n",
    "商品の状態\n",
    "未使用\n",
    "エリア・駅がありません\n",
    "見つかりませんでした。\n",
    "無料登録\n",
    "人気記事\n",
    "Top\n",
    "新規登録\n",
    "検索結果\n",
    "当院\n",
    "ホームページ\n",
    "代表挨拶\n",
    "チャンネル登録\n",
    "借金\n",
    "不動産\n",
    "投資\n",
    "カードローン\n",
    "トップページ\n",
    "Podcast\n",
    "検索\n",
    "賃貸\n",
    "物件\n",
    "Views\n",
    "Clicks\n",
    ".com\n",
    "URL\n",
    "更新日\n",
    "配信\n",
    "ランキング\n",
    "徒歩\n",
    "Javascriptの\n",
    "記事一覧\n",
    "ブログ\n",
    "公開中\n",
    "カート\n",
    "このコンテンツ\n",
    "パスワードで保護\n",
    "サイト\n",
    "公開物件\n",
    "ホーム\n",
    "予約\n",
    "商品の\n",
    "お探しの\n",
    "サポートします\n",
    "を掲載!\n",
    "はこちら\n",
    "レス数が1000を超えています\n",
    "現在表示しているスレッド\n",
    "このスレは\n",
    "新着レスの表示\n",
    "掲示板に戻る\n",
    "前100\n",
    "次100\n",
    "最新50\n",
    "Twitter\n",
    "Facebook\n",
    "はてブ\n",
    "Pocket\n",
    "LINE\n",
    "ゲストさん\n",
    "コメント(\n",
    "コメント（\n",
    "が提供\n",
    "限定公開記事\n",
    "本文へ\n",
    "再度アクセス\n",
    "メールでの\n",
    "買い物を続ける\n",
    "査定\n",
    "個人情報保護\n",
    "お買い上げ\n",
    "印刷する\n",
    "会員限定\n",
    "プライバシー保護\n",
    "無料\n",
    "ご案内\n",
    "mail\n",
    "データ取得に失敗しました\n",
    "@\n",
    "このサイト\n",
    "求人\n",
    "タイムライン\n",
    "SHARE\n",
    "日記\n",
    "閲覧\n",
    "trackback\n",
    "(-)\n",
    "|\n",
    "スポンサー広告\n",
    "--------\n",
    "page\n",
    "top\n",
    "Home\n",
    "»\n",
    "Loading.\n",
    "URL\n",
    "[編集]\n",
    "▲\n",
    "《\n",
    ".jp\n",
    "Escキー\n",
    "楽天\n",
    "営業\n",
    "Yahoo!\n",
    "メールアドレス\n",
    "Share\n",
    "フォロー\n",
    "フォロワー\n",
    "Youtube\n",
    "VIEWS\n",
    "購読する\n",
    "最新情報\n",
    "公開日\n",
    "ちゃんねる\n",
    "コンテンツ\n",
    "購入手続\n",
    "Twitter\n",
    "@5ch\n",
    "広告なし\n",
    "周辺施設情報\n",
    "賃貸マンション\n",
    "Cookie\n",
    "QRコード\n",
    "出張\n",
    "承ります\n",
    "お任せください\n",
    "タグ\n",
    "ログイン\n",
    "ユーザー登録\n",
    "トップへ\n",
    "食べログ\n",
    "ご相談ください\n",
    "全国対応\n",
    "記事\n",
    "お会計\n",
    "リンク\n",
    "ツール\n",
    "引用\n",
    "ストック\n",
    "掲載\n",
    "1 2 3 4 5\n",
    "クリック\n",
    "ご注文\n",
    "メッセージ\n",
    "お送りください\n",
    "買い物\n",
    "カート\n",
    "RSS\n",
    "全リスト\n",
    "新着\n",
    "いいね!\n",
    "読む\n",
    "とは?\n",
    "CONTACT\n",
    "エラーが発生しました\n",
    "モバイル\n",
    "お気に入り\n",
    "もぐもぐ\n",
    "利用者情報\n",
    "運営者について\n",
    "クリックで拡大\n",
    "アイコン上\n",
    "マウスカーソル\n",
    "詳細情報\n",
    "TikTok\n",
    "特定商取引法\n",
    "に基づく表示\n",
    "プレイ日記\n",
    "Q&A\n",
    "よくあるご質問と回答\n",
    "トピックス\n",
    "背景色\n",
    "メルマガ\n",
    "(c)\n",
    "javascript\n",
    "dobe Reader\n",
    "カテゴリ\n",
    "ユーザー認証\n",
    "早速見る\n",
    "投稿\n",
    "ナビゲーション\n",
    "印刷\n",
    "SNSでシェアしよう!\n",
    "English\n",
    "お問い合わせ\n",
    "発送\n",
    "業務\n",
    "休日\n",
    "状態\n",
    "スペースキー\n",
    "矢印キー\n",
    "選択します\n",
    "お客様各位\n",
    "キャンペーン\n",
    "お気に入り\n",
    "探し方を変更する\n",
    "掲示板\n",
    "買い物を続ける\n",
    "選んでください\n",
    "配達日\n",
    "選択してください\n",
    "お電話\n",
    "コメント\n",
    "非公開\n",
    "利用特定商取引法\n",
    "基づく表示規約\n",
    "お役立ち\n",
    "個人情報の取扱\n",
    "同意\n",
    "Contact\n",
    "URL\n",
    "http\n",
    "shares\n",
    "ニュースレター\n",
    "結果を見る\n",
    "免責\n",
    "PICK UP!\n",
    "最近の記事\n",
    "New!\n",
    "Tweets\n",
    "＝＝t?\n",
    "をフォロー\n",
    "お支払い\n",
    "お選びいただけます。\n",
    "ツイートする\n",
    "すべて表示\n",
    "お問合わせ\n",
    "お客様\n",
    "入会案内\n",
    "事業全般\n",
    "メールフォーム\n",
    "カテゴリー\n",
    "チェック\n",
    "商品\n",
    "お使いのブラウザ\n",
    "JavaScript\n",
    "店舗\n",
    "登録\n",
    "一覧\n",
    "戻る\n",
    "サービス\n",
    "紹介\n",
    "ページ\n",
    "管理\n",
    "送料\n",
    "ショッピング\n",
    "クレジットカード\n",
    "ご利用\n",
    "ガイド\n",
    "お伝えします\n",
    "ご紹介\n",
    "お部屋探し\n",
    "限定公開記事\n",
    "再度お試し\n",
    "利用規約\n",
    "プライバシー\n",
    "同意します\n",
    "整骨院\n",
    "クリニック\n",
    "方針\n",
    "Instagram\n",
    "スマートフォン\n",
    "ポリシー\n",
    "contact\n",
    "profile\n",
    "void\n",
    "会社案内\n",
    "詳しく見る\n",
    "ソーシャルメディア\n",
    "1件\n",
    "病院への地図\n",
    "無料相談\n",
    "文字を入力\n",
    "過去の記事\n",
    "メニューを閉じる\n",
    "▶\n",
    "Social\n",
    "サイト\n",
    "物件\n",
    "メルカリ\n",
    "ポイント\n",
    "買取\n",
    "ツイート\n",
    "\"\"\"\n",
    "random_mid_list = random_mid_list.split(\"\\n\")\n",
    "random_mid_list = [x for x in random_mid_list if len(x) > 0]\n",
    "random_mid_list = list(set(random_mid_list))\n",
    "\n",
    "\n",
    "def clean(sent: str):\n",
    "    for broken_ending in broken_ending_list:\n",
    "        # print(\"aa\", broken_ending)\n",
    "        if sent.endswith(broken_ending):\n",
    "            return None\n",
    "        if len(sent) < 2:\n",
    "            return None\n",
    "\n",
    "    for noise_mid in noise_mid_list:\n",
    "        if noise_mid in sent:\n",
    "            return None\n",
    "\n",
    "    # インターネットに特有の単語を含む文章を確率的に落とす\n",
    "    for random_mid in random_mid_list:\n",
    "        if random_mid in sent:\n",
    "            if random.random() < 0.9:\n",
    "                return None\n",
    "\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import functools\n",
    "from ja_sentence_segmenter.common.pipeline import make_pipeline\n",
    "from ja_sentence_segmenter.concatenate.simple_concatenator import concatenate_matching\n",
    "from ja_sentence_segmenter.normalize.neologd_normalizer import normalize\n",
    "from ja_sentence_segmenter.split.simple_splitter import split_newline, split_punctuation\n",
    "\n",
    "\n",
    "def text_to_paragraph_sentences(text: str) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Split a text into paragraphs and sentences.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to split.\n",
    "\n",
    "    Returns:\n",
    "        List[List[str]]: The paragraphs and sentences.\n",
    "    \"\"\"\n",
    "    paragraphs = paragraph_split(text)\n",
    "    return [sentence_split(paragraph) for paragraph in paragraphs]\n",
    "\n",
    "\n",
    "def paragraph_split(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split a text into paragraphs.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to split.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: The paragraphs.\n",
    "    \"\"\"\n",
    "    return text.split(\"\\n\")\n",
    "\n",
    "\n",
    "split_punc2 = functools.partial(split_punctuation, punctuations=r\"。!?\")\n",
    "concat_tail_te = functools.partial(\n",
    "    concatenate_matching, remove_former_matched=False)\n",
    "segmenter = make_pipeline(normalize, split_newline,\n",
    "                          concat_tail_te, split_punc2)\n",
    "\n",
    "\n",
    "def sentence_split(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split a text into sentences.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to split.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: The sentences.\n",
    "    \"\"\"\n",
    "    return list(segmenter(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/fairy_hdd4/takumi/matsuo_llm/Dataset_for_BTM/01web_codes/src/cleaner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msplitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text_to_paragraph_sentences\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_normalizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text_checker\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/data/fairy_hdd4/takumi/matsuo_llm/Dataset_for_BTM/01web_codes/src/cleaner\")\n",
    "from .splitter import text_to_paragraph_sentences\n",
    "# from .text_normalizer import normalize\n",
    "#from . import text_checker\n",
    "from . import rule_based_line_checker\n",
    "# from . import parts_filter\n",
    "from .line_end_cleaner import clean_line_endings\n",
    "#from .hojichar_filter import hoji_filter, prob_hoji_filter, prob_filter\n",
    "#from . import rule_based_text_checker\n",
    "from .line_dedup import remove_multi_headers\n",
    "#from . import repeated_phrase\n",
    "try:\n",
    "    from .TextClassifier import TextClassifier\n",
    "    classifier = TextClassifier()\n",
    "except:\n",
    "    print(\"error loading TextClassifier. install fasttext to use it\")\n",
    "\n",
    "def text_to_cleaned_paragraphs(text):\n",
    "    text = normalize(text)  # 正規化\n",
    "    # text = text_checker.check(text)  # ひらがなを含まないテキストは除外\n",
    "\n",
    "    # 繰り返し表現を除外 by Naito\n",
    "    text = repeated_phrase.repeated_id(text)\n",
    "    text = repeated_phrase.is_repetitive_japanese(\n",
    "        text)  # n-gramの計算(計算量が多そうな場合、削る)\n",
    "\n",
    "    # パラグラフと文章に分割\n",
    "    paragraphs = text_to_paragraph_sentences(text)\n",
    "\n",
    "    new_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        new_lines = []\n",
    "        old_line = \"\"\n",
    "        for line in (paragraph):\n",
    "            # ルールベースの行チェック\n",
    "            new_line = rule_based_line_checker.clean(line)\n",
    "\n",
    "            # 名詞だらけのlineを除外\n",
    "            try:\n",
    "                # new_line = parts_filter.filter(new_line)\n",
    "                new_line = parts_filter.filter2(new_line)  # n-gramによる重複の除外\n",
    "            except:\n",
    "                pass\n",
    "            if new_line:\n",
    "                if new_line == old_line:\n",
    "                    continue\n",
    "                old_line = new_line\n",
    "                # ppl=perp_checker(new_line)\n",
    "                # print(ppl,new_line)\n",
    "                new_lines.append(new_line)\n",
    "\n",
    "        if new_lines:\n",
    "            new_paragraphs.append(new_lines)\n",
    "\n",
    "    # 文末が｡などでおわらないパラグラフ中の文章を削除\n",
    "    # clean_line_endings(new_paragraphs)\n",
    "\n",
    "    # パラグラフにまとめる\n",
    "    cleaned_paragraphs = []\n",
    "    old_lines = \"\"\n",
    "    for paragraph in new_paragraphs:\n",
    "        lines = \"\".join(paragraph)\n",
    "        if lines == old_lines:\n",
    "            continue\n",
    "        cleaned_paragraphs.append(lines)\n",
    "        old_lines = lines\n",
    "\n",
    "    return cleaned_paragraphs\n",
    "\n",
    "\n",
    "def clean_text(text, hoji=True):\n",
    "    if hoji:\n",
    "        # text = prob_filter(text)\n",
    "        text = hoji_filter(text)\n",
    "        text = prob_hoji_filter(text)\n",
    "\n",
    "    paragraphs = text_to_cleaned_paragraphs(text)\n",
    "    # print(\"aa\", original_text)\n",
    "    text = \"\\n\".join(paragraphs)\n",
    "\n",
    "    text = remove_multi_headers(text)\n",
    "    text = rule_based_text_checker.clean(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def ml_clean_text(text):\n",
    "    text = prob_hoji_filter(text)\n",
    "    text = hoji_filter(text)\n",
    "    text = classifier.clean(text)\n",
    "    if text != \"\":\n",
    "        text = clean_text(text, hoji=False)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### line dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_symbols = [\n",
    "    \"(\",\n",
    "    \"[\",\n",
    "    \"{\",\n",
    "    \"（\",\n",
    "    \"［\",\n",
    "    \"｛\",\n",
    "    \"〈\",\n",
    "    \"《\",\n",
    "    \"「\",\n",
    "    \"『\",\n",
    "    \"【\",\n",
    "    \"〔\",\n",
    "    \"・\",\n",
    "    \"、\",\n",
    "    \",\",\n",
    "    \"，\",\n",
    "]\n",
    "\n",
    "end_symbols = [\n",
    "    \")\",\n",
    "    \"]\",\n",
    "    \"}\",\n",
    "    \"）\",\n",
    "    \"］\",\n",
    "    \"｝\",\n",
    "    \"〉\",\n",
    "    \"》\",\n",
    "    \"」\",\n",
    "    \"』\",\n",
    "    \"】\",\n",
    "    \"〕\",\n",
    "]\n",
    "\n",
    "\n",
    "def is_end_with_begin_symbol(sent):\n",
    "    for s in begin_symbols:\n",
    "        if sent.endswith(s):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_start_with_end_symbol(sent):\n",
    "    for s in end_symbols:\n",
    "        if sent.startswith(s):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_sentence_end(sent):\n",
    "    end_symbols = [\".\", \"。\", \"!\", \"！\", \"?\", \"？\", \"ました\", \"ます\",\n",
    "                   \"れる\", \"する\", \"すよ\", \"です\", \"」\", \"』\", \"さい\",]\n",
    "    for s in end_symbols:\n",
    "        if sent.endswith(s):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def remove_multi_headers(new_texts):\n",
    "    \"\"\"\n",
    "    見出し1\n",
    "    見出し2\n",
    "    見出し3\n",
    "    こんにちは｡\n",
    "\n",
    "    →\n",
    "    見出し3\n",
    "    こんにちは｡\n",
    "\n",
    "    にする\n",
    "    \"\"\"\n",
    "    new_texts = new_texts.split(\"\\n\")\n",
    "    new_texts = remove_dup_lines(new_texts)\n",
    "    cleaned_texts = []\n",
    "    cap_line = \"\"\n",
    "    for line in new_texts[::-1]:\n",
    "        if not is_sentence_end(line):\n",
    "            # print(\"line\",line)\n",
    "            if cap_line == \"\":\n",
    "                cap_line = line\n",
    "                # print(\"cap_line\",cap_line)\n",
    "\n",
    "        else:\n",
    "            if cap_line != \"\":\n",
    "                cleaned_texts.append(cap_line)\n",
    "                cap_line = \"\"\n",
    "\n",
    "            cleaned_texts.append(line)\n",
    "    cleaned_texts.append(cap_line)\n",
    "    cleaned_texts = cleaned_texts[::-1]\n",
    "    cleaned_texts = [i.strip() for i in cleaned_texts]\n",
    "    cleaned_texts = \"\\n\".join(cleaned_texts)\n",
    "    return cleaned_texts\n",
    "\n",
    "\n",
    "def remove_dup_lines(lines, dup_n_threshold=100):\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        # line=line[0].strip()\n",
    "\n",
    "        # 行の重複を避ける\n",
    "        if line in new_lines[-dup_n_threshold:]:\n",
    "            continue\n",
    "        new_lines.append(line)\n",
    "\n",
    "    return new_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### line end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentence_endings = ['。', '！', '？', '.', '!', '?', \"．\", \"」\"]\n",
    "\n",
    "\n",
    "def clean_line_endings(paragraphs):\n",
    "    \"\"\"文末記号以外の文字を削除する\"\"\"\n",
    "    for paragraph in paragraphs:\n",
    "        if len(paragraph) < 2:\n",
    "            continue\n",
    "        for line in paragraph:\n",
    "            if line[-1] not in sentence_endings:\n",
    "                paragraph.remove(line)\n",
    "                # print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warcio.archiveiterator import ArchiveIterator\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from trafilatura import fetch_url, extract\n",
    "\n",
    "def halfwidth_ratio(s):\n",
    "    if len(s) == 0:  # 空の文字列の場合は0を返す\n",
    "        return 0\n",
    "    halfwidth_count = sum(\n",
    "        1 for char in s\n",
    "        if '\\u0020' <= char <= '\\u007E' or  # 基本的なASCII範囲\n",
    "           '\\uFF61' <= char <= '\\uFF9F' or  # 半角カタカナ\n",
    "           char in ('\\u0009', '\\u000A', '\\u000D')  # タブ、改行、復帰\n",
    "    )\n",
    "    return halfwidth_count / len(s)\n",
    "\n",
    "\n",
    "def pre_clean(soup):\n",
    "    texts_with_tags = []\n",
    "    for tag in soup.find_all(True):\n",
    "        # 特定のタグを除外する場合\n",
    "        # if tag.name not in ['html', 'body', 'ul']:\n",
    "        text = tag.get_text(separator=\"\\n\", strip=True)\n",
    "        spl_text = text.split(\"\\n\")\n",
    "        spl_text = [i.strip() for i in spl_text if i.strip()]  # 空の文字列を除外\n",
    "        for item in spl_text:\n",
    "            if tag.name == \"script\" or tag.name == \"style\":\n",
    "                continue\n",
    "            texts_with_tags.append((item, tag.name))  # テキストとタグの名前をタプルとして追加\n",
    "    return texts_with_tags\n",
    "\n",
    "\n",
    "def extract_japanese_from_warc(path,\n",
    "                               save_dir=\"json\",\n",
    "                               max_num=10**10,\n",
    "                               ):\n",
    "    ja_soup_list = []\n",
    "    path = path.replace(\"\\\\\", \"/\")  # for windows env\n",
    "    filename = path.split(\"/\")[-1].replace(\".warc\", \".json\")\n",
    "    if os.path.exists(f\"{save_dir}/{filename}\"):\n",
    "        print(\"already done\")\n",
    "        return\n",
    "    # 途中から再開する用の位置情報の取得\n",
    "    if len(ja_soup_list) > 0:\n",
    "        fin_record_id = ja_soup_list[-1][\"record_id\"]\n",
    "    else:\n",
    "        fin_record_id = 0\n",
    "    # WARCファイルを開く\n",
    "    record_id = 0\n",
    "    with open(path, 'rb') as stream:\n",
    "        for record in tqdm(ArchiveIterator(stream)):\n",
    "            record_id += 1\n",
    "            if record_id <= fin_record_id:\n",
    "                continue\n",
    "            if record.rec_type == 'response':\n",
    "                if record.http_headers.get_header('Content-Type') == 'text/html':\n",
    "                    content = record.content_stream().read()\n",
    "                    soup = BeautifulSoup(content, 'html.parser')\n",
    "                    # <html>タグからlang属性を取得\n",
    "                    html_tag = soup.find('html')\n",
    "                    if html_tag and html_tag.has_attr('lang'):\n",
    "                        lang = html_tag['lang']\n",
    "                        texts = pre_clean(soup)\n",
    "\n",
    "                        if len(texts) == 0:\n",
    "                            continue\n",
    "                        if lang == \"ja\":\n",
    "                            if soup.title is not None:\n",
    "                                title = soup.title.string\n",
    "                            else:\n",
    "                                title = \"\"\n",
    "                            texts = extract(content, include_tables=False,target_lang='ja',favour_precision=True) #trafilaturaでテキスト抽出\n",
    "                            print(texts)\n",
    "                            d = {\n",
    "                                \"record_id\": record_id,\n",
    "                                \"url\": record.rec_headers.get_header('WARC-Target-URI'),\n",
    "                                \"title\": title,\n",
    "                                \"timestamp\": record.rec_headers.get_header('WARC-Date'),\n",
    "                                \"text\": texts,\n",
    "                            }\n",
    "                            ja_soup_list.append(d)\n",
    "                        if len(ja_soup_list) > max_num:\n",
    "                            break\n",
    "    return ja_soup_list\n",
    "\n",
    "def download_and_parse(cc_path, base_dir=None):\n",
    "    # warcファイルのダウンロード\n",
    "    warc_path = download_warc_file(cc_path)\n",
    "    # ファイル関連の処理\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    # パス関連の処理\n",
    "    file_name = os.path.basename(warc_path)\n",
    "    base_name = os.path.splitext(file_name)[0]\n",
    "    file_base_name = \"_\".join(base_name.split(\"_\")[2:])\n",
    "    if base_dir is None:\n",
    "        base_dir = \"/tmp/\"\n",
    "    save_gz_path = f\"{base_dir}/{file_base_name}_japanese.json.gz\"\n",
    "    try:\n",
    "        tag_records = extract_japanese_from_warc(warc_path)\n",
    "        is_error = False\n",
    "        error_text = \"\"\n",
    "    except Exception as e:\n",
    "        tag_records = []\n",
    "        is_error = True\n",
    "        print(e)\n",
    "        error_text = str(e)\n",
    "    # 保存用のdictを作製\n",
    "    save_dict = {\n",
    "      \"tag_records\" : tag_records,\n",
    "      \"is_error\" : is_error,\n",
    "      \"cc_path\" : cc_path,\n",
    "      \"warc_path\" : warc_path,\n",
    "      \"error_text\" : error_text\n",
    "    }\n",
    "    with gzip.open(save_gz_path, 'wt', encoding=\"utf-8\") as zipfile:\n",
    "       json.dump(save_dict, zipfile, indent=2, ensure_ascii=False)\n",
    "    return\n",
    "\n",
    "def curation(batch_number, submit_dir=\"/content/submit\", is_debug=False):\n",
    "    cc_path_list = get_cc_path_list()\n",
    "    if is_debug:\n",
    "        n_batch = 3\n",
    "    else:\n",
    "        n_batch = 10\n",
    "    start_idx, end_idx = batch_number * n_batch, (batch_number+1) * n_batch\n",
    "    target_path_list  = cc_path_list[start_idx:end_idx]\n",
    "    for cc_path in tqdm(target_path_list):\n",
    "        download_and_parse(cc_path, f\"process/batch{batch_number}\")\n",
    "    shutil.make_archive(f'{submit_dir}/{batch_number}',\n",
    "                        format='zip', root_dir=f\"process/batch{batch_number}\")\n",
    "\n",
    "    shutil.rmtree(\"process/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 整理する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/fairy_hdd4/takumi/matsuo_llm/Dataset_for_BTM/01web_codes/src/cleaner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msplitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text_to_paragraph_sentences\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_normalizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text_checker\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/data/fairy_hdd4/takumi/matsuo_llm/Dataset_for_BTM/01web_codes/src/cleaner\")\n",
    "from .splitter import text_to_paragraph_sentences\n",
    "from .text_normalizer import normalize\n",
    "from . import text_checker\n",
    "from . import rule_based_line_checker\n",
    "from . import parts_filter\n",
    "from .line_end_cleaner import clean_line_endings\n",
    "from .hojichar_filter import hoji_filter, prob_hoji_filter, prob_filter\n",
    "from . import rule_based_text_checker\n",
    "from .line_dedup import remove_multi_headers\n",
    "from . import repeated_phrase\n",
    "\n",
    "# クラシファイアのインポートと初期化\n",
    "try:\n",
    "    from .TextClassifier import TextClassifier\n",
    "    classifier = TextClassifier()\n",
    "except:\n",
    "    print(\"error loading TextClassifier. install fasttext to use it\")\n",
    "\n",
    "def process_line(line):\n",
    "    \"\"\"行の清掃とフィルタリングを行う\"\"\"\n",
    "    new_line = rule_based_line_checker.clean(line)\n",
    "    new_line = parts_filter.filter2(new_line)  # n-gramによる重複の除外\n",
    "    return new_line\n",
    "\n",
    "def process_paragraph(paragraph):\n",
    "    \"\"\"パラグラフ内の各行を処理し、清掃されたパラグラフを返す\"\"\"\n",
    "    new_lines = []\n",
    "    old_line = \"\"\n",
    "    for line in paragraph:\n",
    "        new_line = process_line(line)\n",
    "        if new_line and new_line != old_line:\n",
    "            new_lines.append(new_line)\n",
    "            old_line = new_line\n",
    "    return new_lines\n",
    "\n",
    "def clean_paragraphs(paragraphs):\n",
    "    \"\"\"パラグラフのリストを受け取り、重複や不適切な行を除外して再構成する\"\"\"\n",
    "    new_paragraphs = [process_paragraph(paragraph) for paragraph in paragraphs]\n",
    "    new_paragraphs = [\"\".join(paragraph) for paragraph in new_paragraphs if paragraph]\n",
    "    return new_paragraphs\n",
    "\n",
    "def text_to_cleaned_paragraphs(text):\n",
    "    \"\"\"テキストを正規化し、適切にパラグラフに分割して清掃する\"\"\"\n",
    "    text = normalize(text)  # 正規化\n",
    "    text = repeated_phrase.repeated_id(text)  # 繰り返し表現を除外\n",
    "    paragraphs = text_to_paragraph_sentences(text)  # パラグラフと文章に分割\n",
    "    cleaned_paragraphs = clean_paragraphs(paragraphs)  # パラグラフを清掃\n",
    "    return cleaned_paragraphs\n",
    "\n",
    "def clean_text(text, hoji=True):\n",
    "    \"\"\"テキスト全体の清掃を行い、整形されたテキストを返す\"\"\"\n",
    "    if hoji:\n",
    "        text = hoji_filter(text)\n",
    "        text = prob_hoji_filter(text)\n",
    "    paragraphs = text_to_cleaned_paragraphs(text)\n",
    "    text = \"\\n\".join(paragraphs)\n",
    "    text = remove_multi_headers(text)\n",
    "    text = rule_based_text_checker.clean(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '...\\u3000', '\"', \"'\", '[…]', '詳細を見る', 'すべて表示する', 'するまとめ', '公式サイト', 'にアクセス!', 'お\\u200b問\\u200bい\\u200b合\\u200bわ\\u200bせ\\u200b', 'アクセスマップ', '送料無料!', 'コメント(0)', 'トラックバックは利用できません。', 'この記事へのコメントはありません。', '返信をキャンセルする。', 'URL', 'お問い合わせ', ',?', '周辺施設', '続きを表示', '音楽出版社求人', 'Check', 'マップMAP', '未入力項目があります', 'SNS', '●facebook→', '●twitter→', 'ABOUT US', '求人情報', '1件中1～1件を表示', '事業内容', 'トップページ', '特集', '賃貸情報', '条件を選ぶ', '現在の選択エリア', '沿線から選ぶ', 'エリアを選びなおす', 'さらにエリアを絞る', '△ページTOP', '公式', 'About', 'search', 'プロフィール', '更新情報をチェックする', '検索:', '関連記事', 'ブックマークする', '友達に教える', 'お問い合せ', 'CLOSE', 'My account', 'Contact Us', 'ご利用方法', '会員規約', '特定商取引法に基づく表記', '組織概要', '<前の20件1次の20件>', '官公庁臨時職員', '湘南国際村仕事', 'タイアップ広告掲載', '広告を掲載しませんか?', '会社概要', 'Pocket', '目次', 'TOP', 'から見つける', 'プライバシーポリシー', '利用規約', 'All Rights Reserved.', '滞在後投稿されたものです。', '法令順守と犯罪抑止のために', 'メルカリあんしん・あんぜん宣言!', '偽ブランド品撲滅への取り組み', 'スタッフブログ', '賃貸ホームページ', '物件カタログ', 'ホーム', 'メニュー', '仲介手数料最大無料', '口コミをもっと', '詳細情報', 'ツイート', 'メルマガ', '登録商標です。', 'ようこそゲストさん', '→', '[...]', 'の先頭へ', '個人情報の取り扱いについて', '中古マンション', '»', 'パスワードをお忘れの方', '上部へ', 'JavaScriptを有効にしてご利用下さい.', 'ログインできない時は?', '運営会社', '企業情報', '問い合わせ', '一覧へ', 'ヘルプ', '物件', 'もっと見る', 'を探す', 'マンション', '検索', '店', '検索のヒント:', '新規の表示', 'Vths', 'Id', '見出し語', 'Yomi', '大分類1', '小分類2', 'Pos', 'Actions', 'メールアドレス', 'パスワード', '次回から自動ログインをする', 'パスワードを忘れた方はこちら', '不動産無料査定', '住まい紹介ブログ', '🗣naamal', '⋆。* ⋆。* ⋆', '122投稿', 'サイトは?', 'トップ', 'ニュース', '日程', '著作権', '記事・写真・動画の利用申込', '採用情報', 'サイト一覧', '関連情報', 'の投稿', 'ページトップへ', '記事を読む', '運営者情報', 'Twitter公式', 'もっとみる', '投稿する', '店舗TOP', 'スタッフ・開発者募集', 'ヘルプページ', '最近のコメント', 'カレンダー', 'の情報:', 'の情報', 'いいね', 'キャンペーン', '|BOOKED', 'HOME', 'よくある質問', 'BLOG', '', '問い合わせください。', '休診日', '木曜日・日曜日・祝日', 'all rights reserved.', '</tr>', '</div>', '浮気調査', 'カテゴリー', 'お知らせ', '新着アイテム', 'アーカイブ', '月', 'Search', 'TOPへ', 'TOPページへ', 'Menu', 'Return Top', '携帯版はこちら', '会社情報', '初めての方へ', 'ダウンロード', 'アクセス', 'IR情報', '一覧', '採用サイト', 'マイページ', '研究会会員', '専用ページ', 'はこちら', '相談する', '相談', '探す', '事例を見る', 'お客様からの声(成功事例)', '書籍', 'メタ情報', 'の詳細', 'Home', 'ログイン', '投稿フィード', 'コメントフィード', '船井総研のサービス', 'お客様からの声(成功事例)', 'レポート', 'サービス', '船井総研のサービス概要', 'セミナー', '経営研究会', 'コンサルティング', '業種・テーマ', '住宅・不動産', 'リフォーム', '不動産', '賃貸', '建設', '病院・クリニック', '歯科医院', '特定商取引法', 'はじめてお越しの方へ', 'LINE', 'menu', '免責事項', '閉じる', '読者になる', 'リンク', 'Tweet', '方法はコチラ', 'トップへ戻る', '[ヘルプ]', 'のプロフィール', 'ランキング】', '更新しました', 'アーカイヴ', '未分類', 'お問合せ', 'シェア', '[お問い合わせ]', '[利用規約]', '[個人情報保護方針]', '/RSS', 'もっと読む', 'の検索結果', '通販ショップ', 'フォローする', 'シェアする', 'MENU', 'このブログについて', 'コメントを書く', 'twitter', 'facebook', 'line', 'コメント', 'arrowup', '読まれている記事', 'のビュー', '質問箱', 'コメントをキャンセル', 'down', 'Website', 'more', 'Name', 'Email', 'home', '新着情報', 'First', 'のQ&A', 'サイトへ', 'Previous', '上へ', '更新', 'Twitter', '拡大', 'ランキング', 'Next', '会員登録', 'Last', 'スタッフ紹介', '1階層ページ', 'タイトル', 'この記事を削除する', '2階層ページ', 'サイド見出し', '送信する', 'イメージナビ', 'お客様の声・対策事例', 'ページ次のページ »', 'こちらから', '役立つ文例集', 'お客様の声', '個人情報保護ポリシー', '特定商取引について', '必須お名前', '申込', 'フォーム', 'FAQ', '電話する', 'ページ先頭へ', '送信する', '天気予報', 'ロフィールを表示', 'リンク集', '上部へ戻る', 'へスキップ', 'Category', 'ご質問', 'ヤミ金の電話番号', '最新記事', 'インフォメーション', 'コラム', '事務所案内', 'サービス料金', 'リストナビ', '掲示板BBS', '地図ページ', 'メールフォーム', 'ケータイサイト', 'おすすめの注目情報を掲載。', 'この記事をツイートする', '自動でツイートされます。', 'コメントをするには、', 'ログインして下さい。', 'ページの先頭へ戻る。', 'page', 'の口コミ', 'メニューにもどる。', '調べる▼', '検索結果', 'とURLをコピーしました', 'のご利用について', '通販サイト', '無料', '無料ブログはココログ!', 'パスワードを忘れた方', '特定商取引法表示', 'note(blog)', 'niconicoへのご意見・ご要望', 'ダウンロードはこちらからどうぞ。', '下記のお問い合わせフォームでご連絡ください。', '買い物かごカートに商品がありません。', 'ニュースレターに登録して最新情報やセール情報をいち早くゲット。', '受信する受信しない', 'JavaScriptを有効にしてご覧ください。', 'このページの機能を利用するにはJavaScriptに対応したブラウザが必要です。', '分で読めます。', '無断転載を禁じます。', '検索条件を入力してください。', 'コメント(0)トラックバックは利用できません。', 'この記事へのコメントはありません。返信をキャンセルする。', '法令順守と犯罪抑止のためにメルカリあんしん・あんぜん宣言!Facebook', 'Facebook', 'はてなブログを使っています。', '見つかりました', 'インプラント', '歯科', 'VIPがお送りします', '見積', 'ID:', 'PR', '一覧', '出会い', '探せます', '商品はございません', '《前のページ|次のページ》', 'RSS', '人気記事まだデータがありません。', '投稿が見つかりませんでした。', '投稿がありません。', '別のサイトにジャンプしようとしています。', 'お問合せは24時間受け付けております。', 'お気軽にご連絡ください。', '左の一覧からカテゴリーをお選びください。', 'までを表示します。', '', 'の人気クチコミ', 'JavaScriptをONにしてください。', 'サンプルです', 'リンクに追加する', 'サイドバー', 'これはテストです', 'のクチコミ', 'サイトについて', '口コミ・感想!', 'ホーム>', 'Next Entry', '']\n",
      "['...\\u3000', '\"', \"'\", '[…]', '詳細を見る', 'すべて表示する', 'するまとめ', '公式サイト', 'にアクセス!', 'お\\u200b問\\u200bい\\u200b合\\u200bわ\\u200bせ\\u200b', 'アクセスマップ', '送料無料!', 'コメント(0)', 'トラックバックは利用できません。', 'この記事へのコメントはありません。', '返信をキャンセルする。', 'URL', 'お問い合わせ', ',?', '周辺施設', '続きを表示', '音楽出版社求人', 'Check', 'マップMAP', '未入力項目があります', 'SNS', '●facebook→', '●twitter→', 'ABOUT US', '求人情報', '1件中1～1件を表示', '事業内容', 'トップページ', '特集', '賃貸情報', '条件を選ぶ', '現在の選択エリア', '沿線から選ぶ', 'エリアを選びなおす', 'さらにエリアを絞る', '△ページTOP', '公式', 'About', 'search', 'プロフィール', '更新情報をチェックする', '検索:', '関連記事', 'ブックマークする', '友達に教える', 'お問い合せ', 'CLOSE', 'My account', 'Contact Us', 'ご利用方法', '会員規約', '特定商取引法に基づく表記', '組織概要', '<前の20件1次の20件>', '官公庁臨時職員', '湘南国際村仕事', 'タイアップ広告掲載', '広告を掲載しませんか?', '会社概要', 'Pocket', '目次', 'TOP', 'から見つける', 'プライバシーポリシー', '利用規約', 'All Rights Reserved.', '滞在後投稿されたものです。', '法令順守と犯罪抑止のために', 'メルカリあんしん・あんぜん宣言!', '偽ブランド品撲滅への取り組み', 'スタッフブログ', '賃貸ホームページ', '物件カタログ', 'ホーム', 'メニュー', '仲介手数料最大無料', '口コミをもっと', '詳細情報', 'ツイート', 'メルマガ', '登録商標です。', 'ようこそゲストさん', '→', '[...]', 'の先頭へ', '個人情報の取り扱いについて', '中古マンション', '»', 'パスワードをお忘れの方', '上部へ', 'JavaScriptを有効にしてご利用下さい.', 'ログインできない時は?', '運営会社', '企業情報', '問い合わせ', '一覧へ', 'ヘルプ', '物件', 'もっと見る', 'を探す', 'マンション', '検索', '店', '検索のヒント:', '新規の表示', 'Vths', 'Id', '見出し語', 'Yomi', '大分類1', '小分類2', 'Pos', 'Actions', 'メールアドレス', 'パスワード', '次回から自動ログインをする', 'パスワードを忘れた方はこちら', '不動産無料査定', '住まい紹介ブログ', '🗣naamal', '⋆。* ⋆。* ⋆', '122投稿', 'サイトは?', 'トップ', 'ニュース', '日程', '著作権', '記事・写真・動画の利用申込', '採用情報', 'サイト一覧', '関連情報', 'の投稿', 'ページトップへ', '記事を読む', '運営者情報', 'Twitter公式', 'もっとみる', '投稿する', '店舗TOP', 'スタッフ・開発者募集', 'ヘルプページ', '最近のコメント', 'カレンダー', 'の情報:', 'の情報', 'いいね', 'キャンペーン', '|BOOKED', 'HOME', 'よくある質問', 'BLOG', '問い合わせください。', '休診日', '木曜日・日曜日・祝日', 'all rights reserved.', '</tr>', '</div>', '浮気調査', 'カテゴリー', 'お知らせ', '新着アイテム', 'アーカイブ', '月', 'Search', 'TOPへ', 'TOPページへ', 'Menu', 'Return Top', '携帯版はこちら', '会社情報', '初めての方へ', 'ダウンロード', 'アクセス', 'IR情報', '一覧', '採用サイト', 'マイページ', '研究会会員', '専用ページ', 'はこちら', '相談する', '相談', '探す', '事例を見る', 'お客様からの声(成功事例)', '書籍', 'メタ情報', 'の詳細', 'Home', 'ログイン', '投稿フィード', 'コメントフィード', '船井総研のサービス', 'お客様からの声(成功事例)', 'レポート', 'サービス', '船井総研のサービス概要', 'セミナー', '経営研究会', 'コンサルティング', '業種・テーマ', '住宅・不動産', 'リフォーム', '不動産', '賃貸', '建設', '病院・クリニック', '歯科医院', '特定商取引法', 'はじめてお越しの方へ', 'LINE', 'menu', '免責事項', '閉じる', '読者になる', 'リンク', 'Tweet', '方法はコチラ', 'トップへ戻る', '[ヘルプ]', 'のプロフィール', 'ランキング】', '更新しました', 'アーカイヴ', '未分類', 'お問合せ', 'シェア', '[お問い合わせ]', '[利用規約]', '[個人情報保護方針]', '/RSS', 'もっと読む', 'の検索結果', '通販ショップ', 'フォローする', 'シェアする', 'MENU', 'このブログについて', 'コメントを書く', 'twitter', 'facebook', 'line', 'コメント', 'arrowup', '読まれている記事', 'のビュー', '質問箱', 'コメントをキャンセル', 'down', 'Website', 'more', 'Name', 'Email', 'home', '新着情報', 'First', 'のQ&A', 'サイトへ', 'Previous', '上へ', '更新', 'Twitter', '拡大', 'ランキング', 'Next', '会員登録', 'Last', 'スタッフ紹介', '1階層ページ', 'タイトル', 'この記事を削除する', '2階層ページ', 'サイド見出し', '送信する', 'イメージナビ', 'お客様の声・対策事例', 'ページ次のページ »', 'こちらから', '役立つ文例集', 'お客様の声', '個人情報保護ポリシー', '特定商取引について', '必須お名前', '申込', 'フォーム', 'FAQ', '電話する', 'ページ先頭へ', '送信する', '天気予報', 'ロフィールを表示', 'リンク集', '上部へ戻る', 'へスキップ', 'Category', 'ご質問', 'ヤミ金の電話番号', '最新記事', 'インフォメーション', 'コラム', '事務所案内', 'サービス料金', 'リストナビ', '掲示板BBS', '地図ページ', 'メールフォーム', 'ケータイサイト', 'おすすめの注目情報を掲載。', 'この記事をツイートする', '自動でツイートされます。', 'コメントをするには、', 'ログインして下さい。', 'ページの先頭へ戻る。', 'page', 'の口コミ', 'メニューにもどる。', '調べる▼', '検索結果', 'とURLをコピーしました', 'のご利用について', '通販サイト', '無料', '無料ブログはココログ!', 'パスワードを忘れた方', '特定商取引法表示', 'note(blog)', 'niconicoへのご意見・ご要望', 'ダウンロードはこちらからどうぞ。', '下記のお問い合わせフォームでご連絡ください。', '買い物かごカートに商品がありません。', 'ニュースレターに登録して最新情報やセール情報をいち早くゲット。', '受信する受信しない', 'JavaScriptを有効にしてご覧ください。', 'このページの機能を利用するにはJavaScriptに対応したブラウザが必要です。', '分で読めます。', '無断転載を禁じます。', '検索条件を入力してください。', 'コメント(0)トラックバックは利用できません。', 'この記事へのコメントはありません。返信をキャンセルする。', '法令順守と犯罪抑止のためにメルカリあんしん・あんぜん宣言!Facebook', 'Facebook', 'はてなブログを使っています。', '見つかりました', 'インプラント', '歯科', 'VIPがお送りします', '見積', 'ID:', 'PR', '一覧', '出会い', '探せます', '商品はございません', '《前のページ|次のページ》', 'RSS', '人気記事まだデータがありません。', '投稿が見つかりませんでした。', '投稿がありません。', '別のサイトにジャンプしようとしています。', 'お問合せは24時間受け付けております。', 'お気軽にご連絡ください。', '左の一覧からカテゴリーをお選びください。', 'までを表示します。', 'の人気クチコミ', 'JavaScriptをONにしてください。', 'サンプルです', 'リンクに追加する', 'サイドバー', 'これはテストです', 'のクチコミ', 'サイトについて', '口コミ・感想!', 'ホーム>', 'Next Entry']\n"
     ]
    }
   ],
   "source": [
    "broken_sentence_endings = \"\"\"\n",
    "...\n",
    "... \n",
    "...　\n",
    "\\\"\n",
    "'\n",
    "[…]\n",
    "詳細を見る\n",
    "すべて表示する\n",
    "するまとめ\n",
    "公式サイト\n",
    "にアクセス!\n",
    "お​問​い​合​わ​せ​\n",
    "アクセスマップ\n",
    "送料無料!\n",
    "コメント(0)\n",
    "トラックバックは利用できません。\n",
    "この記事へのコメントはありません。\n",
    "返信をキャンセルする。\n",
    "URL\n",
    "お問い合わせ\n",
    ",?\n",
    "周辺施設\n",
    "続きを表示\n",
    "音楽出版社求人\n",
    "Check\n",
    "マップMAP\n",
    "未入力項目があります\n",
    "SNS\n",
    "●facebook→\n",
    "●twitter→\n",
    "ABOUT US\n",
    "求人情報\n",
    "1件中1～1件を表示\n",
    "事業内容\n",
    "トップページ\n",
    "特集\n",
    "賃貸情報\n",
    "条件を選ぶ\n",
    "現在の選択エリア\n",
    "沿線から選ぶ\n",
    "エリアを選びなおす\n",
    "さらにエリアを絞る\n",
    "△ページTOP\n",
    "公式\n",
    "About\n",
    "search\n",
    "プロフィール\n",
    "更新情報をチェックする\n",
    "検索:\n",
    "関連記事\n",
    "ブックマークする\n",
    "友達に教える\n",
    "お問い合せ\n",
    "CLOSE\n",
    "My account\n",
    "Contact Us\n",
    "ご利用方法\n",
    "会員規約\n",
    "特定商取引法に基づく表記\n",
    "組織概要\n",
    "<前の20件1次の20件>\n",
    "官公庁臨時職員\n",
    "湘南国際村仕事\n",
    "タイアップ広告掲載\n",
    "広告を掲載しませんか?\n",
    "会社概要\n",
    "Pocket\n",
    "目次\n",
    "TOP\n",
    "から見つける\n",
    "プライバシーポリシー\n",
    "利用規約\n",
    "All Rights Reserved.\n",
    "滞在後投稿されたものです。\n",
    "法令順守と犯罪抑止のために\n",
    "メルカリあんしん・あんぜん宣言!\n",
    "偽ブランド品撲滅への取り組み\n",
    "スタッフブログ\n",
    "賃貸ホームページ\n",
    "物件カタログ\n",
    "ホーム\n",
    "メニュー\n",
    "仲介手数料最大無料\n",
    "口コミをもっと\n",
    "詳細情報\n",
    "ツイート\n",
    "メルマガ\n",
    "登録商標です。\n",
    "ようこそゲストさん\n",
    "→\n",
    "[...]\n",
    "の先頭へ\n",
    "個人情報の取り扱いについて\n",
    "中古マンション\n",
    "»\n",
    "パスワードをお忘れの方\n",
    "上部へ\n",
    "JavaScriptを有効にしてご利用下さい.\n",
    "ログインできない時は?\n",
    "運営会社\n",
    "企業情報\n",
    "問い合わせ\n",
    "一覧へ\n",
    "ヘルプ\n",
    "物件\n",
    "もっと見る\n",
    "を探す\n",
    "マンション\n",
    "検索\n",
    "店\n",
    "検索のヒント:\n",
    "新規の表示\n",
    "Vths\n",
    "Id\n",
    "見出し語\n",
    "Yomi\n",
    "大分類1\n",
    "小分類2\n",
    "Pos\n",
    "Actions\n",
    "メールアドレス\n",
    "パスワード\n",
    "次回から自動ログインをする\n",
    "パスワードを忘れた方はこちら\n",
    "不動産無料査定\n",
    "住まい紹介ブログ\n",
    "🗣naamal\n",
    "⋆。* ⋆。* ⋆\n",
    "122投稿\n",
    "サイトは?\n",
    "トップ\n",
    "ニュース\n",
    "日程\n",
    "著作権\n",
    "記事・写真・動画の利用申込\n",
    "採用情報\n",
    "サイト一覧\n",
    "関連情報\n",
    "の投稿\n",
    "ページトップへ\n",
    "記事を読む\n",
    "運営者情報\n",
    "Twitter公式\n",
    "もっとみる\n",
    "投稿する\n",
    "店舗TOP\n",
    "スタッフ・開発者募集\n",
    "ヘルプページ\n",
    "最近のコメント\n",
    "カレンダー\n",
    "の情報:\n",
    "の情報\n",
    "いいね\n",
    "キャンペーン\n",
    "|BOOKED\n",
    "HOME\n",
    "よくある質問\n",
    "BLOG\n",
    "...\n",
    "問い合わせください。\n",
    "休診日\n",
    "木曜日・日曜日・祝日\n",
    "all rights reserved.\n",
    "</tr>\n",
    "</div>\n",
    "浮気調査\n",
    "カテゴリー\n",
    "お知らせ\n",
    "新着アイテム\n",
    "アーカイブ\n",
    "月\n",
    "Search\n",
    "TOPへ\n",
    "TOPページへ\n",
    "Menu\n",
    "Return Top\n",
    "携帯版はこちら\n",
    "会社情報\n",
    "初めての方へ\n",
    "ダウンロード\n",
    "アクセス\n",
    "IR情報\n",
    "一覧\n",
    "採用サイト\n",
    "マイページ\n",
    "研究会会員\n",
    "専用ページ\n",
    "はこちら\n",
    "相談する\n",
    "相談\n",
    "探す\n",
    "事例を見る\n",
    "お客様からの声(成功事例)\n",
    "書籍\n",
    "メタ情報\n",
    "の詳細\n",
    "Home\n",
    "ログイン\n",
    "投稿フィード\n",
    "コメントフィード\n",
    "船井総研のサービス\n",
    "お客様からの声(成功事例)\n",
    "レポート\n",
    "サービス\n",
    "船井総研のサービス概要\n",
    "セミナー\n",
    "経営研究会\n",
    "コンサルティング\n",
    "業種・テーマ\n",
    "住宅・不動産\n",
    "リフォーム\n",
    "不動産\n",
    "賃貸\n",
    "建設\n",
    "病院・クリニック\n",
    "歯科医院\n",
    "特定商取引法\n",
    "はじめてお越しの方へ\n",
    "LINE\n",
    "menu\n",
    "免責事項\n",
    "閉じる\n",
    "読者になる\n",
    "リンク\n",
    "Tweet\n",
    "方法はコチラ\n",
    "トップへ戻る\n",
    "[ヘルプ]\n",
    "のプロフィール\n",
    "ランキング】\n",
    "更新しました\n",
    "アーカイヴ\n",
    "未分類\n",
    "お問合せ\n",
    "シェア\n",
    "[お問い合わせ]\n",
    "[利用規約]\n",
    "[個人情報保護方針]\n",
    "/RSS\n",
    "もっと読む\n",
    "の検索結果\n",
    "通販ショップ\n",
    "フォローする\n",
    "シェアする\n",
    "MENU\n",
    "このブログについて\n",
    "コメントを書く\n",
    "twitter\n",
    "facebook\n",
    "line\n",
    "コメント\n",
    "arrowup\n",
    "読まれている記事\n",
    "のビュー\n",
    "質問箱\n",
    "コメントをキャンセル\n",
    "down\n",
    "Website\n",
    "more\n",
    "Name\n",
    "Email\n",
    "home\n",
    "新着情報\n",
    "First\n",
    "のQ&A\n",
    "サイトへ\n",
    "Previous\n",
    "上へ\n",
    "更新\n",
    "Twitter\n",
    "拡大\n",
    "ランキング\n",
    "Next\n",
    "会員登録\n",
    "Last\n",
    "スタッフ紹介\n",
    "1階層ページ\n",
    "タイトル\n",
    "この記事を削除する\n",
    "2階層ページ\n",
    "サイド見出し\n",
    "送信する\n",
    "イメージナビ\n",
    "お客様の声・対策事例\n",
    "ページ次のページ »\n",
    "こちらから\n",
    "役立つ文例集\n",
    "お客様の声\n",
    "個人情報保護ポリシー\n",
    "特定商取引について\n",
    "必須お名前\n",
    "申込\n",
    "フォーム\n",
    "FAQ\n",
    "電話する\n",
    "ページ先頭へ\n",
    "送信する\n",
    "天気予報\n",
    "ロフィールを表示\n",
    "リンク集\n",
    "上部へ戻る\n",
    "へスキップ\n",
    "Category\n",
    "ご質問\n",
    "ヤミ金の電話番号\n",
    "最新記事\n",
    "インフォメーション\n",
    "コラム\n",
    "事務所案内\n",
    "サービス料金\n",
    "リストナビ\n",
    "掲示板BBS\n",
    "地図ページ\n",
    "メールフォーム\n",
    "ケータイサイト\n",
    "おすすめの注目情報を掲載。\n",
    "この記事をツイートする\n",
    "自動でツイートされます。\n",
    "コメントをするには、\n",
    "ログインして下さい。\n",
    "ページの先頭へ戻る。\n",
    "page\n",
    "の口コミ\n",
    "メニューにもどる。\n",
    "調べる▼\n",
    "検索結果\n",
    "とURLをコピーしました\n",
    "のご利用について\n",
    "通販サイト\n",
    "無料\n",
    "無料ブログはココログ!\n",
    "パスワードを忘れた方\n",
    "特定商取引法表示\n",
    "note(blog)\n",
    "niconicoへのご意見・ご要望\n",
    "ダウンロードはこちらからどうぞ。\n",
    "下記のお問い合わせフォームでご連絡ください。\n",
    "買い物かごカートに商品がありません。\n",
    "ニュースレターに登録して最新情報やセール情報をいち早くゲット。\n",
    "受信する受信しない\n",
    "JavaScriptを有効にしてご覧ください。\n",
    "このページの機能を利用するにはJavaScriptに対応したブラウザが必要です。\n",
    "分で読めます。\n",
    "無断転載を禁じます。\n",
    "検索条件を入力してください。\n",
    "コメント(0)トラックバックは利用できません。\n",
    "この記事へのコメントはありません。返信をキャンセルする。\n",
    "法令順守と犯罪抑止のためにメルカリあんしん・あんぜん宣言!Facebook\n",
    "Facebook\n",
    "はてなブログを使っています。\n",
    "見つかりました\n",
    "インプラント\n",
    "歯科\n",
    "VIPがお送りします\n",
    "見積\n",
    "ID:\n",
    "PR\n",
    "一覧\n",
    "出会い\n",
    "探せます\n",
    "商品はございません\n",
    "《前のページ|次のページ》\n",
    "RSS\n",
    "人気記事まだデータがありません。\n",
    "投稿が見つかりませんでした。\n",
    "投稿がありません。\n",
    "別のサイトにジャンプしようとしています。\n",
    "お問合せは24時間受け付けております。\n",
    "お気軽にご連絡ください。\n",
    "左の一覧からカテゴリーをお選びください。\n",
    "までを表示します。\n",
    "\n",
    "の人気クチコミ\n",
    "JavaScriptをONにしてください。\n",
    "サンプルです\n",
    "リンクに追加する\n",
    "サイドバー\n",
    "これはテストです\n",
    "のクチコミ\n",
    "サイトについて\n",
    "口コミ・感想!\n",
    "ホーム>\n",
    "Next Entry\n",
    "\"\"\"\n",
    "\n",
    "broken_ending_list = broken_sentence_endings.split(\"\\n\")\n",
    "print(broken_ending_list)\n",
    "broken_ending_list = [x for x in broken_ending_list if len(x) > 0]\n",
    "print(broken_ending_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error1\n",
      "char_in_line_dup\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error2\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "error3\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error4\n",
      "paragraph_dup\n",
      "char_in_paragraph_dup\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error5\n",
      "char_in_paragraph_dup\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error6\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error7\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "error8\n",
      "9-gram\n",
      "10-gram\n",
      "error9\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "error10\n",
      "char_in_paragraph_dup\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error11\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "error12\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error13\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error14\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error15\n",
      "line_dup\n",
      "char_in_paragraph_dup\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error16\n",
      "5-gram\n",
      "error17\n",
      "5-gram\n",
      "error18\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error19\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error20\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error21\n",
      "paragraph_dup\n",
      "char_in_paragraph_dup\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error22\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error23\n",
      "5-gram\n",
      "6-gram\n",
      "error24\n",
      "5-gram\n",
      "error25\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error26\n",
      "paragraph_dup\n",
      "char_in_paragraph_dup\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error27\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "error28\n",
      "line_dup\n",
      "paragraph_dup\n",
      "char_in_line_dup\n",
      "char_in_paragraph_dup\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error29\n",
      "char_in_paragraph_dup\n",
      "error30\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "error31\n",
      "5-gram\n",
      "error32\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error33\n",
      "5-gram\n",
      "6-gram\n",
      "error34\n",
      "5-gram\n",
      "6-gram\n",
      "error35\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error36\n",
      "5-gram\n",
      "error37\n",
      "paragraph_dup\n",
      "char_in_paragraph_dup\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error38\n",
      "line_dup\n",
      "char_in_paragraph_dup\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error39\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error40\n",
      "paragraph_dup\n",
      "char_in_line_dup\n",
      "char_in_paragraph_dup\n",
      "5-gram\n",
      "6-gram\n",
      "7-gram\n",
      "8-gram\n",
      "9-gram\n",
      "10-gram\n",
      "error41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 単一文字の繰り返し200----------------------------------------\n",
    "\n",
    "def repeated_id(text, threshold_ratio=0.3):\n",
    "    # # 文字の繰り返しをチェックする辞書\n",
    "    char_count = {}\n",
    "    # 各文字の繰り返し回数をカウント\n",
    "    for char in text:\n",
    "        if char in char_count:\n",
    "            char_count[char] += 1\n",
    "        else:\n",
    "            char_count[char] = 1\n",
    "    # ratio%以上繰り返される文字があるかチェック\n",
    "    repeated_chars = [char for char, count in char_count.items(\n",
    "    ) if count >= threshold_ratio*len(text)]\n",
    "    # 繰り返される文字がなければmatrix_tempに追加\n",
    "    if not repeated_chars:\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def is_repetitive_japanese(text, thresholds={\n",
    "    'line_dup': 0.30,\n",
    "    'paragraph_dup': 0.30,\n",
    "    'char_in_line_dup': 0.20,\n",
    "    'char_in_paragraph_dup': 0.20,\n",
    "    '2-gram': 0.20,\n",
    "    '3-gram': 0.18,\n",
    "    '4-gram': 0.16,\n",
    "    '5-gram': 0.15,\n",
    "    '6-gram': 0.14,\n",
    "    '7-gram': 0.13,\n",
    "    '8-gram': 0.12,\n",
    "    '9-gram': 0.11,\n",
    "        '10-gram': 0.10}):\n",
    "    if text == \"\":\n",
    "        return \"\"\n",
    "    # 段落と行に分割\n",
    "    paragraphs = text.split('\\n')\n",
    "    lines = text.replace('\\n', ' ').split('。')\n",
    "\n",
    "    # 段落と行の重複率を計算\n",
    "    paragraph_dup_rate = calc_dup_rate(paragraphs)\n",
    "    line_dup_rate = calc_dup_rate(lines)\n",
    "\n",
    "    # 文字に含まれる重複の割合を計算\n",
    "    char_in_paragraph_dup_rate = calc_char_dup_rate(paragraphs, text)\n",
    "    char_in_line_dup_rate = calc_char_dup_rate(lines, text)\n",
    "\n",
    "    # n-gramの重複率を計算\n",
    "    ngram_dup_rates = {}\n",
    "    for n in range(2, 11):\n",
    "        ngrams = extract_ngrams(text.replace('\\n', ''), n)\n",
    "        if n < 5:\n",
    "            # 最頻出のn-gramの出現回数を計算\n",
    "            ngram_dup_rates[n] = calc_max_freq_rate(ngrams)\n",
    "        else:\n",
    "            # 2回以上出現するn-gramの総出現回数を計算\n",
    "            ngram_dup_rates[n] = calc_total_dup_freq_rate(ngrams)\n",
    "\n",
    "    # 各指標が閾値を超えているかチェック\n",
    "    if (line_dup_rate > thresholds['line_dup'] or\n",
    "        paragraph_dup_rate > thresholds['paragraph_dup'] or\n",
    "        char_in_line_dup_rate > thresholds['char_in_line_dup'] or\n",
    "        char_in_paragraph_dup_rate > thresholds['char_in_paragraph_dup'] or\n",
    "            any(ngram_dup_rates[n] > thresholds[f'{n}-gram'] for n in range(2, 11))):\n",
    "        if line_dup_rate > thresholds['line_dup']:\n",
    "            print(\"line_dup\")\n",
    "        if paragraph_dup_rate > thresholds['paragraph_dup']:\n",
    "            print(\"paragraph_dup\")\n",
    "        if char_in_line_dup_rate > thresholds['char_in_line_dup']:\n",
    "            print(\"char_in_line_dup\")\n",
    "        if char_in_paragraph_dup_rate > thresholds['char_in_paragraph_dup']:\n",
    "            print('char_in_paragraph_dup')\n",
    "        for n in range(2, 11):\n",
    "            if ngram_dup_rates[n] > thresholds[f'{n}-gram']:\n",
    "                print(f\"{n}-gram\")\n",
    "                #print(text)\n",
    "        return \"\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def calc_dup_rate(items):\n",
    "    counter = Counter(items)\n",
    "    total = len(items)\n",
    "    dup_count = sum(1 for count in counter.values() if count > 1)\n",
    "    return dup_count / total if total > 0 else 0\n",
    "\n",
    "\n",
    "def calc_char_dup_rate(items, text):\n",
    "    counter = Counter(items)\n",
    "    total_chars = len(text)\n",
    "    dup_chars = sum(len(item) * (count - 1)\n",
    "                    for item, count in counter.items() if count > 1)\n",
    "    return dup_chars / total_chars if total_chars > 0 else 0\n",
    "\n",
    "\n",
    "def extract_ngrams(text, n):\n",
    "    return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "\n",
    "def calc_max_freq_rate(ngrams):\n",
    "    counter = Counter(ngrams)\n",
    "    total = len(ngrams)\n",
    "    max_freq = max(counter.values())\n",
    "    return max_freq / total if total > 0 else 0\n",
    "\n",
    "\n",
    "def calc_total_dup_freq_rate(ngrams):\n",
    "    counter = Counter(ngrams)\n",
    "    total = len(ngrams)\n",
    "    dup_freq = sum(count - 1 for count in counter.values() if count > 1)\n",
    "    return dup_freq / total if total > 0 else 0\n",
    "\n",
    "# text = \"これはテストです。これはテストです。これはテストです。\\nこれもテストです。これもテストです。\"\n",
    "\n",
    "# print(text)\n",
    "# print(is_repetitive_japanese(text, thresholds))  # 空の文字列を出力\n",
    "count=0\n",
    "new_texts=[]\n",
    "for text in texts:\n",
    "    #print(text)\n",
    "    text=extract(text, include_tables=False,target_lang='ja',favour_precision=True)\n",
    "    if text ==None:\n",
    "        continue\n",
    "\n",
    "    #count+=1\n",
    "    \n",
    "    # if count<4:\n",
    "    #     print(\"extract\",text)\n",
    "    text=is_repetitive_japanese(text)\n",
    "    if text !=\"\":\n",
    "        new_texts.append(text)\n",
    "    else:\n",
    "        count+=1\n",
    "        print(f\"error{count}\")\n",
    "    # if count>=4:\n",
    "    #     break\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'当ホームページではjavascriptを使用しています。 javascriptの使用を有効にしなければ、一部の機能が正確に動作しない恐れがあります。お手数ですがjavascriptの使用を有効にしてください。\\n子育て・教育は稲沢で！\\nサイト内検索使い方\\n現在の位置：\\nトップページ >\\nイベント >\\nスポーツ >\\nスポーツ教室 >\\n祖父江の森スポーツ教室\\nここから本文です。\\nページID1005015\\n印刷大きな文字で印刷'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_texts[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_n_grams(words: list[str], n: int) -> list[str]:\n",
    "    return [\" \".join(words[i : i + n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "\n",
    "def find_duplicates(x: list[str]) -> tuple[int, int]:\n",
    "    unique_x = set()\n",
    "    duplicate_chars = 0\n",
    "    duplicate_elements = 0\n",
    "    for element in x:\n",
    "        if element in unique_x:\n",
    "            duplicate_chars += len(element)\n",
    "            duplicate_elements += 1\n",
    "\n",
    "        else:\n",
    "            unique_x.add(element)\n",
    "    return duplicate_elements, duplicate_chars\n",
    "\n",
    "def find_top_duplicate(x: list[str]) -> int:\n",
    "    counter = Counter()\n",
    "    for element in x:\n",
    "        counter[element] += 1\n",
    "    top_n_gram = counter.most_common(1)[0]\n",
    "    return len(top_n_gram[0]) * top_n_gram[1]\n",
    "\n",
    "\n",
    "def find_all_duplicate(words: list[str], n: int) -> int:\n",
    "    n_words = len(words)\n",
    "    unique = set()\n",
    "    repeated_chars, idx = 0, 0\n",
    "    while idx < n_words - n + 1:\n",
    "        n_gram = \"\".join(words[idx : idx + n])\n",
    "        if n_gram in unique:\n",
    "            repeated_chars += len(n_gram)\n",
    "            idx += n\n",
    "        else:\n",
    "            unique.add(n_gram)\n",
    "            idx += 1\n",
    "    assert repeated_chars <= len(\"\".join(words))\n",
    "    return repeated_chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter(self, doc: Document) -> bool | tuple[bool, str]:\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    text = doc.text\n",
    "\n",
    "    paragraphs = self.paragraph_exp.split(text.strip())\n",
    "    paragraphs_duplicates, char_duplicates = find_duplicates(paragraphs)\n",
    "    if self.dup_para_frac and paragraphs_duplicates / len(paragraphs) > self.dup_para_frac:\n",
    "        return False, \"dup_para_frac\"\n",
    "    if self.dup_para_char_frac and char_duplicates / len(text) > self.dup_para_char_frac:\n",
    "        return False, \"dup_para_char_frac\"\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    line_duplicates, char_duplicates = find_duplicates(lines)\n",
    "    if self.dup_line_frac and line_duplicates / len(lines) > self.dup_line_frac:\n",
    "        return False, \"dup_line_frac\"\n",
    "    if self.dup_line_char_frac and char_duplicates / len(text) > self.dup_line_char_frac:\n",
    "        return False, \"dup_line_char_frac\"\n",
    "\n",
    "    words = word_tokenize(text, language=\"english\")  # TODO we should use language id filter\n",
    "\n",
    "    for n, n_frac in self.top_n_grams:\n",
    "        n_grams = get_n_grams(words, n)\n",
    "        if not n_grams:\n",
    "            continue\n",
    "        top_char_length = find_top_duplicate(n_grams)\n",
    "        if top_char_length / len(text) > n_frac:\n",
    "            return False, f\"top_{n}_gram\"\n",
    "\n",
    "    for n, n_frac in self.dup_n_grams:\n",
    "        n_duplicates_char = find_all_duplicate(words, n)\n",
    "        if n_duplicates_char / len(text) > n_frac:\n",
    "            return False, f\"duplicated_{n}_n_grams\"\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract トップページ\n",
    "製品マニュアル\n",
    "よくあるご質問[FAQ]\n",
    "トラブルシューティング\n",
    "すべてから検索\n",
    "製品マニュアルから検索\n",
    "よくある質問[FAQ]から検索\n",
    "トラブルシューティングから検索\n",
    "お知らせ\n",
    "障害情報\n",
    "マイサイトを編集\n",
    "サイトの統合・一部ページの移行・分割はできますか\n",
    "＜＜統合や移行について＞＞各サイトごとにサイトを構築・管理していますので、サイトAとサイトBを統合したり、各サイトの一部のページのみ移行することはできません。 ＜＜分割について＞＞ 分割を行...\n",
    "よくある質問\n",
    "全般\n",
    "ご利用環境・編集関連\n",
    "コントロールパネル\n",
    "アプリを起動\n",
    "各種設定\n",
    "サイトシアター画面\n",
    "マイサイトを編集\n",
    "テンプレートを選ぶ・ブランクサイトから作成\n",
    "サイトをインポート\n",
    "サイトエディタ画面\n",
    "全般\n",
    "サイト設定\n",
    "プレビュー・サイトを公開\n",
    "Dress・ウェブフォント・ページレイアウト\n",
    "ブロックエディタ画面\n",
    "ブロックテンプレート\n",
    "編集\n",
    "設定\n",
    "CSS詳細設定\n",
    "各種パーツ\n",
    "画像を選択\n",
    "画像を作成(SiGN Pro)\n",
    "スライドショー\n",
    "リンク\n",
    "アイコン・区切り線\n",
    "表 テーブル\n",
    "フォーム\n",
    "ダウンロード\n",
    "カスタムタグ\n",
    "ソーシャル系\n",
    "ウェブサービス系\n",
    "ショッピングカートパーツ\n",
    "その他\n",
    "SYNC for WebLiFE*5\n",
    "ソーシャル系/ソーシャルボタン・Instagram\n",
    "ソーシャル系/タイムライン･Twitterバッジ\n",
    "ソーシャル系/Page Plugin・コメント欄\n",
    "ソーシャル系/YouTube・Ustream\n",
    "ウェブサービス系/Googleマップ・Yahoo!地図\n",
    "ウェブサービス系/サイト内検索・翻訳\n",
    "ウェブサービス系/ニュース・ブログ\n",
    "ウェブサービス系/ドキュメント\n",
    "サイト・コーナー・ページ設定\n",
    "サイト設定\n",
    "コーナー設定\n",
    "ページ設定\n",
    "アップロード・FTP設定\n",
    "サーバー関連\n",
    "アップロード関連\n",
    "閲覧\n",
    "その他・リファレンス\n",
    "対応言語\n",
    "その他\n",
    "各種設定\n",
    "外部サービス連携/Benchmark Email\n",
    "サイト管理/かんたんパスワード\n",
    "サイト管理/独自ドメイン\n",
    "Copyright(C)BIGLOBE Inc. 1996-2016\n",
    "filter トップページ\n",
    "製品マニュアル\n",
    "よくあるご質問[FAQ]\n",
    "トラブルシューティング\n",
    "すべてから検索\n",
    "製品マニュアルから検索\n",
    "よくある質問[FAQ]から検索\n",
    "トラブルシューティングから検索\n",
    "お知らせ\n",
    "障害情報\n",
    "マイサイトを編集\n",
    "サイトの統合・一部ページの移行・分割はできますか\n",
    "＜＜統合や移行について＞＞各サイトごとにサイトを構築・管理していますので、サイトAとサイトBを統合したり、各サイトの一部のページのみ移行することはできません。 ＜＜分割について＞＞ 分割を行...\n",
    "よくある質問\n",
    "全般\n",
    "ご利用環境・編集関連\n",
    "コントロールパネル\n",
    "アプリを起動\n",
    "各種設定\n",
    "サイトシアター画面\n",
    "マイサイトを編集\n",
    "テンプレートを選ぶ・ブランクサイトから作成\n",
    "サイトをインポート\n",
    "サイトエディタ画面\n",
    "全般\n",
    "サイト設定\n",
    "プレビュー・サイトを公開\n",
    "Dress・ウェブフォント・ページレイアウト\n",
    "ブロックエディタ画面\n",
    "ブロックテンプレート\n",
    "編集\n",
    "設定\n",
    "CSS詳細設定\n",
    "各種パーツ\n",
    "画像を選択\n",
    "画像を作成(SiGN Pro)\n",
    "スライドショー\n",
    "リンク\n",
    "アイコン・区切り線\n",
    "表 テーブル\n",
    "フォーム\n",
    "ダウンロード\n",
    "カスタムタグ\n",
    "ソーシャル系\n",
    "ウェブサービス系\n",
    "ショッピングカートパーツ\n",
    "その他\n",
    "SYNC for WebLiFE*5\n",
    "ソーシャル系/ソーシャルボタン・Instagram\n",
    "ソーシャル系/タイムライン･Twitterバッジ\n",
    "ソーシャル系/Page Plugin・コメント欄\n",
    "ソーシャル系/YouTube・Ustream\n",
    "ウェブサービス系/Googleマップ・Yahoo!地図\n",
    "ウェブサービス系/サイト内検索・翻訳\n",
    "ウェブサービス系/ニュース・ブログ\n",
    "ウェブサービス系/ドキュメント\n",
    "サイト・コーナー・ページ設定\n",
    "サイト設定\n",
    "コーナー設定\n",
    "ページ設定\n",
    "アップロード・FTP設定\n",
    "サーバー関連\n",
    "アップロード関連\n",
    "閲覧\n",
    "その他・リファレンス\n",
    "対応言語\n",
    "その他\n",
    "各種設定\n",
    "外部サービス連携/Benchmark Email\n",
    "サイト管理/かんたんパスワード\n",
    "サイト管理/独自ドメイン\n",
    "Copyright(C)BIGLOBE Inc. 1996-2016\n",
    "extract 一関市の広告代理店FREE LINE（フリーライン）はお得なクーポン情報をフリーペーパーで紹介しております\n",
    "広告代理店FREE LINE（フリーライン）では一関市のフリーペーパーを発行しております。お得な情報満載で、皆様のライフスタイルを応援いたします。\n",
    "一関市のフリーペーパーに情報掲載をお考えなら広告代理店「FREE LINE（フリーライン）」までご連絡ください！\n",
    "True\n",
    "filter \n",
    "extract Hair Scene023\n",
    "Creator：Fetish Factory\n",
    "Format：download\n",
    "Scene time：00:11:54\n",
    "Price 999yen\n",
    "<ホームページ特別価格>\n",
    "\"髪大好き\"伸ばせるところまで伸ばしたいという黒髪ロングのゆかりさん。ちょっとサラサラの黒髪ロングを触らせてもらいました。着衣のままで洗髪のお手入れ。\n",
    "This is an image of woman's hair.\n",
    "＜当サイト内でのキーワード検索ができます。＞You can search keywords in this site.\n",
    "＞カテゴリートップへ戻る(Category top)\n",
    "・ご利用ガイド\n",
    "ダウンロード販売のシステムはXCREAMに委託しております。\n",
    "・\n",
    "ダウンロードについて\n",
    "・\n",
    "Engloish FAQ\n",
    ">\n",
    "中文(簡体字)\n",
    ">\n",
    "中文(繁体字)\n",
    ">\n",
    "한국어\n",
    "・決済手段\n",
    "お支払はクレジットカードとビットキャッシュ EXがご利用できます。\n",
    "株式会社ゼウスが提供するクレジットカード決済システムを導入、安心してお買い物いただけます。\n",
    "Copyright(C) GAGON.COM All right reserved.\n",
    "filter Hair Scene023\n",
    "Creator：Fetish Factory\n",
    "Format：download\n",
    "Scene time：00:11:54\n",
    "Price 999yen\n",
    "<ホームページ特別価格>\n",
    "\"髪大好き\"伸ばせるところまで伸ばしたいという黒髪ロングのゆかりさん。ちょっとサラサラの黒髪ロングを触らせてもらいました。着衣のままで洗髪のお手入れ。\n",
    "This is an image of woman's hair.\n",
    "＜当サイト内でのキーワード検索ができます。＞You can search keywords in this site.\n",
    "＞カテゴリートップへ戻る(Category top)\n",
    "・ご利用ガイド\n",
    "ダウンロード販売のシステムはXCREAMに委託しております。\n",
    "・\n",
    "ダウンロードについて\n",
    "・\n",
    "Engloish FAQ\n",
    ">\n",
    "中文(簡体字)\n",
    ">\n",
    "中文(繁体字)\n",
    ">\n",
    "한국어\n",
    "・決済手段\n",
    "お支払はクレジットカードとビットキャッシュ EXがご利用できます。\n",
    "株式会社ゼウスが提供するクレジットカード決済システムを導入、安心してお買い物いただけます。\n",
    "Copyright(C) GAGON.COM All right reserved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from datatrove.utils.text import PUNCTUATION_SET\n",
    "\n",
    "PUNCTUATION = \"!/—”:％１〈&(、━\\\\【#%「」，】；+^]~“《„';’{|∶´[=-`*．（–？！：$～«〉,><》)?）。…@_.\\\"}►»\" + \"\".join(\n",
    "    map(\n",
    "        chr,\n",
    "        (x for a, b in ((0, 9), (11, 13), (13, 32), (127, 160)) for x in range(a, b)),\n",
    "    )\n",
    ")\n",
    "PUNCTUATION_SET = set(PUNCTUATION)\n",
    "\n",
    "\n",
    "\n",
    "STOP_WORDS = [\"the\", \"be\", \"to\", \"of\", \"and\", \"that\", \"have\", \"with\"]\n",
    "\n",
    "\n",
    "class GopherQualityFilter():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_doc_words: int | None = 50,\n",
    "        max_doc_words: int | None = 100000,\n",
    "        min_avg_word_length: int | None = 3,\n",
    "        max_avg_word_length: int | None = 10,\n",
    "        max_symbol_word_ratio: float | None = 0.1,\n",
    "        max_bullet_lines_ratio: float | None = 0.9,\n",
    "        max_ellipsis_lines_ratio: float | None = 0.3,\n",
    "        max_non_alpha_words_ratio: float | None = 0.8,\n",
    "        min_stop_words: int | None = 2,\n",
    "        stop_words: list[str] | None = None,\n",
    "        #exclusion_writer: DiskWriter = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Filter to apply Gopher's quality heuristic rules.\n",
    "        \"\"\"\n",
    "        self.min_doc_words = min_doc_words\n",
    "        self.max_doc_words = max_doc_words\n",
    "        self.min_avg_word_length = min_avg_word_length\n",
    "        self.max_avg_word_length = max_avg_word_length\n",
    "        self.max_symbol_word_ratio = max_symbol_word_ratio\n",
    "        self.max_bullet_lines_ratio = max_bullet_lines_ratio\n",
    "        self.max_ellipsis_lines_ratio = max_ellipsis_lines_ratio\n",
    "        self.max_non_alpha_words_ratio = max_non_alpha_words_ratio\n",
    "        self.min_stop_words = min_stop_words\n",
    "        self.stop_words = set(STOP_WORDS if stop_words is None else stop_words)\n",
    "\n",
    "    def filter(self, text: str) -> bool | tuple[bool, str]:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            doc: Applies the heuristics rules to decide if a document should be REMOVED\n",
    "\n",
    "        Returns: False if sample.text does not pass any of the the heuristic tests\n",
    "\n",
    "        \"\"\"\n",
    "        from nltk.tokenize import word_tokenize\n",
    "\n",
    "        words = word_tokenize(text)  # TODO we should use language id filter\n",
    "        n_words = len(words)\n",
    "        non_symbol_words = [w for w in words if any(ch not in PUNCTUATION_SET for ch in w)]\n",
    "        n_non_symbol_words_words = len(non_symbol_words)\n",
    "\n",
    "        # words < min_doc_words or words > max_doc_words\n",
    "        #if self.min_doc_words and n_non_symbol_words_words < self.min_doc_words:\n",
    "         #   print(text)\n",
    "           # return \"\",False, \"gopher_short_doc\"\n",
    "        if self.max_doc_words and n_non_symbol_words_words > self.max_doc_words:\n",
    "            return \"\",False, \"gopher_long_doc\"\n",
    "\n",
    "        # mean word length is outside the range of 3 to 10 characters\n",
    "        avg_n_words = np.mean([len(w) for w in non_symbol_words])\n",
    "        if self.min_avg_word_length and avg_n_words < self.min_avg_word_length:\n",
    "            return \"\",False, \"gopher_below_avg_threshold\"\n",
    "        if self.max_avg_word_length and avg_n_words > self.max_avg_word_length:\n",
    "            return \"\",False, \"gopher_above_avg_threshold\"\n",
    "\n",
    "        # symbol-to-word ratio greater than 0.1 for either the hash symbol or the ellipsis\n",
    "        if self.max_symbol_word_ratio and text.count(\"#\") / n_words > self.max_symbol_word_ratio:\n",
    "            return \"\",False, \"gopher_too_many_hashes\"\n",
    "        if self.max_symbol_word_ratio and (text.count(\"...\") + text.count(\"…\")) / n_words > self.max_symbol_word_ratio:\n",
    "            return \"\",False, \"gopher_too_many_ellipsis\"\n",
    "\n",
    "        # any document with more than 90 % of lines starting with a bullet point,\n",
    "        # or more than 30 % ending with an ellipsis.\n",
    "        lines = text.splitlines()\n",
    "        if (\n",
    "            self.max_bullet_lines_ratio\n",
    "            and sum(s.lstrip().startswith(\"•\") or s.lstrip().startswith(\"-\") for s in lines) / len(lines)\n",
    "            > self.max_bullet_lines_ratio\n",
    "        ):\n",
    "            return \"\",False, \"gopher_too_many_bullets\"\n",
    "        if (\n",
    "            self.max_ellipsis_lines_ratio\n",
    "            and sum(s.rstrip().endswith(\"...\") or s.rstrip().endswith(\"…\") for s in lines) / len(lines)\n",
    "            > self.max_ellipsis_lines_ratio\n",
    "        ):\n",
    "            return \"\",False, \"gopher_too_many_end_ellipsis\"\n",
    "\n",
    "        # that 80 % of words in a document contain at least one alphabetic character\n",
    "        if (\n",
    "            self.max_non_alpha_words_ratio\n",
    "            and sum([any((c.isalpha() for c in w)) for w in words]) / n_words < self.max_non_alpha_words_ratio\n",
    "        ):\n",
    "            return \"\" ,False, \"gopher_below_alpha_threshold\"\n",
    "\n",
    "        # stop word filter\n",
    "        # if self.min_stop_words and sum(w in self.stop_words for w in words) < self.min_stop_words:\n",
    "        #     print(text)\n",
    "        #     return \"\",False, \"gopher_enough_stop_words\"\n",
    "\n",
    "        return text ,True ,\"ok\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: tqdm in /data/satori_hdd4/takumi/.local/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: joblib in /data/satori_hdd4/takumi/.local/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /data/satori_hdd4/takumi/.local/lib/python3.10/site-packages (from nltk) (2023.6.3)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "gopher_below_alpha_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_below_alpha_threshold\n",
      "gopher_above_avg_threshold\n",
      "ok\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "ok\n",
      "gopher_above_avg_threshold\n",
      "ok\n",
      "gopher_above_avg_threshold\n",
      "ok\n",
      "gopher_above_avg_threshold\n",
      "ok\n",
      "gopher_above_avg_threshold\n",
      "gopher_below_alpha_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_below_alpha_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_below_alpha_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "ok\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_below_alpha_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "ok\n",
      "ok\n",
      "gopher_above_avg_threshold\n",
      "ok\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "ok\n",
      "ok\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "quality_filter=GopherQualityFilter()\n",
    "new_texts2=[]\n",
    "for text in new_texts:\n",
    "    text, _ , m =quality_filter.filter(text)\n",
    "    print(m)\n",
    "    if text!=\"\":\n",
    "        new_texts2.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TOPページ\\n新規登録\\nマイページ\\nブログリーダー\\n利用規約\\n広告掲載\\nヘルプ\\nNEW\\nブロガーのためのSNS「ブログサークル」\\nブログに特化した新しいSNSが誕生！ 同じ趣味の仲間とつながったり、自分のブログをみんなにアピールしよう。\\n今すぐ登録\\nツイート\\n投票TOP\\n新着トピック\\n話題のトピック\\n作成・管理\\nカテゴリ\\nサービスガイド\\nＴＯＰ\\n乗り物\\n飛行機\\n受付期間 : 2011年06月11日〜無期限\\n自衛隊航空機で好きな機種は？\\nF-2\\nF-4 ファントム\\nF-15 イーグル\\nE-2C ホークアイ\\nT-4 （ブルーも）\\nC-１\\nYS-11\\nCH-47\\nUH-60\\nAH-1S コブラ\\nUH-1H イロコイ\\nP-３C オライオン\\nC-130 ハーキュリーズ\\nUS-1A\\nない\\nその他（過去に所属した機種も）\\nコメント\\n性別：\\n【未選択】\\n男性\\n女性\\n年齢：\\n【未選択】\\n10未満\\n10歳代\\n20歳代\\n30歳代\\n40歳代\\n50歳代\\n60歳代\\n70歳代\\n80歳代\\n90歳代\\n100以上\\n地域：\\n【未選択】\\n北海道\\n青森\\n岩手\\n秋田\\n宮城\\n山形\\n福島\\n新潟\\n栃木\\n茨城\\n群馬\\n埼玉\\n東京\\n千葉\\n神奈川\\n山梨\\n静岡\\n長野\\n富山\\n岐阜\\n愛知\\n石川\\n福井\\n滋賀\\n三重\\n京都\\n奈良\\n大阪\\n和歌山\\n兵庫\\n鳥取\\n岡山\\n島根\\n広島\\n山口\\n香川\\n徳島\\n高知\\n愛媛\\n福岡\\n佐賀\\n大分\\n長崎\\n熊本\\n宮崎\\n鹿児島\\n沖縄\\n【海外】\\n©\\nさいたかのブログ\\nこのトピックをブログパーツにする\\nサイズ選択\\n自動\\n小サイズ\\n中サイズ\\n大サイズ\\n直接リンクを張る場合のURL\\nデザインをカスタマイズする\\n(細かいデザインの変更が可能です)\\n投票TOPへ\\n｜\\n「飛行機」カテゴリへ\\nTOPページ\\n新規登録\\nマイページ\\nブログリーダー\\n利用規約\\n広告掲載\\nヘルプ\\nCopyright © 2004 - 2023 \"\\n@With\\n\" All rights reserved.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_texts2[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
