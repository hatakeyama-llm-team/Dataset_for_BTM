{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç›®æ¨™\n",
    "* ã€Œhtmlæ§‹é€ ã€â†’ã€Œtrafilaturaã€â†’ã€Œã§ãã‚‹ã ã‘åˆ†è§£ã—ãŸå‡¦ç†ç¾¤ã€â†’ã€ŒMLãƒ™ãƒ¼ã‚¹å‰ã€\n",
    "* trafilaturaã®å‡¦ç†\n",
    "* å‡¦ç†ç¾¤ã®åˆ†è§£\n",
    "\n",
    "* Mecabã®ã‚¨ãƒ©ãƒ¼ã«ã¤ã„ã¦ï¼šè¾æ›¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã«ãªã‚‹ãŒã€[ã“ã®è¨˜äº‹](https://analytics-note.xyz/programming/python-mecabrc-error/)ã¨å…¨ãåŒã˜ç¾è±¡ãŒèµ·ãã¦ã„ãŸã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: number expected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: unknown file attribute: b\n",
      "zsh:1: number expected\n",
      "zsh:1: number expected\n",
      "zsh:1: missing delimiter for 'u' glob qualifier\n",
      "zsh:1: missing delimiter for 'u' glob qualifier\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet (warcio)\n",
    "!pip install --quiet (beautifulsoup4)\n",
    "!pip install --quiet (trafilatura)\n",
    "!pip install --quiet (mecab-python3)\n",
    "!pip install --quiet (unidic)\n",
    "!pip install --quiet (unidic-lite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç§\tä»£åè©,,,,,,ãƒ¯ã‚¿ã‚¯ã‚·,ç§-ä»£åè©,ç§,ãƒ¯ã‚¿ã‚¯ã‚·,ç§,ãƒ¯ã‚¿ã‚¯ã‚·,å’Œ,\"\",\"\",\"\",\"\",\"\",\"\",ä½“,ãƒ¯ã‚¿ã‚¯ã‚·,ãƒ¯ã‚¿ã‚¯ã‚·,ãƒ¯ã‚¿ã‚¯ã‚·,ãƒ¯ã‚¿ã‚¯ã‚·,\"0\",\"\",\"\",11345327978324480,41274\n",
      "ã¯\tåŠ©è©,ä¿‚åŠ©è©,,,,,ãƒ,ã¯,ã¯,ãƒ¯,ã¯,ãƒ¯,å’Œ,\"\",\"\",\"\",\"\",\"\",\"\",ä¿‚åŠ©,ãƒ,ãƒ,ãƒ,ãƒ,\"\",\"å‹•è©%F2@0,åè©%F1,å½¢å®¹è©%F2@-1\",\"\",8059703733133824,29321\n",
      "è‡ªç„¶\tåè©,æ™®é€šåè©,ä¸€èˆ¬,,,,ã‚·ã‚¼ãƒ³,è‡ªç„¶,è‡ªç„¶,ã‚·ã‚¼ãƒ³,è‡ªç„¶,ã‚·ã‚¼ãƒ³,æ¼¢,\"\",\"\",\"\",\"\",\"\",\"\",ä½“,ã‚·ã‚¼ãƒ³,ã‚·ã‚¼ãƒ³,ã‚·ã‚¼ãƒ³,ã‚·ã‚¼ãƒ³,\"0\",\"C2\",\"\",4227355954520576,15379\n",
      "è¨€èª\tåè©,æ™®é€šåè©,ä¸€èˆ¬,,,,ã‚²ãƒ³ã‚´,è¨€èª,è¨€èª,ã‚²ãƒ³ã‚´,è¨€èª,ã‚²ãƒ³ã‚´,æ¼¢,\"\",\"\",\"\",\"\",\"\",\"\",ä½“,ã‚²ãƒ³ã‚´,ã‚²ãƒ³ã‚´,ã‚²ãƒ³ã‚´,ã‚²ãƒ³ã‚´,\"1\",\"C1\",\"\",3205085018595840,11660\n",
      "å‡¦ç†\tåè©,æ™®é€šåè©,ã‚µå¤‰å¯èƒ½,,,,ã‚·ãƒ§ãƒª,å‡¦ç†,å‡¦ç†,ã‚·ãƒ§ãƒª,å‡¦ç†,ã‚·ãƒ§ãƒª,æ¼¢,\"\",\"\",\"\",\"\",\"\",\"\",ä½“,ã‚·ãƒ§ãƒª,ã‚·ãƒ§ãƒª,ã‚·ãƒ§ãƒª,ã‚·ãƒ§ãƒª,\"1\",\"C3\",\"\",4691624739348992,17068\n",
      "åˆ†é‡\tåè©,æ™®é€šåè©,ä¸€èˆ¬,,,,ãƒ–ãƒ³ãƒ¤,åˆ†é‡,åˆ†é‡,ãƒ–ãƒ³ãƒ¤,åˆ†é‡,ãƒ–ãƒ³ãƒ¤,æ¼¢,\"\",\"\",\"\",\"\",\"\",\"B\",ä½“,ãƒ–ãƒ³ãƒ¤,ãƒ–ãƒ³ãƒ¤,ãƒ–ãƒ³ãƒ¤,ãƒ–ãƒ³ãƒ¤,\"1\",\"C1\",\"\",9249925070201344,33651\n",
      "ã®\tåŠ©è©,æ ¼åŠ©è©,,,,,ãƒ,ã®,ã®,ãƒ,ã®,ãƒ,å’Œ,\"\",\"\",\"\",\"\",\"\",\"\",æ ¼åŠ©,ãƒ,ãƒ,ãƒ,ãƒ,\"\",\"åè©%F1\",\"\",7968444268028416,28989\n",
      "ç ”ç©¶\tåè©,æ™®é€šåè©,ã‚µå¤‰å¯èƒ½,,,,ã‚±ãƒ³ã‚­ãƒ¥ã‚¦,ç ”ç©¶,ç ”ç©¶,ã‚±ãƒ³ã‚­ãƒ¥ãƒ¼,ç ”ç©¶,ã‚±ãƒ³ã‚­ãƒ¥ãƒ¼,æ¼¢,\"\",\"\",\"\",\"\",\"\",\"\",ä½“,ã‚±ãƒ³ã‚­ãƒ¥ã‚¦,ã‚±ãƒ³ã‚­ãƒ¥ã‚¦,ã‚±ãƒ³ã‚­ãƒ¥ã‚¦,ã‚±ãƒ³ã‚­ãƒ¥ã‚¦,\"0\",\"C2\",\"\",3104754582561280,11295\n",
      "ã‚’\tåŠ©è©,æ ¼åŠ©è©,,,,,ãƒ²,ã‚’,ã‚’,ã‚ª,ã‚’,ã‚ª,å’Œ,\"\",\"\",\"\",\"\",\"\",\"\",æ ¼åŠ©,ãƒ²,ãƒ²,ãƒ²,ãƒ²,\"\",\"å‹•è©%F2@0,åè©%F1,å½¢å®¹è©%F2@-1\",\"\",11381878116459008,41407\n",
      "ã—\tå‹•è©,éè‡ªç«‹å¯èƒ½,,,ã‚µè¡Œå¤‰æ ¼,é€£ç”¨å½¢-ä¸€èˆ¬,ã‚¹ãƒ«,ç‚ºã‚‹,ã—,ã‚·,ã™ã‚‹,ã‚¹ãƒ«,å’Œ,\"\",\"\",\"\",\"\",\"\",\"\",ç”¨,ã‚·,ã‚¹ãƒ«,ã‚·,ã‚¹ãƒ«,\"0\",\"C5\",\"\",5370298291593857,19537\n",
      "ã¦\tåŠ©è©,æ¥ç¶šåŠ©è©,,,,,ãƒ†,ã¦,ã¦,ãƒ†,ã¦,ãƒ†,å’Œ,\"\",\"\",\"\",\"\",\"\",\"\",æ¥åŠ©,ãƒ†,ãƒ†,ãƒ†,ãƒ†,\"\",\"å‹•è©%F1,å½¢å®¹è©%F2@-1\",\"\",6837321680953856,24874\n",
      "ã„\tå‹•è©,éè‡ªç«‹å¯èƒ½,,,ä¸Šä¸€æ®µ-ã‚¢è¡Œ,é€£ç”¨å½¢-ä¸€èˆ¬,ã‚¤ãƒ«,å±…ã‚‹,ã„,ã‚¤,ã„ã‚‹,ã‚¤ãƒ«,å’Œ,\"\",\"\",\"\",\"\",\"\",\"\",ç”¨,ã‚¤,ã‚¤ãƒ«,ã‚¤,ã‚¤ãƒ«,\"0\",\"C4\",\"M4@1\",710568013079169,2585\n",
      "ã¾ã™\tåŠ©å‹•è©,,,,åŠ©å‹•è©-ãƒã‚¹,çµ‚æ­¢å½¢-ä¸€èˆ¬,ãƒã‚¹,ã¾ã™,ã¾ã™,ãƒã‚¹,ã¾ã™,ãƒã‚¹,å’Œ,\"\",\"\",\"\",\"\",\"\",\"\",åŠ©å‹•,ãƒã‚¹,ãƒã‚¹,ãƒã‚¹,ãƒã‚¹,\"\",\"å‹•è©%F4@1\",\"\",9812325267808939,35697\n",
      "ã€‚\tè£œåŠ©è¨˜å·,å¥ç‚¹,,,,,,ã€‚,ã€‚,,ã€‚,,è¨˜å·,\"\",\"\",\"\",\"\",\"\",\"\",è£œåŠ©,,,,,\"\",\"\",\"\",6880571302400,25\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Mecabã®å‹•ä½œãƒã‚§ãƒƒã‚¯ã€€\n",
    "#ã“ã“ã§ã‚¨ãƒ©ãƒ¼ã‚’ã¯ã„ãŸå ´åˆã€å½¢æ…‹ç´ è§£æã‚’ä¼´ã†å‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚\n",
    "import MeCab\n",
    "mecab = MeCab.Tagger()\n",
    "print(mecab.parse(\"ç§ã¯è‡ªç„¶è¨€èªå‡¦ç†åˆ†é‡ã®ç ”ç©¶ã‚’ã—ã¦ã„ã¾ã™ã€‚\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARCå½¢å¼ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url  \\\n",
      "0             http://feathercloud.blks.jp/faq/04/01/   \n",
      "1  http://fline.biz/list/detail_gourmet/post-108.php   \n",
      "2     http://gagon.com/videos/xha/xha023/xhs023.html   \n",
      "3      http://ikechang.com/news/2008/news0809j5.html   \n",
      "4                               http://ks-com.co.jp/   \n",
      "\n",
      "                                       title             timestamp  \\\n",
      "0                  Feathercloud [ã‚ˆãã‚ã‚‹è³ªå•-FAQ]  2023-11-28T10:19:01Z   \n",
      "1  ãƒ›ãƒ¼ãƒ ãƒ©ã‚¤ãƒ•ã‚µãƒãƒ¼ãƒˆï½œä¸€é–¢å¸‚ã€€ãƒ•ãƒªãƒ¼ãƒšãƒ¼ãƒ‘ãƒ¼ã€FREE LINEã€‘åœ°åŸŸæƒ…å ±ãŒæº€è¼‰ï¼  2023-11-28T11:18:14Z   \n",
      "2        Fetish Factory Videos Hair Scene019  2023-11-28T10:21:42Z   \n",
      "3           å¤©ç«¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆå¹³æˆ20å¹´9æœˆãƒ»ç¬¬5é€±ï¼‰ - å¤©ç«¥å¸‚ã®è¦³å…‰ã‚¬ã‚¤ãƒ‰  2023-11-28T10:52:42Z   \n",
      "4                              K'S COM GROUP  2023-11-28T10:07:26Z   \n",
      "\n",
      "                                             content  \n",
      "0  b'<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01...  \n",
      "1  b'<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0...  \n",
      "2  b'<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0...  \n",
      "3  b'<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01...  \n",
      "4  b'<?xml version=\"1.0\" encoding=\"Shift_JIS\"?>\\n...  \n"
     ]
    }
   ],
   "source": [
    "# WARCå½¢å¼ã®èª­ã¿è¾¼ã¿\n",
    "import pandas as pd\n",
    "path=\"/data/fairy_hdd4/takumi/matsuo_llm/Dataset_for_BTM/01web_codes/processes/CC-MAIN-2023-50_text_batch0_CC-MAIN-2023-50-batch0-iter0.parquet\"\n",
    "df=pd.read_parquet(path)\n",
    "print(df.head())\n",
    "df=df[[\"content\",\"url\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=df['content'].iloc[1].decode(\"UTF-8\")\n",
    "text1 = extract(text1, include_tables=False,target_lang='ja',favour_precision=True) #trafilaturaã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¤‰æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¤‰æ›\n",
    "texts=[]\n",
    "euc_jp_list=[3,29,73]\n",
    "utf_list=[0,1,2,5,8,10,11,12,14,16,17,19,21,22,24,27,30,31,32,33,34,36,37,38,\n",
    "          39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,55,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,91,94,95,96,97,99,100,101,103]\n",
    "shift_jis_list=[4,7,9,13,15,18,23,25,26,28,35,54,56,74,90,92,93,102] #6,20,98\n",
    "for i in range(len(df)):\n",
    "    # print(i)\n",
    "    if i in euc_jp_list:\n",
    "        text=df['content'].iloc[i].decode(\"EUC-JP\")\n",
    "        #print(\"EUC-JP\")\n",
    "        texts.append(text)\n",
    "    elif i in shift_jis_list:\n",
    "        text=df['content'].iloc[i].decode(\"Shift_JIS\")\n",
    "        #print(\"Shift_JIS\")\n",
    "        texts.append(text)\n",
    "    elif i in [6,20,98]:\n",
    "        #text=df['content'].iloc[i].decode(\"shift_jis\")\n",
    "        # print(\"no\")\n",
    "        pass\n",
    "    else:# i in utf_list:\n",
    "        text=df['content'].iloc[i].decode(\"UTF-8\")\n",
    "        # print(\"UTF-8\")\n",
    "        texts.append(text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean_textãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
    "- lang\n",
    "- url\n",
    "- trafilatura\n",
    "\n",
    "- hoji_filter(åˆ†å‰²ã™ã¹ãï¼Ÿ)\n",
    "    - ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãªã©\n",
    "- prob_hoji_filter\n",
    "- normlize\n",
    "- repetition1\n",
    "- repetition2\n",
    "- line-wise(å‡¦ç†ã‚’å¢—ã‚„ã™)\n",
    "- remove_multi_headers\n",
    "- rule_based_text_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#é †ç•ª\n",
    "\n",
    "# def clean_text(text):\n",
    "#     text=extract_by_trafilatura(text)\n",
    "#     text=hojifilter(text)\n",
    "#     text=normalize(text)\n",
    "#     text=repetition1(text)\n",
    "#     text=repetition2(text)ã€€#å‡¦ç†é‡ã„\n",
    "#     text=line_wise(text)ã€€#å‡¦ç†é‡ã„\n",
    "#     text=remove_multi_headers(text)\n",
    "#     text=rule_based_text_checker(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é–¢æ•°ï¼šextract_by_trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trafilatura import fetch_url, extract\n",
    "def extract_by_trafilatura(text):\n",
    "    text = extract(text1, include_tables=False,target_lang='ja',favour_precision=True) #trafilaturaã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text checker\n",
    "def char_is_hiragana(c):\n",
    "    return u'\\u3040' <= c <= u'\\u309f'\n",
    "\n",
    "def contains_hiragana(s):\n",
    "    return any(char_is_hiragana(c) for c in s)\n",
    "\n",
    "def check(s):\n",
    "    if not contains_hiragana(s):\n",
    "        return \"\"\n",
    "\n",
    "    else:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é–¢æ•°ï¼šhojichar_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hojichar\n",
    "from hojichar import Compose, document_filters\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "sys.path.append(\"/data/fairy_hdd4/takumi/matsuo_llm/Dataset_for_BTM/01web_codes/src\")\n",
    "\n",
    "base_path = \"/data/fairy_hdd4/takumi/matsuo_llm/Dataset_for_BTM/01web_codes/src/cleaner/hoji_dict/\"\n",
    "cleaner = Compose([\n",
    "    document_filters.JSONLoader(key=\"text\"),\n",
    "    document_filters.AcceptJapanese(),\n",
    "    document_filters.DocumentLengthFilter(min_doc_len=10, max_doc_len=50000),\n",
    "    document_filters.MaskPersonalInformation(),\n",
    "    document_filters.JSONDumper(),\n",
    "])\n",
    "\n",
    "with open(base_path + \"adult_keywords_ja.txt\") as f:\n",
    "    adult_keywords_ja = f.read().splitlines()\n",
    "with open(base_path + \"adult_keywords_en.txt\") as f:\n",
    "    adult_keywords_en = f.read().splitlines()\n",
    "with open(base_path + \"advertisement_keywords_ja.txt\") as f:\n",
    "    advertisement_keywords_ja = f.read().splitlines()\n",
    "\n",
    "noise_keywords = adult_keywords_ja + adult_keywords_en + advertisement_keywords_ja\n",
    "noise_keywords = list(set(noise_keywords))\n",
    "noise_keywords = [k for k in noise_keywords if k != \"\"]\n",
    "\n",
    "prob_cleaner = Compose([\n",
    "    document_filters.JSONLoader(key=\"text\"),\n",
    "    # document_filters.DiscardRareKuten(),  # æ—¥æœ¬èªä»¥å¤–ã‚’æ¶ˆã™\n",
    "    document_filters.DiscardAdultContentJa(\n",
    "        base_path + \"adult_keywords_ja.txt\"),\n",
    "    document_filters.DiscardAdultContentEn(\n",
    "        base_path + \"adult_keywords_en.txt\"\n",
    "    ),\n",
    "    # document_filters.DiscardDiscriminationContentJa(\n",
    "    #    base_path + \"discrimination_keywords_ja.txt\"\n",
    "    # ),\n",
    "    # document_filters.DiscardViolenceContentJa(\n",
    "    #    base_path + \"violence_keywords_ja.txt\"\n",
    "    # ),\n",
    "    document_filters.DiscardBBSComments(),\n",
    "    document_filters.DiscardAds(\n",
    "        base_path + \"advertisement_keywords_ja.txt\",\n",
    "        max_allowed_num=10,\n",
    "    ),\n",
    "    document_filters.JSONDumper(),\n",
    "])\n",
    "\n",
    "def hoji_filter(text):\n",
    "    d = {\"text\": text}\n",
    "    parsed = cleaner(json.dumps(d))\n",
    "    if parsed == \"\":\n",
    "        return \"\"\n",
    "    text = json.loads(parsed)[\"text\"]\n",
    "    return text\n",
    "\n",
    "\n",
    "# ç¢ºç‡çš„ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ãŒï½¤ã¡ã‚‡ã£ã¨ã‚¤ãƒã‚¤ãƒ\n",
    "def prob_hoji_filter(text, survive_ratio=0.5):\n",
    "    # ngãƒ¯ãƒ¼ãƒ‰é¡ã«ã¤ã„ã¦ã¯ï½¤ç¢ºç‡çš„ã«å‡¦ç†ã™ã‚‹\n",
    "    if random.random() < survive_ratio:\n",
    "        return text\n",
    "\n",
    "    d = {\"text\": text}\n",
    "    parsed = prob_cleaner(json.dumps(d))\n",
    "    if parsed == \"\":\n",
    "        return \"\"\n",
    "    text = json.loads(parsed)[\"text\"]\n",
    "    return text\n",
    "\n",
    "\n",
    "def prob_filter(text, ratio=5):\n",
    "    # é…ã„\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã®é•·ã•\n",
    "    total_length = len(text)\n",
    "\n",
    "    # noise_keywordsã®å„å˜èªãŒãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹å›æ•°ã®åˆè¨ˆã‚’è¨ˆç®—\n",
    "    noise_length = sum(text.count(word) for word in noise_keywords)\n",
    "\n",
    "    # noise_keywordsãŒãƒ†ã‚­ã‚¹ãƒˆã«å ã‚ã‚‹å‰²åˆã‚’è¨ˆç®—\n",
    "    noise_ratio = noise_length / total_length\n",
    "\n",
    "    # print(noise_ratio*ratio)\n",
    "    # å‰²åˆãŒé–¾å€¤ä»¥ä¸Šã§ã‚ã‚Œã°ã€ç¢ºç‡çš„ã«\"\"ã‚’è¿”ã™\n",
    "    if noise_ratio*ratio > random.random():\n",
    "        return \"\"\n",
    "    else:\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é–¢æ•°ï¼šnormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/lighttransport/japanese-llama-experiment/blob/main/02_normalize/text_normalizer.py\n",
    "\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "# Added some modification for Japanese text normalization.\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "#\n",
    "# NOTE: å®Ÿéš›ã®ã¨ã“ã‚, 01_normalize ã§ã¯ PUNCT å¤‰æ›ã¯è¡Œã‚ãªã„.\n",
    "# dedup æ™‚ã«ã¯è¡Œã†(PUNCT é™¤å»)\n",
    "# TODO: dedup æ™‚, æ—¥æœ¬èªã®å ´åˆã¯å¥èª­ç‚¹ã‚’å«ã‚“ã ã¾ã¾ã®ã»ã†ãŒè‰¯ã„ã‚ˆã†ãªæ°—ã‚‚ã™ã‚‹ãŒã©ã†ã‹.\n",
    "#\n",
    "\n",
    "UNICODE_PUNCT = {\n",
    "    # æ—¥æœ¬èªã®å ´åˆã¯å¥èª­ç‚¹ã¯ã€ã€‚ã®ã¾ã¾ãŒã‚ˆã„ã§ã—ã‚‡ã†\n",
    "    \"ã€‚\": \"ã€‚\",\n",
    "    \"ã€\": \"ã€\",\n",
    "    \"ï¼Œ\": \"ã€\",\n",
    "\n",
    "    \"â€\": '\"',\n",
    "    \"â€\": '\"',\n",
    "    \"â€œ\": '\"',\n",
    "    \"Â«\": '\"',\n",
    "    \"Â»\": '\"',\n",
    "    \"ï¼‘\": '\"',\n",
    "    \"ã€\": '\"',\n",
    "    \"ã€Œ\": '\"',\n",
    "    \"ã€Š\": '\"',\n",
    "    \"ã€‹\": '\"',\n",
    "    \"Â´\": \"'\",\n",
    "    \"âˆ¶\": \":\",\n",
    "    \"ï¼š\": \":\",\n",
    "    \"ï¼Ÿ\": \"?\",\n",
    "    \"ï¼\": \"!\",\n",
    "    \"ï¼ˆ\": \"(\",\n",
    "    \"ï¼‰\": \")\",\n",
    "    \"ï¼›\": \";\",\n",
    "    \"â€“\": \"-\",\n",
    "    \"â€”\": \" - \",\n",
    "    \"ï¼\": \". \",\n",
    "    \"ï½\": \"~\",\n",
    "    \"â€™\": \"'\",\n",
    "    \"â€¦\": \"...\",\n",
    "    \"â”\": \"-\",\n",
    "    \"ã€ˆ\": \"<\",\n",
    "    \"ã€‰\": \">\",\n",
    "    \"ã€\": \"[\",\n",
    "    \"ã€‘\": \"]\",\n",
    "    \"ï¼…\": \"%\",\n",
    "    \"â–º\": \"-\",\n",
    "}\n",
    "\n",
    "UNICODE_PUNCT_RE = re.compile(f\"[{''.join(UNICODE_PUNCT.keys())}]\")\n",
    "\n",
    "\n",
    "def replace_unicode_punct(text: str) -> str:\n",
    "    return \"\".join((UNICODE_PUNCT.get(c, c) for c in text))\n",
    "\n",
    "\n",
    "def remove_unicode_punct(text: str) -> str:\n",
    "    \"\"\"More aggressive version of replace_unicode_punct but also faster.\"\"\"\n",
    "    return UNICODE_PUNCT_RE.sub(\"\", text)\n",
    "\n",
    "\n",
    "# Reuse `strip_accents` for CJK text. Use NFKC\n",
    "def strip_accents(line: str) -> str:\n",
    "    \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "    # nfd = unicodedata.normalize(\"NFD\", line)\n",
    "    nkfc = unicodedata.normalize(\"NFKC\", line)\n",
    "    output = [c for c in nkfc if unicodedata.category(c) != \"Mn\"]\n",
    "    if len(output) == line:\n",
    "        return line\n",
    "    return \"\".join(output)\n",
    "\n",
    "\n",
    "# Build a regex matching all control characters.\n",
    "# newline(LF, 10) has meaningful infor in CJK text, so do not remove it.\n",
    "NON_PRINTING_CHARS_RE = re.compile(\n",
    "    f\"[{''.join(map(chr, list(range(0,10)) + list(range(11, 32)) + list(range(127,160))))}]\"\n",
    ")\n",
    "DIGIT_RE = re.compile(r\"\\d\")\n",
    "PUNCT_OR_NON_PRINTING_CHARS_RE = re.compile(\n",
    "    (UNICODE_PUNCT_RE.pattern +\n",
    "     NON_PRINTING_CHARS_RE.pattern).replace(\"][\", \"\")\n",
    ")\n",
    "\n",
    "\n",
    "def remove_non_printing_char(text: str) -> str:\n",
    "    return NON_PRINTING_CHARS_RE.sub(\"\", text)\n",
    "\n",
    "\n",
    "def normalize_spacing_for_tok(text: str, language: str = \"en\") -> str:\n",
    "    res = (\n",
    "        text.replace(\"\\r\", \"\")\n",
    "        # remove extra spaces\n",
    "        .replace(\"(\", \" (\")\n",
    "        .replace(\")\", \") \")\n",
    "        .replace(\" +\", \" \")\n",
    "    )\n",
    "    res = re.sub(r\"\\) ([\\.\\!\\:\\?\\;\\,])\", r\"\\)\\1\", res)\n",
    "    res = res.replace(\"( \", \"(\").replace(\" )\", \")\")\n",
    "    res = re.sub(r\"(\\d) \\%\", r\"\\1\\%\", res)\n",
    "    res = res.replace(\" :\", \":\").replace(\" ;\", \";\")\n",
    "    res = res.replace(\"`\", \"'\").replace(\"''\", ' \" ')\n",
    "\n",
    "    res = (\n",
    "        res.replace(\"â€\", '\"')\n",
    "        .replace(\"â€œ\", '\"')\n",
    "        .replace(\"â€\", '\"')\n",
    "        .replace(\"â€“\", \"-\")\n",
    "        .replace(\"â€”\", \" - \")\n",
    "        .replace(\" +\", \" \")\n",
    "        .replace(\"Â´\", \"'\")\n",
    "        .replace(\"([a-z])â€˜([a-z])\", r\"\\1'\\2/\")\n",
    "        .replace(\"([a-z])â€™([a-z])\", r\"\\1'\\2/\")\n",
    "        .replace(\"â€˜\", '\"')\n",
    "        .replace(\"â€š\", '\"')\n",
    "        .replace(\"â€™\", '\"')\n",
    "        .replace(\"''\", '\"')\n",
    "        .replace(\"Â´Â´\", '\"')\n",
    "        .replace(\"â€¦\", \"...\")\n",
    "        # French quotes\n",
    "        .replace(\" Â« \", ' \"')\n",
    "        .replace(\"Â« \", '\"')\n",
    "        .replace(\"Â«\", '\"')\n",
    "        .replace(\" Â» \", '\" ')\n",
    "        .replace(\" Â»\", '\"')\n",
    "        .replace(\"Â»\", '\"')\n",
    "        # handle pseudo-spaces\n",
    "        .replace(\" %\", \"%\")\n",
    "        .replace(\"nÂº \", \"nÂº \")\n",
    "        .replace(\" :\", \":\")\n",
    "        .replace(\" ÂºC\", \" ÂºC\")\n",
    "        .replace(\" cm\", \" cm\")\n",
    "        .replace(\" ?\", \"?\")\n",
    "        .replace(\" !\", \"!\")\n",
    "        .replace(\" ;\", \";\")\n",
    "        .replace(\", \", \", \")\n",
    "        .replace(\" +\", \" \")\n",
    "        .replace(\"ï¼\", \". \")\n",
    "    )\n",
    "    # English \"quotation,\" followed by comma, style\n",
    "    if language == \"en\":\n",
    "        res = re.sub(r\"\\\"([,\\.]+)\", r\"\\1\\\"\", res)\n",
    "    # Czech is confused\n",
    "    elif language == \"cs\" or language == \"cz\":\n",
    "        pass\n",
    "    # German/Spanish/French \"quotation\", followed by comma, style\n",
    "    else:\n",
    "        res = res.replace(',\"', '\",')\n",
    "        res = re.sub(\n",
    "            r\"(\\.+)\\\"(\\s*[^<])\", r\"\\\"\\1\\2\", res\n",
    "        )  # don't fix period at end of sentence\n",
    "\n",
    "    if (\n",
    "        language == \"de\"\n",
    "        or language == \"es\"\n",
    "        or language == \"cz\"\n",
    "        or language == \"cs\"\n",
    "        or language == \"fr\"\n",
    "    ):\n",
    "        res = re.sub(r\"(\\d) (\\d)\", r\"\\1,\\2\", res)\n",
    "    else:\n",
    "        res = re.sub(r\"(\\d) (\\d)\", r\"\\1.\\2\", res)\n",
    "    return res\n",
    "\n",
    "\n",
    "# NOTE accent=True will do NFKC normalization\n",
    "# NOTE: set punct=0(no zenkaku->hankaku conversion) hby default for Japanese dataset\n",
    "def normalize(line: str, accent=True, case=False, numbers=False, punct=0) -> str:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return line\n",
    "    if case:\n",
    "        line = line.lower()\n",
    "\n",
    "    # FIXME: Always apply NKFC normalization for CJK text.\n",
    "    if accent:\n",
    "        line = strip_accents(line)\n",
    "    if numbers:\n",
    "        line = DIGIT_RE.sub(\"0\", line)\n",
    "    if punct == 1:\n",
    "        line = replace_unicode_punct(line)\n",
    "    elif punct == 2:\n",
    "        line = remove_unicode_punct(line)\n",
    "    line = remove_non_printing_char(line)\n",
    "    return line\n",
    "\n",
    "\n",
    "def slow_normalize_for_dedup(line: str) -> str:\n",
    "    return normalize(line, accent=False, case=True, numbers=True, punct=2)\n",
    "\n",
    "\n",
    "def normalize_for_dedup(line: str) -> str:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return line\n",
    "    # case\n",
    "    line = line.lower()\n",
    "    # numbers\n",
    "    line = DIGIT_RE.sub(\"0\", line)\n",
    "    line = PUNCT_OR_NON_PRINTING_CHARS_RE.sub(\"\", line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é–¢æ•°ï¼šrepetition_filter1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# å˜ä¸€æ–‡å­—ã®ç¹°ã‚Šè¿”ã—200----------------------------------------\n",
    "\n",
    "\n",
    "def repetition_filter1(text, threshold_ratio=0.3):\n",
    "    # # æ–‡å­—ã®ç¹°ã‚Šè¿”ã—ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹è¾æ›¸\n",
    "    char_count = {}\n",
    "    # å„æ–‡å­—ã®ç¹°ã‚Šè¿”ã—å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    for char in text:\n",
    "        if char in char_count:\n",
    "            char_count[char] += 1\n",
    "        else:\n",
    "            char_count[char] = 1\n",
    "    # ratio%ä»¥ä¸Šç¹°ã‚Šè¿”ã•ã‚Œã‚‹æ–‡å­—ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "    #print(text)\n",
    "    repeated_chars = [char for char, count in char_count.items() if count >= threshold_ratio*len(text)]\n",
    "    # ç¹°ã‚Šè¿”ã•ã‚Œã‚‹æ–‡å­—ãŒãªã‘ã‚Œã°matrix_tempã«è¿½åŠ \n",
    "    if not repeated_chars:\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def repetition_filter2(text, thresholds={\n",
    "    'line_dup': 0.30,\n",
    "    'paragraph_dup': 0.30,\n",
    "    'char_in_line_dup': 0.20,\n",
    "    'char_in_paragraph_dup': 0.20,\n",
    "    '2-gram': 0.20,\n",
    "    '3-gram': 0.18,\n",
    "    '4-gram': 0.16,\n",
    "    '5-gram': 0.15,\n",
    "    '6-gram': 0.14,\n",
    "    '7-gram': 0.13,\n",
    "    '8-gram': 0.12,\n",
    "    '9-gram': 0.11,\n",
    "        '10-gram': 0.10}):\n",
    "    if text == \"\":\n",
    "        return \"\"\n",
    "    # æ®µè½ã¨è¡Œã«åˆ†å‰²\n",
    "    paragraphs = text.split('\\n')\n",
    "    lines = text.replace('\\n', ' ').split('ã€‚')\n",
    "\n",
    "    # æ®µè½ã¨è¡Œã®é‡è¤‡ç‡ã‚’è¨ˆç®—\n",
    "    paragraph_dup_rate = calc_dup_rate(paragraphs)\n",
    "    line_dup_rate = calc_dup_rate(lines)\n",
    "\n",
    "    # æ–‡å­—ã«å«ã¾ã‚Œã‚‹é‡è¤‡ã®å‰²åˆã‚’è¨ˆç®—\n",
    "    char_in_paragraph_dup_rate = calc_char_dup_rate(paragraphs, text)\n",
    "    char_in_line_dup_rate = calc_char_dup_rate(lines, text)\n",
    "\n",
    "    # n-gramã®é‡è¤‡ç‡ã‚’è¨ˆç®—\n",
    "    ngram_dup_rates = {}\n",
    "    for n in range(2, 11):\n",
    "        ngrams = extract_ngrams(text.replace('\\n', ''), n)\n",
    "        if n < 5:\n",
    "            # æœ€é »å‡ºã®n-gramã®å‡ºç¾å›æ•°ã‚’è¨ˆç®—\n",
    "            ngram_dup_rates[n] = calc_max_freq_rate(ngrams)\n",
    "        else:\n",
    "            # 2å›ä»¥ä¸Šå‡ºç¾ã™ã‚‹n-gramã®ç·å‡ºç¾å›æ•°ã‚’è¨ˆç®—\n",
    "            ngram_dup_rates[n] = calc_total_dup_freq_rate(ngrams)\n",
    "\n",
    "    # å„æŒ‡æ¨™ãŒé–¾å€¤ã‚’è¶…ãˆã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "    if (line_dup_rate > thresholds['line_dup'] or\n",
    "        paragraph_dup_rate > thresholds['paragraph_dup'] or\n",
    "        char_in_line_dup_rate > thresholds['char_in_line_dup'] or\n",
    "        char_in_paragraph_dup_rate > thresholds['char_in_paragraph_dup'] or\n",
    "            any(ngram_dup_rates[n] > thresholds[f'{n}-gram'] for n in range(2, 11))):\n",
    "        # print(text)\n",
    "        return \"\"\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def calc_dup_rate(items):\n",
    "    counter = Counter(items)\n",
    "    total = len(items)\n",
    "    dup_count = sum(1 for count in counter.values() if count > 1)\n",
    "    return dup_count / total if total > 0 else 0\n",
    "\n",
    "\n",
    "def calc_char_dup_rate(items, text):\n",
    "    counter = Counter(items)\n",
    "    total_chars = len(text)\n",
    "    dup_chars = sum(len(item) * (count - 1)\n",
    "                    for item, count in counter.items() if count > 1)\n",
    "    return dup_chars / total_chars if total_chars > 0 else 0\n",
    "\n",
    "\n",
    "def extract_ngrams(text, n):\n",
    "    return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "\n",
    "\n",
    "def calc_max_freq_rate(ngrams):\n",
    "    counter = Counter(ngrams)\n",
    "    total = len(ngrams)\n",
    "    max_freq = max(counter.values())\n",
    "    return max_freq / total if total > 0 else 0\n",
    "\n",
    "\n",
    "def calc_total_dup_freq_rate(ngrams):\n",
    "    counter = Counter(ngrams)\n",
    "    total = len(ngrams)\n",
    "    dup_freq = sum(count - 1 for count in counter.values() if count > 1)\n",
    "    return dup_freq / total if total > 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é–¢æ•°ï¼šline_wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import functools\n",
    "from ja_sentence_segmenter.common.pipeline import make_pipeline\n",
    "from ja_sentence_segmenter.concatenate.simple_concatenator import concatenate_matching\n",
    "from ja_sentence_segmenter.normalize.neologd_normalizer import normalize\n",
    "from ja_sentence_segmenter.split.simple_splitter import split_newline, split_punctuation\n",
    "\n",
    "\n",
    "def text_to_paragraph_sentences(text: str) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Split a text into paragraphs and sentences.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to split.\n",
    "\n",
    "    Returns:\n",
    "        List[List[str]]: The paragraphs and sentences.\n",
    "    \"\"\"\n",
    "    paragraphs = paragraph_split(text)\n",
    "    return [sentence_split(paragraph) for paragraph in paragraphs]\n",
    "\n",
    "\n",
    "def paragraph_split(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split a text into paragraphs.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to split.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: The paragraphs.\n",
    "    \"\"\"\n",
    "    return text.split(\"\\n\")\n",
    "\n",
    "\n",
    "split_punc2 = functools.partial(split_punctuation, punctuations=r\"ã€‚!?\")\n",
    "concat_tail_te = functools.partial(\n",
    "    concatenate_matching, remove_former_matched=False)\n",
    "segmenter = make_pipeline(normalize, split_newline,\n",
    "                          concat_tail_te, split_punc2)\n",
    "\n",
    "\n",
    "def sentence_split(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split a text into sentences.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to split.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: The sentences.\n",
    "    \"\"\"\n",
    "    return list(segmenter(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"/data/fairy_hdd4/takumi/matsuo_llm/Dataset_for_BTM/01web_codes/src/cleaner\")\n",
    "# from .splitter import text_to_paragraph_sentences\n",
    "# # from .text_normalizer import normalize\n",
    "# #from . import text_checker\n",
    "# from . import rule_based_line_checker\n",
    "# # from . import parts_filter\n",
    "# from .line_end_cleaner import clean_line_endings\n",
    "# #from .hojichar_filter import hoji_filter, prob_hoji_filter, prob_filter\n",
    "# #from . import rule_based_text_checker\n",
    "# from .line_dedup import remove_multi_headers\n",
    "# #from . import repeated_phrase\n",
    "# # try:\n",
    "#     from .TextClassifier import TextClassifier\n",
    "#     classifier = TextClassifier()\n",
    "# except:\n",
    "#     print(\"error loading TextClassifier. install fasttext to use it\")\n",
    "\n",
    "def text_to_cleaned_paragraphs(text):\n",
    "    text = normalize(text)  # æ­£è¦åŒ–\n",
    "    # text = text_checker.check(text)  # ã²ã‚‰ãŒãªã‚’å«ã¾ãªã„ãƒ†ã‚­ã‚¹ãƒˆã¯é™¤å¤–\n",
    "\n",
    "    # ç¹°ã‚Šè¿”ã—è¡¨ç¾ã‚’é™¤å¤– by Naito\n",
    "    text = repeated_phrase.repeated_id(text)\n",
    "    text = repeated_phrase.is_repetitive_japanese(\n",
    "        text)  # n-gramã®è¨ˆç®—(è¨ˆç®—é‡ãŒå¤šãã†ãªå ´åˆã€å‰Šã‚‹)\n",
    "\n",
    "    # ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã¨æ–‡ç« ã«åˆ†å‰²\n",
    "    paragraphs = text_to_paragraph_sentences(text)\n",
    "\n",
    "    new_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        new_lines = []\n",
    "        old_line = \"\"\n",
    "        for line in (paragraph):\n",
    "            # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®è¡Œãƒã‚§ãƒƒã‚¯\n",
    "            new_line = rule_based_line_checker.clean(line)\n",
    "\n",
    "            # åè©ã ã‚‰ã‘ã®lineã‚’é™¤å¤–\n",
    "            try:\n",
    "                # new_line = parts_filter.filter(new_line)\n",
    "                new_line = parts_filter.filter2(new_line)  # n-gramã«ã‚ˆã‚‹é‡è¤‡ã®é™¤å¤–\n",
    "            except:\n",
    "                pass\n",
    "            if new_line:\n",
    "                if new_line == old_line:\n",
    "                    continue\n",
    "                old_line = new_line\n",
    "                # ppl=perp_checker(new_line)\n",
    "                # print(ppl,new_line)\n",
    "                new_lines.append(new_line)\n",
    "\n",
    "        if new_lines:\n",
    "            new_paragraphs.append(new_lines)\n",
    "\n",
    "    # æ–‡æœ«ãŒï½¡ãªã©ã§ãŠã‚ã‚‰ãªã„ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ä¸­ã®æ–‡ç« ã‚’å‰Šé™¤\n",
    "    # clean_line_endings(new_paragraphs)\n",
    "\n",
    "    # ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã«ã¾ã¨ã‚ã‚‹\n",
    "    cleaned_paragraphs = []\n",
    "    old_lines = \"\"\n",
    "    for paragraph in new_paragraphs:\n",
    "        lines = \"\".join(paragraph)\n",
    "        if lines == old_lines:\n",
    "            continue\n",
    "        cleaned_paragraphs.append(lines)\n",
    "        old_lines = lines\n",
    "\n",
    "    return cleaned_paragraphs\n",
    "\n",
    "\n",
    "def clean_text(text, hoji=True):\n",
    "    if hoji:\n",
    "        # text = prob_filter(text)\n",
    "        text = hoji_filter(text)\n",
    "        text = prob_hoji_filter(text)\n",
    "\n",
    "    paragraphs = text_to_cleaned_paragraphs(text)\n",
    "    # print(\"aa\", original_text)\n",
    "    text = \"\\n\".join(paragraphs)\n",
    "\n",
    "    text = remove_multi_headers(text)\n",
    "    text = rule_based_text_checker.clean(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def ml_clean_text(text):\n",
    "    text = prob_hoji_filter(text)\n",
    "    text = hoji_filter(text)\n",
    "    text = classifier.clean(text)\n",
    "    if text != \"\":\n",
    "        text = clean_text(text, hoji=False)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### noise_line(rule-base-line-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/lighttransport/japanese-llama-experiment/blob/main/03_clean_step1/clean_text.py\n",
    "\n",
    "# TODO: hard codeã—ãªã„\n",
    "import random\n",
    "import unicodedata\n",
    "import re\n",
    "broken_sentence_endings = \"\"\"\n",
    "...\n",
    "... \n",
    "...ã€€\n",
    "\\\"\n",
    "'\n",
    "[â€¦]\n",
    "è©³ç´°ã‚’è¦‹ã‚‹\n",
    "ã™ã¹ã¦è¡¨ç¤ºã™ã‚‹\n",
    "ã™ã‚‹ã¾ã¨ã‚\n",
    "å…¬å¼ã‚µã‚¤ãƒˆ\n",
    "ã«ã‚¢ã‚¯ã‚»ã‚¹!\n",
    "ãŠâ€‹å•â€‹ã„â€‹åˆâ€‹ã‚â€‹ã›â€‹\n",
    "ã‚¢ã‚¯ã‚»ã‚¹ãƒãƒƒãƒ—\n",
    "é€æ–™ç„¡æ–™!\n",
    "ã‚³ãƒ¡ãƒ³ãƒˆ(0)\n",
    "ãƒˆãƒ©ãƒƒã‚¯ãƒãƒƒã‚¯ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚\n",
    "ã“ã®è¨˜äº‹ã¸ã®ã‚³ãƒ¡ãƒ³ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
    "è¿”ä¿¡ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã™ã‚‹ã€‚\n",
    "URL\n",
    "ãŠå•ã„åˆã‚ã›\n",
    ",?\n",
    "å‘¨è¾ºæ–½è¨­\n",
    "ç¶šãã‚’è¡¨ç¤º\n",
    "éŸ³æ¥½å‡ºç‰ˆç¤¾æ±‚äºº\n",
    "Check\n",
    "ãƒãƒƒãƒ—MAP\n",
    "æœªå…¥åŠ›é …ç›®ãŒã‚ã‚Šã¾ã™\n",
    "SNS\n",
    "â—facebookâ†’\n",
    "â—twitterâ†’\n",
    "ABOUT US\n",
    "æ±‚äººæƒ…å ±\n",
    "1ä»¶ä¸­1ï½1ä»¶ã‚’è¡¨ç¤º\n",
    "äº‹æ¥­å†…å®¹\n",
    "ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸\n",
    "ç‰¹é›†\n",
    "è³ƒè²¸æƒ…å ±\n",
    "æ¡ä»¶ã‚’é¸ã¶\n",
    "ç¾åœ¨ã®é¸æŠã‚¨ãƒªã‚¢\n",
    "æ²¿ç·šã‹ã‚‰é¸ã¶\n",
    "ã‚¨ãƒªã‚¢ã‚’é¸ã³ãªãŠã™\n",
    "ã•ã‚‰ã«ã‚¨ãƒªã‚¢ã‚’çµã‚‹\n",
    "â–³ãƒšãƒ¼ã‚¸TOP\n",
    "å…¬å¼\n",
    "About\n",
    "search\n",
    "ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«\n",
    "æ›´æ–°æƒ…å ±ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹\n",
    "æ¤œç´¢:\n",
    "é–¢é€£è¨˜äº‹\n",
    "ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã™ã‚‹\n",
    "å‹é”ã«æ•™ãˆã‚‹\n",
    "ãŠå•ã„åˆã›\n",
    "CLOSE\n",
    "My account\n",
    "Contact Us\n",
    "ã”åˆ©ç”¨æ–¹æ³•\n",
    "ä¼šå“¡è¦ç´„\n",
    "ç‰¹å®šå•†å–å¼•æ³•ã«åŸºã¥ãè¡¨è¨˜\n",
    "çµ„ç¹”æ¦‚è¦\n",
    "<å‰ã®20ä»¶1æ¬¡ã®20ä»¶>\n",
    "å®˜å…¬åºè‡¨æ™‚è·å“¡\n",
    "æ¹˜å—å›½éš›æ‘ä»•äº‹\n",
    "ã‚¿ã‚¤ã‚¢ãƒƒãƒ—åºƒå‘Šæ²è¼‰\n",
    "åºƒå‘Šã‚’æ²è¼‰ã—ã¾ã›ã‚“ã‹?\n",
    "ä¼šç¤¾æ¦‚è¦\n",
    "Pocket\n",
    "ç›®æ¬¡\n",
    "TOP\n",
    "ã‹ã‚‰è¦‹ã¤ã‘ã‚‹\n",
    "ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼\n",
    "åˆ©ç”¨è¦ç´„\n",
    "All Rights Reserved.\n",
    "æ»åœ¨å¾ŒæŠ•ç¨¿ã•ã‚ŒãŸã‚‚ã®ã§ã™ã€‚\n",
    "æ³•ä»¤é †å®ˆã¨çŠ¯ç½ªæŠ‘æ­¢ã®ãŸã‚ã«\n",
    "ãƒ¡ãƒ«ã‚«ãƒªã‚ã‚“ã—ã‚“ãƒ»ã‚ã‚“ãœã‚“å®£è¨€!\n",
    "å½ãƒ–ãƒ©ãƒ³ãƒ‰å“æ’²æ»…ã¸ã®å–ã‚Šçµ„ã¿\n",
    "ã‚¹ã‚¿ãƒƒãƒ•ãƒ–ãƒ­ã‚°\n",
    "è³ƒè²¸ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸\n",
    "ç‰©ä»¶ã‚«ã‚¿ãƒ­ã‚°\n",
    "ãƒ›ãƒ¼ãƒ \n",
    "ãƒ¡ãƒ‹ãƒ¥ãƒ¼\n",
    "ä»²ä»‹æ‰‹æ•°æ–™æœ€å¤§ç„¡æ–™\n",
    "å£ã‚³ãƒŸã‚’ã‚‚ã£ã¨\n",
    "è©³ç´°æƒ…å ±\n",
    "ãƒ„ã‚¤ãƒ¼ãƒˆ\n",
    "ãƒ¡ãƒ«ãƒã‚¬\n",
    "ç™»éŒ²å•†æ¨™ã§ã™ã€‚\n",
    "ã‚ˆã†ã“ãã‚²ã‚¹ãƒˆã•ã‚“\n",
    "â†’\n",
    "[...]\n",
    "ã®å…ˆé ­ã¸\n",
    "å€‹äººæƒ…å ±ã®å–ã‚Šæ‰±ã„ã«ã¤ã„ã¦\n",
    "ä¸­å¤ãƒãƒ³ã‚·ãƒ§ãƒ³\n",
    "Â»\n",
    "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’ãŠå¿˜ã‚Œã®æ–¹\n",
    "ä¸Šéƒ¨ã¸\n",
    "JavaScriptã‚’æœ‰åŠ¹ã«ã—ã¦ã”åˆ©ç”¨ä¸‹ã•ã„.\n",
    "ãƒ­ã‚°ã‚¤ãƒ³ã§ããªã„æ™‚ã¯?\n",
    "é‹å–¶ä¼šç¤¾\n",
    "ä¼æ¥­æƒ…å ±\n",
    "å•ã„åˆã‚ã›\n",
    "ä¸€è¦§ã¸\n",
    "ãƒ˜ãƒ«ãƒ—\n",
    "ç‰©ä»¶\n",
    "ã‚‚ã£ã¨è¦‹ã‚‹\n",
    "ã‚’æ¢ã™\n",
    "ãƒãƒ³ã‚·ãƒ§ãƒ³\n",
    "æ¤œç´¢\n",
    "åº—\n",
    "æ¤œç´¢ã®ãƒ’ãƒ³ãƒˆ:\n",
    "æ–°è¦ã®è¡¨ç¤º\n",
    "Vths\n",
    "Id\n",
    "è¦‹å‡ºã—èª\n",
    "Yomi\n",
    "å¤§åˆ†é¡1\n",
    "å°åˆ†é¡2\n",
    "Pos\n",
    "Actions\n",
    "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹\n",
    "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰\n",
    "æ¬¡å›ã‹ã‚‰è‡ªå‹•ãƒ­ã‚°ã‚¤ãƒ³ã‚’ã™ã‚‹\n",
    "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å¿˜ã‚ŒãŸæ–¹ã¯ã“ã¡ã‚‰\n",
    "ä¸å‹•ç”£ç„¡æ–™æŸ»å®š\n",
    "ä½ã¾ã„ç´¹ä»‹ãƒ–ãƒ­ã‚°\n",
    "ğŸ—£naamal\n",
    "â‹†ã€‚* â‹†ã€‚* â‹†\n",
    "122æŠ•ç¨¿\n",
    "ã‚µã‚¤ãƒˆã¯?\n",
    "ãƒˆãƒƒãƒ—\n",
    "ãƒ‹ãƒ¥ãƒ¼ã‚¹\n",
    "æ—¥ç¨‹\n",
    "è‘—ä½œæ¨©\n",
    "è¨˜äº‹ãƒ»å†™çœŸãƒ»å‹•ç”»ã®åˆ©ç”¨ç”³è¾¼\n",
    "æ¡ç”¨æƒ…å ±\n",
    "ã‚µã‚¤ãƒˆä¸€è¦§\n",
    "é–¢é€£æƒ…å ±\n",
    "ã®æŠ•ç¨¿\n",
    "ãƒšãƒ¼ã‚¸ãƒˆãƒƒãƒ—ã¸\n",
    "è¨˜äº‹ã‚’èª­ã‚€\n",
    "é‹å–¶è€…æƒ…å ±\n",
    "Twitterå…¬å¼\n",
    "ã‚‚ã£ã¨ã¿ã‚‹\n",
    "æŠ•ç¨¿ã™ã‚‹\n",
    "åº—èˆ—TOP\n",
    "ã‚¹ã‚¿ãƒƒãƒ•ãƒ»é–‹ç™ºè€…å‹Ÿé›†\n",
    "ãƒ˜ãƒ«ãƒ—ãƒšãƒ¼ã‚¸\n",
    "æœ€è¿‘ã®ã‚³ãƒ¡ãƒ³ãƒˆ\n",
    "ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼\n",
    "ã®æƒ…å ±:\n",
    "ã®æƒ…å ±\n",
    "ã„ã„ã­\n",
    "ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³\n",
    "|BOOKED\n",
    "HOME\n",
    "ã‚ˆãã‚ã‚‹è³ªå•\n",
    "BLOG\n",
    "...\n",
    "å•ã„åˆã‚ã›ãã ã•ã„ã€‚\n",
    "ä¼‘è¨ºæ—¥\n",
    "æœ¨æ›œæ—¥ãƒ»æ—¥æ›œæ—¥ãƒ»ç¥æ—¥\n",
    "all rights reserved.\n",
    "</tr>\n",
    "</div>\n",
    "æµ®æ°—èª¿æŸ»\n",
    "ã‚«ãƒ†ã‚´ãƒªãƒ¼\n",
    "ãŠçŸ¥ã‚‰ã›\n",
    "æ–°ç€ã‚¢ã‚¤ãƒ†ãƒ \n",
    "ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–\n",
    "æœˆ\n",
    "Search\n",
    "TOPã¸\n",
    "TOPãƒšãƒ¼ã‚¸ã¸\n",
    "Menu\n",
    "Return Top\n",
    "æºå¸¯ç‰ˆã¯ã“ã¡ã‚‰\n",
    "ä¼šç¤¾æƒ…å ±\n",
    "åˆã‚ã¦ã®æ–¹ã¸\n",
    "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "ã‚¢ã‚¯ã‚»ã‚¹\n",
    "IRæƒ…å ±\n",
    "ä¸€è¦§\n",
    "æ¡ç”¨ã‚µã‚¤ãƒˆ\n",
    "ãƒã‚¤ãƒšãƒ¼ã‚¸\n",
    "ç ”ç©¶ä¼šä¼šå“¡\n",
    "å°‚ç”¨ãƒšãƒ¼ã‚¸\n",
    "ã¯ã“ã¡ã‚‰\n",
    "ç›¸è«‡ã™ã‚‹\n",
    "ç›¸è«‡\n",
    "æ¢ã™\n",
    "äº‹ä¾‹ã‚’è¦‹ã‚‹\n",
    "ãŠå®¢æ§˜ã‹ã‚‰ã®å£°(æˆåŠŸäº‹ä¾‹)\n",
    "æ›¸ç±\n",
    "ãƒ¡ã‚¿æƒ…å ±\n",
    "ã®è©³ç´°\n",
    "Home\n",
    "ãƒ­ã‚°ã‚¤ãƒ³\n",
    "æŠ•ç¨¿ãƒ•ã‚£ãƒ¼ãƒ‰\n",
    "ã‚³ãƒ¡ãƒ³ãƒˆãƒ•ã‚£ãƒ¼ãƒ‰\n",
    "èˆ¹äº•ç·ç ”ã®ã‚µãƒ¼ãƒ“ã‚¹\n",
    "ãŠå®¢æ§˜ã‹ã‚‰ã®å£°(æˆåŠŸäº‹ä¾‹)\n",
    "ãƒ¬ãƒãƒ¼ãƒˆ\n",
    "ã‚µãƒ¼ãƒ“ã‚¹\n",
    "èˆ¹äº•ç·ç ”ã®ã‚µãƒ¼ãƒ“ã‚¹æ¦‚è¦\n",
    "ã‚»ãƒŸãƒŠãƒ¼\n",
    "çµŒå–¶ç ”ç©¶ä¼š\n",
    "ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°\n",
    "æ¥­ç¨®ãƒ»ãƒ†ãƒ¼ãƒ\n",
    "ä½å®…ãƒ»ä¸å‹•ç”£\n",
    "ãƒªãƒ•ã‚©ãƒ¼ãƒ \n",
    "ä¸å‹•ç”£\n",
    "è³ƒè²¸\n",
    "å»ºè¨­\n",
    "ç—…é™¢ãƒ»ã‚¯ãƒªãƒ‹ãƒƒã‚¯\n",
    "æ­¯ç§‘åŒ»é™¢\n",
    "ç‰¹å®šå•†å–å¼•æ³•\n",
    "ã¯ã˜ã‚ã¦ãŠè¶Šã—ã®æ–¹ã¸\n",
    "LINE\n",
    "menu\n",
    "å…è²¬äº‹é …\n",
    "é–‰ã˜ã‚‹\n",
    "èª­è€…ã«ãªã‚‹\n",
    "ãƒªãƒ³ã‚¯\n",
    "Tweet\n",
    "æ–¹æ³•ã¯ã‚³ãƒãƒ©\n",
    "ãƒˆãƒƒãƒ—ã¸æˆ»ã‚‹\n",
    "[ãƒ˜ãƒ«ãƒ—]\n",
    "ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«\n",
    "ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€‘\n",
    "æ›´æ–°ã—ã¾ã—ãŸ\n",
    "ã‚¢ãƒ¼ã‚«ã‚¤ãƒ´\n",
    "æœªåˆ†é¡\n",
    "ãŠå•åˆã›\n",
    "ã‚·ã‚§ã‚¢\n",
    "[ãŠå•ã„åˆã‚ã›]\n",
    "[åˆ©ç”¨è¦ç´„]\n",
    "[å€‹äººæƒ…å ±ä¿è­·æ–¹é‡]\n",
    "/RSS\n",
    "ã‚‚ã£ã¨èª­ã‚€\n",
    "ã®æ¤œç´¢çµæœ\n",
    "é€šè²©ã‚·ãƒ§ãƒƒãƒ—\n",
    "ãƒ•ã‚©ãƒ­ãƒ¼ã™ã‚‹\n",
    "ã‚·ã‚§ã‚¢ã™ã‚‹\n",
    "MENU\n",
    "ã“ã®ãƒ–ãƒ­ã‚°ã«ã¤ã„ã¦\n",
    "ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ›¸ã\n",
    "twitter\n",
    "facebook\n",
    "line\n",
    "ã‚³ãƒ¡ãƒ³ãƒˆ\n",
    "arrowup\n",
    "èª­ã¾ã‚Œã¦ã„ã‚‹è¨˜äº‹\n",
    "ã®ãƒ“ãƒ¥ãƒ¼\n",
    "è³ªå•ç®±\n",
    "ã‚³ãƒ¡ãƒ³ãƒˆã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«\n",
    "down\n",
    "Website\n",
    "more\n",
    "Name\n",
    "Email\n",
    "home\n",
    "æ–°ç€æƒ…å ±\n",
    "First\n",
    "ã®Q&A\n",
    "ã‚µã‚¤ãƒˆã¸\n",
    "Previous\n",
    "ä¸Šã¸\n",
    "æ›´æ–°\n",
    "Twitter\n",
    "æ‹¡å¤§\n",
    "ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n",
    "Next\n",
    "ä¼šå“¡ç™»éŒ²\n",
    "Last\n",
    "ã‚¹ã‚¿ãƒƒãƒ•ç´¹ä»‹\n",
    "1éšå±¤ãƒšãƒ¼ã‚¸\n",
    "ã‚¿ã‚¤ãƒˆãƒ«\n",
    "ã“ã®è¨˜äº‹ã‚’å‰Šé™¤ã™ã‚‹\n",
    "2éšå±¤ãƒšãƒ¼ã‚¸\n",
    "ã‚µã‚¤ãƒ‰è¦‹å‡ºã—\n",
    "é€ä¿¡ã™ã‚‹\n",
    "ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒŠãƒ“\n",
    "ãŠå®¢æ§˜ã®å£°ãƒ»å¯¾ç­–äº‹ä¾‹\n",
    "ãƒšãƒ¼ã‚¸æ¬¡ã®ãƒšãƒ¼ã‚¸ Â»\n",
    "ã“ã¡ã‚‰ã‹ã‚‰\n",
    "å½¹ç«‹ã¤æ–‡ä¾‹é›†\n",
    "ãŠå®¢æ§˜ã®å£°\n",
    "å€‹äººæƒ…å ±ä¿è­·ãƒãƒªã‚·ãƒ¼\n",
    "ç‰¹å®šå•†å–å¼•ã«ã¤ã„ã¦\n",
    "å¿…é ˆãŠåå‰\n",
    "ç”³è¾¼\n",
    "ãƒ•ã‚©ãƒ¼ãƒ \n",
    "FAQ\n",
    "é›»è©±ã™ã‚‹\n",
    "ãƒšãƒ¼ã‚¸å…ˆé ­ã¸\n",
    "é€ä¿¡ã™ã‚‹\n",
    "å¤©æ°—äºˆå ±\n",
    "ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‚’è¡¨ç¤º\n",
    "ãƒªãƒ³ã‚¯é›†\n",
    "ä¸Šéƒ¨ã¸æˆ»ã‚‹\n",
    "ã¸ã‚¹ã‚­ãƒƒãƒ—\n",
    "Category\n",
    "ã”è³ªå•\n",
    "ãƒ¤ãƒŸé‡‘ã®é›»è©±ç•ªå·\n",
    "æœ€æ–°è¨˜äº‹\n",
    "ã‚¤ãƒ³ãƒ•ã‚©ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "ã‚³ãƒ©ãƒ \n",
    "äº‹å‹™æ‰€æ¡ˆå†…\n",
    "ã‚µãƒ¼ãƒ“ã‚¹æ–™é‡‘\n",
    "ãƒªã‚¹ãƒˆãƒŠãƒ“\n",
    "æ²ç¤ºæ¿BBS\n",
    "åœ°å›³ãƒšãƒ¼ã‚¸\n",
    "ãƒ¡ãƒ¼ãƒ«ãƒ•ã‚©ãƒ¼ãƒ \n",
    "ã‚±ãƒ¼ã‚¿ã‚¤ã‚µã‚¤ãƒˆ\n",
    "ãŠã™ã™ã‚ã®æ³¨ç›®æƒ…å ±ã‚’æ²è¼‰ã€‚\n",
    "ã“ã®è¨˜äº‹ã‚’ãƒ„ã‚¤ãƒ¼ãƒˆã™ã‚‹\n",
    "è‡ªå‹•ã§ãƒ„ã‚¤ãƒ¼ãƒˆã•ã‚Œã¾ã™ã€‚\n",
    "ã‚³ãƒ¡ãƒ³ãƒˆã‚’ã™ã‚‹ã«ã¯ã€\n",
    "ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ä¸‹ã•ã„ã€‚\n",
    "ãƒšãƒ¼ã‚¸ã®å…ˆé ­ã¸æˆ»ã‚‹ã€‚\n",
    "page\n",
    "ã®å£ã‚³ãƒŸ\n",
    "ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã«ã‚‚ã©ã‚‹ã€‚\n",
    "èª¿ã¹ã‚‹â–¼\n",
    "æ¤œç´¢çµæœ\n",
    "ã¨URLã‚’ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ\n",
    "ã®ã”åˆ©ç”¨ã«ã¤ã„ã¦\n",
    "é€šè²©ã‚µã‚¤ãƒˆ\n",
    "ç„¡æ–™\n",
    "ç„¡æ–™ãƒ–ãƒ­ã‚°ã¯ã‚³ã‚³ãƒ­ã‚°!\n",
    "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å¿˜ã‚ŒãŸæ–¹\n",
    "ç‰¹å®šå•†å–å¼•æ³•è¡¨ç¤º\n",
    "note(blog)\n",
    "niconicoã¸ã®ã”æ„è¦‹ãƒ»ã”è¦æœ›\n",
    "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯ã“ã¡ã‚‰ã‹ã‚‰ã©ã†ãã€‚\n",
    "ä¸‹è¨˜ã®ãŠå•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ ã§ã”é€£çµ¡ãã ã•ã„ã€‚\n",
    "è²·ã„ç‰©ã‹ã”ã‚«ãƒ¼ãƒˆã«å•†å“ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\n",
    "ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚¿ãƒ¼ã«ç™»éŒ²ã—ã¦æœ€æ–°æƒ…å ±ã‚„ã‚»ãƒ¼ãƒ«æƒ…å ±ã‚’ã„ã¡æ—©ãã‚²ãƒƒãƒˆã€‚\n",
    "å—ä¿¡ã™ã‚‹å—ä¿¡ã—ãªã„\n",
    "JavaScriptã‚’æœ‰åŠ¹ã«ã—ã¦ã”è¦§ãã ã•ã„ã€‚\n",
    "ã“ã®ãƒšãƒ¼ã‚¸ã®æ©Ÿèƒ½ã‚’åˆ©ç”¨ã™ã‚‹ã«ã¯JavaScriptã«å¯¾å¿œã—ãŸãƒ–ãƒ©ã‚¦ã‚¶ãŒå¿…è¦ã§ã™ã€‚\n",
    "åˆ†ã§èª­ã‚ã¾ã™ã€‚\n",
    "ç„¡æ–­è»¢è¼‰ã‚’ç¦ã˜ã¾ã™ã€‚\n",
    "æ¤œç´¢æ¡ä»¶ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\n",
    "ã‚³ãƒ¡ãƒ³ãƒˆ(0)ãƒˆãƒ©ãƒƒã‚¯ãƒãƒƒã‚¯ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚\n",
    "ã“ã®è¨˜äº‹ã¸ã®ã‚³ãƒ¡ãƒ³ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã€‚è¿”ä¿¡ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã™ã‚‹ã€‚\n",
    "æ³•ä»¤é †å®ˆã¨çŠ¯ç½ªæŠ‘æ­¢ã®ãŸã‚ã«ãƒ¡ãƒ«ã‚«ãƒªã‚ã‚“ã—ã‚“ãƒ»ã‚ã‚“ãœã‚“å®£è¨€!Facebook\n",
    "Facebook\n",
    "ã¯ã¦ãªãƒ–ãƒ­ã‚°ã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚\n",
    "è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ\n",
    "ã‚¤ãƒ³ãƒ—ãƒ©ãƒ³ãƒˆ\n",
    "æ­¯ç§‘\n",
    "VIPãŒãŠé€ã‚Šã—ã¾ã™\n",
    "è¦‹ç©\n",
    "ID:\n",
    "PR\n",
    "ä¸€è¦§\n",
    "å‡ºä¼šã„\n",
    "æ¢ã›ã¾ã™\n",
    "å•†å“ã¯ã”ã–ã„ã¾ã›ã‚“\n",
    "ã€Šå‰ã®ãƒšãƒ¼ã‚¸|æ¬¡ã®ãƒšãƒ¼ã‚¸ã€‹\n",
    "RSS\n",
    "äººæ°—è¨˜äº‹ã¾ã ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\n",
    "æŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n",
    "æŠ•ç¨¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\n",
    "åˆ¥ã®ã‚µã‚¤ãƒˆã«ã‚¸ãƒ£ãƒ³ãƒ—ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ã€‚\n",
    "ãŠå•åˆã›ã¯24æ™‚é–“å—ã‘ä»˜ã‘ã¦ãŠã‚Šã¾ã™ã€‚\n",
    "ãŠæ°—è»½ã«ã”é€£çµ¡ãã ã•ã„ã€‚\n",
    "å·¦ã®ä¸€è¦§ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’ãŠé¸ã³ãã ã•ã„ã€‚\n",
    "ã¾ã§ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚\n",
    "\n",
    "ã®äººæ°—ã‚¯ãƒã‚³ãƒŸ\n",
    "JavaScriptã‚’ONã«ã—ã¦ãã ã•ã„ã€‚\n",
    "ã‚µãƒ³ãƒ—ãƒ«ã§ã™\n",
    "ãƒªãƒ³ã‚¯ã«è¿½åŠ ã™ã‚‹\n",
    "ã‚µã‚¤ãƒ‰ãƒãƒ¼\n",
    "ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™\n",
    "ã®ã‚¯ãƒã‚³ãƒŸ\n",
    "ã‚µã‚¤ãƒˆã«ã¤ã„ã¦\n",
    "å£ã‚³ãƒŸãƒ»æ„Ÿæƒ³!\n",
    "ãƒ›ãƒ¼ãƒ >\n",
    "Next Entry\n",
    "\"\"\"\n",
    "\n",
    "broken_ending_list = broken_sentence_endings.split(\"\\n\")\n",
    "broken_ending_list = [x for x in broken_ending_list if len(x) > 0]\n",
    "\n",
    "broken_ending_list += [\n",
    "    'ç¶šãã‚’èª­ã‚€', '[ç¶šãã‚’èª­ã‚€]', '(ç¶šãã‚’èª­ã‚€)', 'ç¶šãã‚’è¦‹ã‚‹', 'ç¶šãã‚’ã¿ã‚‹', '(ç¶šã)', '(ç¶šãã‚’è¡¨ç¤º)', '(ç¶šãã‚’ã¿ã‚‹)', '[ç¶šãã‚’ã¿ã‚‹]', '[ç¶šãã‚’è¦‹ã‚‹]',\n",
    "]\n",
    "broken_ending_list += ['...(ç¶šãã‚’è¡¨ç¤º)', '[ ç¶šãã‚’è¦‹ã‚‹ ]',\n",
    "                       'ãƒ»ãƒ»ãƒ»ç¶šãã‚’è¦‹ã‚‹', '... ç¶šãã‚’èª­ã‚€',\n",
    "                       \"è©³ç´°ã¯ã“ã¡ã‚‰ Â»\",\n",
    "                       \"è©³ç´°>>>\",\n",
    "                       \"ã‚µã‚¤ãƒˆãƒãƒƒãƒ—\",\n",
    "                       ]\n",
    "\n",
    "\n",
    "noise_mid_list = \"\"\"\n",
    "Copyright(C)\n",
    "ãƒ‡ã‚¸ã‚¿ãƒ«åºƒå‘Šã‚¬ã‚¤ãƒ‰\n",
    "Copyright\n",
    "HOME>\n",
    "æ–°èåºƒå‘Šã‚¬ã‚¤ãƒ‰\n",
    "Â©\n",
    "ç„¡æ–­è»¢è¼‰ã‚’ç¦æ­¢ã—ã¾ã™\n",
    "è‘—ä½œæ¨©\n",
    "è¨˜äº‹ãƒ»å†™çœŸãƒ»å‹•ç”»ã®åˆ©ç”¨ç”³è¾¼\n",
    "æ¡ç”¨æƒ…å ±\n",
    "Copyright Â© \n",
    "Copyright \n",
    "\"\"\"\n",
    "noise_mid_list = noise_mid_list.split(\"\\n\")\n",
    "noise_mid_list = [x for x in noise_mid_list if len(x) > 0]\n",
    "\n",
    "\n",
    "random_mid_list = \"\"\"\n",
    "JavaScriptã‚’\n",
    "ãƒšãƒ¼ã‚¸ã®å…ˆé ­\n",
    "ç™»éŒ²ãƒ»è§£é™¤\n",
    "ãŠå•ã„åˆã‚ã›ç­‰\n",
    "ç„¡æ–™ãƒ¡ãƒ¼ãƒ«ãƒã‚¬ã‚¸ãƒ³\n",
    "é…ä¿¡ç™»éŒ²\n",
    "æ€§ç™–\n",
    "Blog\n",
    "ãƒ–ãƒ­ã‚°\n",
    "ã“ã“ã«èª¬æ˜æ–‡ãŒå…¥ã‚Šã¾ã™\n",
    "è³‡æ–™è«‹æ±‚\n",
    "ãƒ¡ãƒ¼ãƒ«ã§ã™ã™ã‚ã‚‹\n",
    "å•†å“ã®çŠ¶æ…‹\n",
    "æœªä½¿ç”¨\n",
    "ã‚¨ãƒªã‚¢ãƒ»é§…ãŒã‚ã‚Šã¾ã›ã‚“\n",
    "è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n",
    "ç„¡æ–™ç™»éŒ²\n",
    "äººæ°—è¨˜äº‹\n",
    "Top\n",
    "æ–°è¦ç™»éŒ²\n",
    "æ¤œç´¢çµæœ\n",
    "å½“é™¢\n",
    "ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸\n",
    "ä»£è¡¨æŒ¨æ‹¶\n",
    "ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²\n",
    "å€Ÿé‡‘\n",
    "ä¸å‹•ç”£\n",
    "æŠ•è³‡\n",
    "ã‚«ãƒ¼ãƒ‰ãƒ­ãƒ¼ãƒ³\n",
    "ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸\n",
    "Podcast\n",
    "æ¤œç´¢\n",
    "è³ƒè²¸\n",
    "ç‰©ä»¶\n",
    "Views\n",
    "Clicks\n",
    ".com\n",
    "URL\n",
    "æ›´æ–°æ—¥\n",
    "é…ä¿¡\n",
    "ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n",
    "å¾’æ­©\n",
    "Javascriptã®\n",
    "è¨˜äº‹ä¸€è¦§\n",
    "ãƒ–ãƒ­ã‚°\n",
    "å…¬é–‹ä¸­\n",
    "ã‚«ãƒ¼ãƒˆ\n",
    "ã“ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„\n",
    "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã§ä¿è­·\n",
    "ã‚µã‚¤ãƒˆ\n",
    "å…¬é–‹ç‰©ä»¶\n",
    "ãƒ›ãƒ¼ãƒ \n",
    "äºˆç´„\n",
    "å•†å“ã®\n",
    "ãŠæ¢ã—ã®\n",
    "ã‚µãƒãƒ¼ãƒˆã—ã¾ã™\n",
    "ã‚’æ²è¼‰!\n",
    "ã¯ã“ã¡ã‚‰\n",
    "ãƒ¬ã‚¹æ•°ãŒ1000ã‚’è¶…ãˆã¦ã„ã¾ã™\n",
    "ç¾åœ¨è¡¨ç¤ºã—ã¦ã„ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰\n",
    "ã“ã®ã‚¹ãƒ¬ã¯\n",
    "æ–°ç€ãƒ¬ã‚¹ã®è¡¨ç¤º\n",
    "æ²ç¤ºæ¿ã«æˆ»ã‚‹\n",
    "å‰100\n",
    "æ¬¡100\n",
    "æœ€æ–°50\n",
    "Twitter\n",
    "Facebook\n",
    "ã¯ã¦ãƒ–\n",
    "Pocket\n",
    "LINE\n",
    "ã‚²ã‚¹ãƒˆã•ã‚“\n",
    "ã‚³ãƒ¡ãƒ³ãƒˆ(\n",
    "ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆ\n",
    "ãŒæä¾›\n",
    "é™å®šå…¬é–‹è¨˜äº‹\n",
    "æœ¬æ–‡ã¸\n",
    "å†åº¦ã‚¢ã‚¯ã‚»ã‚¹\n",
    "ãƒ¡ãƒ¼ãƒ«ã§ã®\n",
    "è²·ã„ç‰©ã‚’ç¶šã‘ã‚‹\n",
    "æŸ»å®š\n",
    "å€‹äººæƒ…å ±ä¿è­·\n",
    "ãŠè²·ã„ä¸Šã’\n",
    "å°åˆ·ã™ã‚‹\n",
    "ä¼šå“¡é™å®š\n",
    "ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·\n",
    "ç„¡æ–™\n",
    "ã”æ¡ˆå†…\n",
    "mail\n",
    "ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ\n",
    "@\n",
    "ã“ã®ã‚µã‚¤ãƒˆ\n",
    "æ±‚äºº\n",
    "ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³\n",
    "SHARE\n",
    "æ—¥è¨˜\n",
    "é–²è¦§\n",
    "trackback\n",
    "(-)\n",
    "|\n",
    "ã‚¹ãƒãƒ³ã‚µãƒ¼åºƒå‘Š\n",
    "--------\n",
    "page\n",
    "top\n",
    "Home\n",
    "Â»\n",
    "Loading.\n",
    "URL\n",
    "[ç·¨é›†]\n",
    "â–²\n",
    "ã€Š\n",
    ".jp\n",
    "Escã‚­ãƒ¼\n",
    "æ¥½å¤©\n",
    "å–¶æ¥­\n",
    "Yahoo!\n",
    "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹\n",
    "Share\n",
    "ãƒ•ã‚©ãƒ­ãƒ¼\n",
    "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼\n",
    "Youtube\n",
    "VIEWS\n",
    "è³¼èª­ã™ã‚‹\n",
    "æœ€æ–°æƒ…å ±\n",
    "å…¬é–‹æ—¥\n",
    "ã¡ã‚ƒã‚“ã­ã‚‹\n",
    "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„\n",
    "è³¼å…¥æ‰‹ç¶š\n",
    "Twitter\n",
    "@5ch\n",
    "åºƒå‘Šãªã—\n",
    "å‘¨è¾ºæ–½è¨­æƒ…å ±\n",
    "è³ƒè²¸ãƒãƒ³ã‚·ãƒ§ãƒ³\n",
    "Cookie\n",
    "QRã‚³ãƒ¼ãƒ‰\n",
    "å‡ºå¼µ\n",
    "æ‰¿ã‚Šã¾ã™\n",
    "ãŠä»»ã›ãã ã•ã„\n",
    "ã‚¿ã‚°\n",
    "ãƒ­ã‚°ã‚¤ãƒ³\n",
    "ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™»éŒ²\n",
    "ãƒˆãƒƒãƒ—ã¸\n",
    "é£Ÿã¹ãƒ­ã‚°\n",
    "ã”ç›¸è«‡ãã ã•ã„\n",
    "å…¨å›½å¯¾å¿œ\n",
    "è¨˜äº‹\n",
    "ãŠä¼šè¨ˆ\n",
    "ãƒªãƒ³ã‚¯\n",
    "ãƒ„ãƒ¼ãƒ«\n",
    "å¼•ç”¨\n",
    "ã‚¹ãƒˆãƒƒã‚¯\n",
    "æ²è¼‰\n",
    "1 2 3 4 5\n",
    "ã‚¯ãƒªãƒƒã‚¯\n",
    "ã”æ³¨æ–‡\n",
    "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
    "ãŠé€ã‚Šãã ã•ã„\n",
    "è²·ã„ç‰©\n",
    "ã‚«ãƒ¼ãƒˆ\n",
    "RSS\n",
    "å…¨ãƒªã‚¹ãƒˆ\n",
    "æ–°ç€\n",
    "ã„ã„ã­!\n",
    "èª­ã‚€\n",
    "ã¨ã¯?\n",
    "CONTACT\n",
    "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ\n",
    "ãƒ¢ãƒã‚¤ãƒ«\n",
    "ãŠæ°—ã«å…¥ã‚Š\n",
    "ã‚‚ãã‚‚ã\n",
    "åˆ©ç”¨è€…æƒ…å ±\n",
    "é‹å–¶è€…ã«ã¤ã„ã¦\n",
    "ã‚¯ãƒªãƒƒã‚¯ã§æ‹¡å¤§\n",
    "ã‚¢ã‚¤ã‚³ãƒ³ä¸Š\n",
    "ãƒã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«\n",
    "è©³ç´°æƒ…å ±\n",
    "TikTok\n",
    "ç‰¹å®šå•†å–å¼•æ³•\n",
    "ã«åŸºã¥ãè¡¨ç¤º\n",
    "ãƒ—ãƒ¬ã‚¤æ—¥è¨˜\n",
    "Q&A\n",
    "ã‚ˆãã‚ã‚‹ã”è³ªå•ã¨å›ç­”\n",
    "ãƒˆãƒ”ãƒƒã‚¯ã‚¹\n",
    "èƒŒæ™¯è‰²\n",
    "ãƒ¡ãƒ«ãƒã‚¬\n",
    "(c)\n",
    "javascript\n",
    "dobe Reader\n",
    "ã‚«ãƒ†ã‚´ãƒª\n",
    "ãƒ¦ãƒ¼ã‚¶ãƒ¼èªè¨¼\n",
    "æ—©é€Ÿè¦‹ã‚‹\n",
    "æŠ•ç¨¿\n",
    "ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "å°åˆ·\n",
    "SNSã§ã‚·ã‚§ã‚¢ã—ã‚ˆã†!\n",
    "English\n",
    "ãŠå•ã„åˆã‚ã›\n",
    "ç™ºé€\n",
    "æ¥­å‹™\n",
    "ä¼‘æ—¥\n",
    "çŠ¶æ…‹\n",
    "ã‚¹ãƒšãƒ¼ã‚¹ã‚­ãƒ¼\n",
    "çŸ¢å°ã‚­ãƒ¼\n",
    "é¸æŠã—ã¾ã™\n",
    "ãŠå®¢æ§˜å„ä½\n",
    "ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³\n",
    "ãŠæ°—ã«å…¥ã‚Š\n",
    "æ¢ã—æ–¹ã‚’å¤‰æ›´ã™ã‚‹\n",
    "æ²ç¤ºæ¿\n",
    "è²·ã„ç‰©ã‚’ç¶šã‘ã‚‹\n",
    "é¸ã‚“ã§ãã ã•ã„\n",
    "é…é”æ—¥\n",
    "é¸æŠã—ã¦ãã ã•ã„\n",
    "ãŠé›»è©±\n",
    "ã‚³ãƒ¡ãƒ³ãƒˆ\n",
    "éå…¬é–‹\n",
    "åˆ©ç”¨ç‰¹å®šå•†å–å¼•æ³•\n",
    "åŸºã¥ãè¡¨ç¤ºè¦ç´„\n",
    "ãŠå½¹ç«‹ã¡\n",
    "å€‹äººæƒ…å ±ã®å–æ‰±\n",
    "åŒæ„\n",
    "Contact\n",
    "URL\n",
    "http\n",
    "shares\n",
    "ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚¿ãƒ¼\n",
    "çµæœã‚’è¦‹ã‚‹\n",
    "å…è²¬\n",
    "PICK UP!\n",
    "æœ€è¿‘ã®è¨˜äº‹\n",
    "New!\n",
    "Tweets\n",
    "ï¼ï¼t?\n",
    "ã‚’ãƒ•ã‚©ãƒ­ãƒ¼\n",
    "ãŠæ”¯æ‰•ã„\n",
    "ãŠé¸ã³ã„ãŸã ã‘ã¾ã™ã€‚\n",
    "ãƒ„ã‚¤ãƒ¼ãƒˆã™ã‚‹\n",
    "ã™ã¹ã¦è¡¨ç¤º\n",
    "ãŠå•åˆã‚ã›\n",
    "ãŠå®¢æ§˜\n",
    "å…¥ä¼šæ¡ˆå†…\n",
    "äº‹æ¥­å…¨èˆ¬\n",
    "ãƒ¡ãƒ¼ãƒ«ãƒ•ã‚©ãƒ¼ãƒ \n",
    "ã‚«ãƒ†ã‚´ãƒªãƒ¼\n",
    "ãƒã‚§ãƒƒã‚¯\n",
    "å•†å“\n",
    "ãŠä½¿ã„ã®ãƒ–ãƒ©ã‚¦ã‚¶\n",
    "JavaScript\n",
    "åº—èˆ—\n",
    "ç™»éŒ²\n",
    "ä¸€è¦§\n",
    "æˆ»ã‚‹\n",
    "ã‚µãƒ¼ãƒ“ã‚¹\n",
    "ç´¹ä»‹\n",
    "ãƒšãƒ¼ã‚¸\n",
    "ç®¡ç†\n",
    "é€æ–™\n",
    "ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°\n",
    "ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰\n",
    "ã”åˆ©ç”¨\n",
    "ã‚¬ã‚¤ãƒ‰\n",
    "ãŠä¼ãˆã—ã¾ã™\n",
    "ã”ç´¹ä»‹\n",
    "ãŠéƒ¨å±‹æ¢ã—\n",
    "é™å®šå…¬é–‹è¨˜äº‹\n",
    "å†åº¦ãŠè©¦ã—\n",
    "åˆ©ç”¨è¦ç´„\n",
    "ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼\n",
    "åŒæ„ã—ã¾ã™\n",
    "æ•´éª¨é™¢\n",
    "ã‚¯ãƒªãƒ‹ãƒƒã‚¯\n",
    "æ–¹é‡\n",
    "Instagram\n",
    "ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³\n",
    "ãƒãƒªã‚·ãƒ¼\n",
    "contact\n",
    "profile\n",
    "void\n",
    "ä¼šç¤¾æ¡ˆå†…\n",
    "è©³ã—ãè¦‹ã‚‹\n",
    "ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢\n",
    "1ä»¶\n",
    "ç—…é™¢ã¸ã®åœ°å›³\n",
    "ç„¡æ–™ç›¸è«‡\n",
    "æ–‡å­—ã‚’å…¥åŠ›\n",
    "éå»ã®è¨˜äº‹\n",
    "ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’é–‰ã˜ã‚‹\n",
    "â–¶\n",
    "Social\n",
    "ã‚µã‚¤ãƒˆ\n",
    "ç‰©ä»¶\n",
    "ãƒ¡ãƒ«ã‚«ãƒª\n",
    "ãƒã‚¤ãƒ³ãƒˆ\n",
    "è²·å–\n",
    "ãƒ„ã‚¤ãƒ¼ãƒˆ\n",
    "\"\"\"\n",
    "random_mid_list = random_mid_list.split(\"\\n\")\n",
    "random_mid_list = [x for x in random_mid_list if len(x) > 0]\n",
    "random_mid_list = list(set(random_mid_list))\n",
    "\n",
    "\n",
    "def noise_line(sent: str):\n",
    "    for broken_ending in broken_ending_list:\n",
    "        # print(\"aa\", broken_ending)\n",
    "        if sent.endswith(broken_ending):\n",
    "            return None\n",
    "        if len(sent) < 2:\n",
    "            return None\n",
    "\n",
    "    for noise_mid in noise_mid_list:\n",
    "        if noise_mid in sent:\n",
    "            return None\n",
    "\n",
    "    # ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã«ç‰¹æœ‰ã®å˜èªã‚’å«ã‚€æ–‡ç« ã‚’ç¢ºç‡çš„ã«è½ã¨ã™\n",
    "    for random_mid in random_mid_list:\n",
    "        if random_mid in sent:\n",
    "            if random.random() < 0.9:\n",
    "                return None\n",
    "\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parts filter(in Line wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "from collections import Counter\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "# ãƒ†ã‚­ã‚¹ãƒˆ\n",
    "tagger = MeCab.Tagger()\n",
    "\n",
    "def parts_count(text, return_word_count=False):\n",
    "    # ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æ\n",
    "    parsed = tagger.parse(text)\n",
    "\n",
    "    # å“è©ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ãŸã‚ã®Counterã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "    pos_counter = Counter()\n",
    "    word_counter = Counter()\n",
    "\n",
    "    # è§£æçµæœã‚’è¡Œã”ã¨ã«å‡¦ç†\n",
    "    all_counts = 0\n",
    "    for line in parsed.split('\\n'):\n",
    "        # EOSã¾ãŸã¯ç©ºè¡Œã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "        if line == 'EOS' or line == '':\n",
    "            continue\n",
    "        # ã‚¿ãƒ–ã§åˆ†å‰²ã—ã€å½¢æ…‹ç´ æƒ…å ±ã‚’å–å¾—\n",
    "        pos_info = line.split('\\t')\n",
    "        # print(pos_info)\n",
    "        pos = pos_info[1]\n",
    "        pos = pos.split(\",\")[0]\n",
    "\n",
    "        if return_word_count:\n",
    "            word = pos_info[0]\n",
    "            word_counter[(word, pos)] += 1\n",
    "\n",
    "        # å“è©ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "        pos_counter[pos] += 1\n",
    "        all_counts += 1\n",
    "\n",
    "    if return_word_count:\n",
    "        return pos_counter, all_counts, word_counter\n",
    "    else:\n",
    "        return pos_counter, all_counts\n",
    "\n",
    "\n",
    "def mecab_filter(text, threshold=0.9, min_length=10):\n",
    "    \"\"\"\n",
    "    åè©ã®ç¾…åˆ—ã®æ–‡ç« ã¯ç„¡åŠ¹ã¨åˆ¤å®šã™ã‚‹\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    if text == \"\":\n",
    "        return None\n",
    "    pos_counter, all_counts = parts_count(text)\n",
    "    # print(pos_counter, all_counts)\n",
    "    # print(pos_counter)\n",
    "    meishi_and_symbol_counts = pos_counter['åè©'] + \\\n",
    "        pos_counter['è¨˜å·']+pos_counter['è£œåŠ©è¨˜å·']+pos_counter['æ¥é ­è©']\n",
    "\n",
    "    if all_counts == 0:\n",
    "        return None\n",
    "    ratio = meishi_and_symbol_counts/all_counts\n",
    "    # print(ratio, pos_counter)\n",
    "    if ratio > threshold and len(text) > min_length:\n",
    "        return None\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "def n_gram(words, n):\n",
    "    return [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "# +(n-gramã«ã‚ˆã£ã¦é‡è¤‡ã®æœ‰ç„¡ã‚’èª¿ã¹ã‚‹)\n",
    "\n",
    "\n",
    "def mecab_filter2(text, threshold=0.9, min_length=10,  ngram_threshold_2gram=0.20, ngram_threshold_3gram=0.20, ngram_threshold_4gram=0.20):\n",
    "    if text is None:\n",
    "        return None\n",
    "\n",
    "    pos_counter, words, word_counter = parts_count(\n",
    "        text, return_word_count=True)\n",
    "    meishi_and_symbol_counts = pos_counter['åè©'] + \\\n",
    "        pos_counter['è¨˜å·'] + pos_counter['è£œåŠ©è¨˜å·']\n",
    "    ratio = meishi_and_symbol_counts / len(words)\n",
    "\n",
    "    if ratio > threshold and len(text) > min_length:\n",
    "        return None\n",
    "\n",
    "    # 2-gramã®å‡¦ç†\n",
    "    ngram_counts_2gram = Counter(n_gram(words, 2))\n",
    "    total_2grams = sum(ngram_counts_2gram.values())\n",
    "    most_common_2gram_count = ngram_counts_2gram.most_common(\n",
    "        1)[0][1] if ngram_counts_2gram else 0\n",
    "    if total_2grams > 0 and most_common_2gram_count / total_2grams > ngram_threshold_2gram:\n",
    "        return None\n",
    "\n",
    "    # 3-gramã®å‡¦ç†\n",
    "    ngram_counts_3gram = Counter(n_gram(words, 3))\n",
    "    total_3grams = sum(ngram_counts_3gram.values())\n",
    "    repeated_3grams = sum(\n",
    "        count for count in ngram_counts_3gram.values() if count > 1)\n",
    "    if total_3grams > 0 and repeated_3grams / total_3grams > ngram_threshold_3gram:\n",
    "        return None\n",
    "\n",
    "    # 4-gramã®å‡¦ç†\n",
    "    ngram_counts_4gram = Counter(n_gram(words, 4))\n",
    "    total_4grams = sum(ngram_counts_4gram.values())\n",
    "    repeated_4grams = sum(\n",
    "        count for count in ngram_counts_4gram.values() if count > 1)\n",
    "    if total_4grams > 0 and repeated_4grams / total_4grams > ngram_threshold_4gram:\n",
    "        return None\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is_remove_num_lines(è¿½åŠ å‡¦ç†)ï¼šæ•°å­—ã ã‘ã®è¡¨ç¾ã™ã¦ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 91011.16it/s]\n"
     ]
    }
   ],
   "source": [
    "##è¿½åŠ å‡¦ç†\n",
    "# ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«å…¥ã‚Œå¿˜ã‚ŒãŸã€æ—¥ä»˜ç³»ã‚’æ¶ˆã™å‡¦ç†\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "# å†’é ­ã®æ•°å­—ã ã‘ã®è¡Œã‚’å‰Šé™¤\n",
    "\n",
    "def is_remove_num_lines(line):\n",
    "    if len(line) == 0:\n",
    "        return False\n",
    "    check_line = line[:20]\n",
    "    count = sum(c.isdigit() for c in check_line)\n",
    "    # num_ratio=count/len(check_line)\n",
    "    num_ratio = count\n",
    "    # print(num_ratio)\n",
    "    ratio = 5\n",
    "    if num_ratio > ratio and check_line.find(\":\") > 0:\n",
    "        return False\n",
    "    if num_ratio > ratio and check_line.find(\"æ—¥\") > 0:\n",
    "        return False\n",
    "    if num_ratio > ratio and check_line.find(\"å¹´\") > 0:\n",
    "        return False\n",
    "    if num_ratio > ratio and check_line.find(\"-\") > 0:\n",
    "        return False\n",
    "    if num_ratio > ratio and check_line.find(\"/\") > 0:\n",
    "        return False\n",
    "    if num_ratio > ratio and check_line.find(\"ï¼\") > 0:\n",
    "        return False\n",
    "    if num_ratio > ratio and check_line.find(\"æœˆ\") > 0:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove_num_line(in line-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##è¿½åŠ å‡¦ç†\n",
    "# ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«å…¥ã‚Œå¿˜ã‚ŒãŸã€æ—¥ä»˜ç³»ã‚’æ¶ˆã™å‡¦ç†\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "# å†’é ­ã®æ•°å­—ã ã‘ã®è¡Œã‚’å‰Šé™¤\n",
    "\n",
    "def is_remove_num_lines(line):\n",
    "    if len(line) == 0:\n",
    "        return \"\"\n",
    "    check_line = line[:20]\n",
    "    count = sum(c.isdigit() for c in check_line)\n",
    "    # num_ratio=count/len(check_line)\n",
    "    num_ratio = count\n",
    "    # print(num_ratio)\n",
    "    ratio = 5\n",
    "    if num_ratio > ratio and check_line.find(\":\") > 0:\n",
    "        return \"\"\n",
    "    if num_ratio > ratio and check_line.find(\"æ—¥\") > 0:\n",
    "        return \"\"\n",
    "    if num_ratio > ratio and check_line.find(\"å¹´\") > 0:\n",
    "        return \"\"\n",
    "    if num_ratio > ratio and check_line.find(\"-\") > 0:\n",
    "        return \"\"\n",
    "    if num_ratio > ratio and check_line.find(\"/\") > 0:\n",
    "        return \"\"\n",
    "    if num_ratio > ratio and check_line.find(\"ï¼\") > 0:\n",
    "        return \"\"\n",
    "    if num_ratio > ratio and check_line.find(\"æœˆ\") > 0:\n",
    "        return \"\"\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ã¾ã¨ã‚ line_wise_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lineã”ã¨ã®filter, clean\n",
    "def process_line(line):\n",
    "    \"\"\"è¡Œã®æ¸…æƒã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã†\"\"\"\n",
    "    new_line = noise_line(line)\n",
    "    new_line = mecab_filter2(new_line)  # n-gramã«ã‚ˆã‚‹é‡è¤‡ã®é™¤å¤–\n",
    "    new_line=  is_remove_num_lines(new_line)\n",
    "    return new_line\n",
    "\n",
    "def process_paragraph(paragraph):\n",
    "    \"\"\"ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•å†…ã®å„è¡Œã‚’å‡¦ç†ã—ã€æ¸…æƒã•ã‚ŒãŸãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã‚’è¿”ã™\"\"\"\n",
    "    new_lines = []\n",
    "    old_line = \"\"\n",
    "    for line in paragraph:\n",
    "        new_line = process_line(line)\n",
    "        if new_line and new_line != old_line:\n",
    "            new_lines.append(new_line)\n",
    "            old_line = new_line\n",
    "    return new_lines\n",
    "\n",
    "def clean_paragraphs(paragraphs):\n",
    "    \"\"\"ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã®ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€é‡è¤‡ã‚„ä¸é©åˆ‡ãªè¡Œã‚’é™¤å¤–ã—ã¦å†æ§‹æˆã™ã‚‹\"\"\"\n",
    "    new_paragraphs = [process_paragraph(paragraph) for paragraph in paragraphs]\n",
    "    new_paragraphs = [\"\".join(paragraph) for paragraph in new_paragraphs if paragraph]\n",
    "    return new_paragraphs\n",
    "\n",
    "def text_to_cleaned_paragraphs(text):\n",
    "    \"\"\"ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ã«ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã«åˆ†å‰²ã—ã¦æ¸…æƒã™ã‚‹\"\"\"\n",
    "    paragraphs = text_to_paragraph_sentences(text)  # ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã¨æ–‡ç« ã«åˆ†å‰²\n",
    "    cleaned_paragraphs = clean_paragraphs(paragraphs)  # ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã‚’æ¸…æƒ\n",
    "    return cleaned_paragraphs\n",
    "\n",
    "##line_wise---------------------------------------------------------------------\n",
    "def line_wise(text):\n",
    "    paragraphs= text_to_paragraph_sentences(text)\n",
    "    #print(paragraphs)\n",
    "    if len(paragraphs) !=0:\n",
    "        text = \"\\n\".join(paragraphs)\n",
    "    return text\n",
    "\n",
    "\n",
    "def line_wise(text):\n",
    "    # ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã¨æ–‡ç« ã«åˆ†å‰²\n",
    "    paragraphs = text_to_paragraph_sentences(text)\n",
    "\n",
    "    new_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        new_lines = []\n",
    "        old_line = \"\"\n",
    "        for line in (paragraph):\n",
    "            # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®è¡Œãƒã‚§ãƒƒã‚¯\n",
    "            new_line = rule_base_text_check_clean(line)\n",
    "\n",
    "            # åè©ã ã‚‰ã‘ã®lineã‚’é™¤å¤–\n",
    "            try:\n",
    "                # new_line = parts_filter.filter(new_line)\n",
    "                # new_line = parts_filter.filter2(new_line)  # n-gramã«ã‚ˆã‚‹é‡è¤‡ã®é™¤å¤–\n",
    "                new_line = noise_line(line)\n",
    "                new_line = mecab_filter2(new_line)  # n-gramã«ã‚ˆã‚‹é‡è¤‡ã®é™¤å¤–\n",
    "                new_line=  is_remove_num_lines(new_line)\n",
    "            except:\n",
    "                pass\n",
    "            if new_line:\n",
    "                if new_line == old_line:\n",
    "                    continue\n",
    "                old_line = new_line\n",
    "                # ppl=perp_checker(new_line)\n",
    "                # print(ppl,new_line)\n",
    "                new_lines.append(new_line)\n",
    "\n",
    "        if new_lines:\n",
    "            new_paragraphs.append(new_lines)\n",
    "\n",
    "    # æ–‡æœ«ãŒï½¡ãªã©ã§ãŠã‚ã‚‰ãªã„ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ä¸­ã®æ–‡ç« ã‚’å‰Šé™¤\n",
    "    # clean_line_endings(new_paragraphs)\n",
    "\n",
    "    # ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã«ã¾ã¨ã‚ã‚‹\n",
    "    cleaned_paragraphs = []\n",
    "    old_lines = \"\"\n",
    "    for paragraph in new_paragraphs:\n",
    "        lines = \"\".join(paragraph)\n",
    "        if lines == old_lines:\n",
    "            continue\n",
    "        cleaned_paragraphs.append(lines)\n",
    "        old_lines = lines\n",
    "    text = \"\\n\".join(cleaned_paragraphs)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é–¢æ•°ï¼šrule baseã€€text check_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .parts_filter import filter\n",
    "\n",
    "noise_ending_list = \"\"\"\n",
    "ä¸€è¦§ãƒšãƒ¼ã‚¸ä¸Šéƒ¨ã¸\n",
    "ãƒšãƒ¼ã‚¸ä¸Šéƒ¨ã¸\n",
    "ä¸Šéƒ¨ã¸\n",
    "ä¸Šã¸æˆ»ã‚‹\n",
    "\"\"\"\n",
    "noise_ending_list = noise_ending_list.split(\"\\n\")\n",
    "noise_ending_list = [x for x in noise_ending_list if len(x) > 0]\n",
    "\n",
    "\n",
    "def clean_endings(sent: str):\n",
    "    for noise_ending in noise_ending_list:\n",
    "        if sent.endswith(noise_ending):\n",
    "            return sent[:-len(noise_ending)]\n",
    "    return sent\n",
    "\n",
    "\n",
    "noise_header_list = \"\"\"\n",
    "ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸>\n",
    "\"\"\"\n",
    "noise_header_list = noise_header_list.split(\"\\n\")\n",
    "noise_header_list = [x for x in noise_header_list if len(x) > 0]\n",
    "\n",
    "\n",
    "def clean_headers(text):\n",
    "    for noise_header in noise_header_list:\n",
    "        if text.startswith(noise_header):\n",
    "            return text[len(noise_header):]\n",
    "    return text\n",
    "\n",
    "\n",
    "sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', \"ï¼\", \"ã€\", \"ã€‚\"]\n",
    "\n",
    "# æ–‡é ­ã®è¦‹å‡ºã—ã‚’æ¶ˆã™\n",
    "\n",
    "\n",
    "def remove_header(txt, header_list, n_check=30):\n",
    "    for header in header_list:\n",
    "        if header in txt[:n_check]:\n",
    "            for delimiter in header_list:\n",
    "                txt = txt.split(delimiter)[1:]\n",
    "                txt = delimiter.join(txt)\n",
    "                break\n",
    "    return txt\n",
    "\n",
    "\n",
    "def rule_base_text_check_clean(text):\n",
    "    text = remove_header(text, header_list=[\"|\", \"ã€‘\", \">\", \"]\",])\n",
    "    text = clean_endings(text)\n",
    "    text = clean_headers(text)\n",
    "    text = dedup_lines(text)\n",
    "\n",
    "    # æ–‡ç« å…¨ä½“ã§åè©ãŒå¤šã„å ´åˆã¯ç„¡åŠ¹ã¨åˆ¤å®š\n",
    "    try:\n",
    "        if not mecab_filter1(text, threshold=0.7):\n",
    "            return \"\"\n",
    "    except:\n",
    "        return \"\"\n",
    "    for ending in sentence_endings:\n",
    "        if text.find(ending) > 0:\n",
    "            return text\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def dedup_lines(data, check_length=10):\n",
    "\n",
    "    lines = data.split('\\n')\n",
    "    new_lines = []\n",
    "    old_line = \"\"\n",
    "    for line in lines:\n",
    "        set_a = set(line[:check_length])\n",
    "        set_b = set(old_line[:check_length])\n",
    "        if len(list(set_a-set_b)) < 2:\n",
    "            continue\n",
    "        old_line = line\n",
    "        new_lines.append(line)\n",
    "\n",
    "    result_text = '\\n'.join(new_lines)\n",
    "    return result_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é–¢æ•°ï¼šremove_multi_headers(in line dedup)\n",
    "ä½¿ã£ã¦ãªã„å‡¦ç†ãŒã‚ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_symbols = [\n",
    "    \"(\",\n",
    "    \"[\",\n",
    "    \"{\",\n",
    "    \"ï¼ˆ\",\n",
    "    \"ï¼»\",\n",
    "    \"ï½›\",\n",
    "    \"ã€ˆ\",\n",
    "    \"ã€Š\",\n",
    "    \"ã€Œ\",\n",
    "    \"ã€\",\n",
    "    \"ã€\",\n",
    "    \"ã€”\",\n",
    "    \"ãƒ»\",\n",
    "    \"ã€\",\n",
    "    \",\",\n",
    "    \"ï¼Œ\",\n",
    "]\n",
    "\n",
    "end_symbols = [\n",
    "    \")\",\n",
    "    \"]\",\n",
    "    \"}\",\n",
    "    \"ï¼‰\",\n",
    "    \"ï¼½\",\n",
    "    \"ï½\",\n",
    "    \"ã€‰\",\n",
    "    \"ã€‹\",\n",
    "    \"ã€\",\n",
    "    \"ã€\",\n",
    "    \"ã€‘\",\n",
    "    \"ã€•\",\n",
    "]\n",
    "\n",
    "\n",
    "def is_end_with_begin_symbol(sent):\n",
    "    for s in begin_symbols:\n",
    "        if sent.endswith(s):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_start_with_end_symbol(sent):\n",
    "    for s in end_symbols:\n",
    "        if sent.startswith(s):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_sentence_end(sent):\n",
    "    end_symbols = [\".\", \"ã€‚\", \"!\", \"ï¼\", \"?\", \"ï¼Ÿ\", \"ã¾ã—ãŸ\", \"ã¾ã™\",\n",
    "                   \"ã‚Œã‚‹\", \"ã™ã‚‹\", \"ã™ã‚ˆ\", \"ã§ã™\", \"ã€\", \"ã€\", \"ã•ã„\",]\n",
    "    for s in end_symbols:\n",
    "        if sent.endswith(s):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def remove_multi_headers(new_texts):\n",
    "    \"\"\"\n",
    "    è¦‹å‡ºã—1\n",
    "    è¦‹å‡ºã—2\n",
    "    è¦‹å‡ºã—3\n",
    "    ã“ã‚“ã«ã¡ã¯ï½¡\n",
    "\n",
    "    â†’\n",
    "    è¦‹å‡ºã—3\n",
    "    ã“ã‚“ã«ã¡ã¯ï½¡\n",
    "\n",
    "    ã«ã™ã‚‹\n",
    "    \"\"\"\n",
    "    new_texts = new_texts.split(\"\\n\")\n",
    "    new_texts = remove_dup_lines(new_texts)\n",
    "    cleaned_texts = []\n",
    "    cap_line = \"\"\n",
    "    for line in new_texts[::-1]:\n",
    "        if not is_sentence_end(line):\n",
    "            # print(\"line\",line)\n",
    "            if cap_line == \"\":\n",
    "                cap_line = line\n",
    "                # print(\"cap_line\",cap_line)\n",
    "\n",
    "        else:\n",
    "            if cap_line != \"\":\n",
    "                cleaned_texts.append(cap_line)\n",
    "                cap_line = \"\"\n",
    "\n",
    "            cleaned_texts.append(line)\n",
    "    cleaned_texts.append(cap_line)\n",
    "    cleaned_texts = cleaned_texts[::-1]\n",
    "    cleaned_texts = [i.strip() for i in cleaned_texts]\n",
    "    cleaned_texts = \"\\n\".join(cleaned_texts)\n",
    "    return cleaned_texts\n",
    "\n",
    "\n",
    "def remove_dup_lines(lines, dup_n_threshold=100):\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        # line=line[0].strip()\n",
    "\n",
    "        # è¡Œã®é‡è¤‡ã‚’é¿ã‘ã‚‹\n",
    "        if line in new_lines[-dup_n_threshold:]:\n",
    "            continue\n",
    "        new_lines.append(line)\n",
    "\n",
    "    return new_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## line end ä½¿ã£ã¦ãªã„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', \"ï¼\", \"ã€\"]\n",
    "\n",
    "\n",
    "def clean_line_endings(paragraphs):\n",
    "    \"\"\"æ–‡æœ«è¨˜å·ä»¥å¤–ã®æ–‡å­—ã‚’å‰Šé™¤ã™ã‚‹\"\"\"\n",
    "    for paragraph in paragraphs:\n",
    "        if len(paragraph) < 2:\n",
    "            continue\n",
    "        for line in paragraph:\n",
    "            if line[-1] not in sentence_endings:\n",
    "                paragraph.remove(line)\n",
    "                # print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å…¨ã¦æ•´ç†\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/data/fairy_hdd4/takumi/matsuo_llm/Dataset_for_BTM/01web_codes/src/cleaner\")\n",
    "# from .splitter import text_to_paragraph_sentences\n",
    "# from .text_normalizer import normalize\n",
    "###from . import text_checker\n",
    "# from . import rule_based_line_checker\n",
    "# from . import parts_filter\n",
    "###from .line_end_cleaner import clean_line_endings\n",
    "# # from .hojichar_filter import hoji_filter, prob_hoji_filter, prob_filter\n",
    "# from . import rule_based_text_checker\n",
    "# from .line_dedup import remove_multi_headers\n",
    "# # from . import repeated_phrase\n",
    "\n",
    "# ã‚¯ãƒ©ã‚·ãƒ•ã‚¡ã‚¤ã‚¢ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨åˆæœŸåŒ–\n",
    "# try:\n",
    "#     from .TextClassifier import TextClassifier\n",
    "#     classifier = TextClassifier()\n",
    "# except:\n",
    "#     print(\"error loading TextClassifier. install fasttext to use it\")\n",
    "\n",
    "def clean_text(text, hoji=True):\n",
    "    \"\"\"ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã®æ¸…æƒã‚’è¡Œã„ã€æ•´å½¢ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™\"\"\"\n",
    "    if hoji:\n",
    "        text = hoji_filter(text)\n",
    "        text = prob_hoji_filter(text)\n",
    "    # paragraphs = text_to_cleaned_paragraphs(text)\n",
    "    # text = \"\\n\".join(paragraphs)\n",
    "    #text=normalize(text)\n",
    "    text=repetition_filter1(text)\n",
    "    text=repetition_filter2(text)\n",
    "    text=line_wise(text)\n",
    "    text = remove_multi_headers(text)\n",
    "    text = rule_base_text_check_clean(text)\n",
    "    text = text.strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# å˜ä¸€æ–‡å­—ã®ç¹°ã‚Šè¿”ã—200----------------------------------------\n",
    "\n",
    "def repeated_id(text, threshold_ratio=0.3):\n",
    "    # # æ–‡å­—ã®ç¹°ã‚Šè¿”ã—ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹è¾æ›¸\n",
    "    char_count = {}\n",
    "    # å„æ–‡å­—ã®ç¹°ã‚Šè¿”ã—å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    for char in text:\n",
    "        if char in char_count:\n",
    "            char_count[char] += 1\n",
    "        else:\n",
    "            char_count[char] = 1\n",
    "    # ratio%ä»¥ä¸Šç¹°ã‚Šè¿”ã•ã‚Œã‚‹æ–‡å­—ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "    repeated_chars = [char for char, count in char_count.items(\n",
    "    ) if count >= threshold_ratio*len(text)]\n",
    "    # ç¹°ã‚Šè¿”ã•ã‚Œã‚‹æ–‡å­—ãŒãªã‘ã‚Œã°matrix_tempã«è¿½åŠ \n",
    "    if not repeated_chars:\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def is_repetitive_japanese(text, thresholds={\n",
    "    'line_dup': 0.30,\n",
    "    'paragraph_dup': 0.30,\n",
    "    'char_in_line_dup': 0.20,\n",
    "    'char_in_paragraph_dup': 0.20,\n",
    "    '2-gram': 0.20,\n",
    "    '3-gram': 0.18,\n",
    "    '4-gram': 0.16,\n",
    "    '5-gram': 0.15,\n",
    "    '6-gram': 0.14,\n",
    "    '7-gram': 0.13,\n",
    "    '8-gram': 0.12,\n",
    "    '9-gram': 0.11,\n",
    "        '10-gram': 0.10}):\n",
    "    if text == \"\":\n",
    "        return \"\"\n",
    "    # æ®µè½ã¨è¡Œã«åˆ†å‰²\n",
    "    paragraphs = text.split('\\n')\n",
    "    lines = text.replace('\\n', ' ').split('ã€‚')\n",
    "\n",
    "    # æ®µè½ã¨è¡Œã®é‡è¤‡ç‡ã‚’è¨ˆç®—\n",
    "    paragraph_dup_rate = calc_dup_rate(paragraphs)\n",
    "    line_dup_rate = calc_dup_rate(lines)\n",
    "\n",
    "    # æ–‡å­—ã«å«ã¾ã‚Œã‚‹é‡è¤‡ã®å‰²åˆã‚’è¨ˆç®—\n",
    "    char_in_paragraph_dup_rate = calc_char_dup_rate(paragraphs, text)\n",
    "    char_in_line_dup_rate = calc_char_dup_rate(lines, text)\n",
    "\n",
    "    # n-gramã®é‡è¤‡ç‡ã‚’è¨ˆç®—\n",
    "    ngram_dup_rates = {}\n",
    "    for n in range(2, 11):\n",
    "        ngrams = extract_ngrams(text.replace('\\n', ''), n)\n",
    "        if n < 5:\n",
    "            # æœ€é »å‡ºã®n-gramã®å‡ºç¾å›æ•°ã‚’è¨ˆç®—\n",
    "            ngram_dup_rates[n] = calc_max_freq_rate(ngrams)\n",
    "        else:\n",
    "            # 2å›ä»¥ä¸Šå‡ºç¾ã™ã‚‹n-gramã®ç·å‡ºç¾å›æ•°ã‚’è¨ˆç®—\n",
    "            ngram_dup_rates[n] = calc_total_dup_freq_rate(ngrams)\n",
    "\n",
    "    # å„æŒ‡æ¨™ãŒé–¾å€¤ã‚’è¶…ãˆã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "    if (line_dup_rate > thresholds['line_dup'] or\n",
    "        paragraph_dup_rate > thresholds['paragraph_dup'] or\n",
    "        char_in_line_dup_rate > thresholds['char_in_line_dup'] or\n",
    "        char_in_paragraph_dup_rate > thresholds['char_in_paragraph_dup'] or\n",
    "            any(ngram_dup_rates[n] > thresholds[f'{n}-gram'] for n in range(2, 11))):\n",
    "        if line_dup_rate > thresholds['line_dup']:\n",
    "            print(\"line_dup\")\n",
    "        if paragraph_dup_rate > thresholds['paragraph_dup']:\n",
    "            print(\"paragraph_dup\")\n",
    "        if char_in_line_dup_rate > thresholds['char_in_line_dup']:\n",
    "            print(\"char_in_line_dup\")\n",
    "        if char_in_paragraph_dup_rate > thresholds['char_in_paragraph_dup']:\n",
    "            print('char_in_paragraph_dup')\n",
    "        for n in range(2, 11):\n",
    "            if ngram_dup_rates[n] > thresholds[f'{n}-gram']:\n",
    "                print(f\"{n}-gram\")\n",
    "                #print(text)\n",
    "        return \"\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def calc_dup_rate(items):\n",
    "    counter = Counter(items)\n",
    "    total = len(items)\n",
    "    dup_count = sum(1 for count in counter.values() if count > 1)\n",
    "    return dup_count / total if total > 0 else 0\n",
    "\n",
    "\n",
    "def calc_char_dup_rate(items, text):\n",
    "    counter = Counter(items)\n",
    "    total_chars = len(text)\n",
    "    dup_chars = sum(len(item) * (count - 1)\n",
    "                    for item, count in counter.items() if count > 1)\n",
    "    return dup_chars / total_chars if total_chars > 0 else 0\n",
    "\n",
    "\n",
    "def extract_ngrams(text, n):\n",
    "    return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "\n",
    "def calc_max_freq_rate(ngrams):\n",
    "    counter = Counter(ngrams)\n",
    "    total = len(ngrams)\n",
    "    max_freq = max(counter.values())\n",
    "    return max_freq / total if total > 0 else 0\n",
    "\n",
    "\n",
    "def calc_total_dup_freq_rate(ngrams):\n",
    "    counter = Counter(ngrams)\n",
    "    total = len(ngrams)\n",
    "    dup_freq = sum(count - 1 for count in counter.values() if count > 1)\n",
    "    return dup_freq / total if total > 0 else 0\n",
    "\n",
    "# text = \"ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™ã€‚ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™ã€‚ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™ã€‚\\nã“ã‚Œã‚‚ãƒ†ã‚¹ãƒˆã§ã™ã€‚ã“ã‚Œã‚‚ãƒ†ã‚¹ãƒˆã§ã™ã€‚\"\n",
    "\n",
    "# print(text)\n",
    "# print(is_repetitive_japanese(text, thresholds))  # ç©ºã®æ–‡å­—åˆ—ã‚’å‡ºåŠ›\n",
    "count=0\n",
    "new_texts=[]\n",
    "for text in texts:\n",
    "    #print(text)\n",
    "    text=extract(text, include_tables=False,target_lang='ja',favour_precision=True)\n",
    "    if text ==None:\n",
    "        continue\n",
    "\n",
    "    #count+=1\n",
    "    \n",
    "    # if count<4:\n",
    "    #     print(\"extract\",text)\n",
    "    text=is_repetitive_japanese(text)\n",
    "    if text !=\"\":\n",
    "        new_texts.append(text)\n",
    "    else:\n",
    "        count+=1\n",
    "        print(f\"error{count}\")\n",
    "    # if count>=4:\n",
    "    #     break\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_n_grams(words: list[str], n: int) -> list[str]:\n",
    "    return [\" \".join(words[i : i + n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "\n",
    "def find_duplicates(x: list[str]) -> tuple[int, int]:\n",
    "    unique_x = set()\n",
    "    duplicate_chars = 0\n",
    "    duplicate_elements = 0\n",
    "    for element in x:\n",
    "        if element in unique_x:\n",
    "            duplicate_chars += len(element)\n",
    "            duplicate_elements += 1\n",
    "\n",
    "        else:\n",
    "            unique_x.add(element)\n",
    "    return duplicate_elements, duplicate_chars\n",
    "\n",
    "def find_top_duplicate(x: list[str]) -> int:\n",
    "    counter = Counter()\n",
    "    for element in x:\n",
    "        counter[element] += 1\n",
    "    top_n_gram = counter.most_common(1)[0]\n",
    "    return len(top_n_gram[0]) * top_n_gram[1]\n",
    "\n",
    "\n",
    "def find_all_duplicate(words: list[str], n: int) -> int:\n",
    "    n_words = len(words)\n",
    "    unique = set()\n",
    "    repeated_chars, idx = 0, 0\n",
    "    while idx < n_words - n + 1:\n",
    "        n_gram = \"\".join(words[idx : idx + n])\n",
    "        if n_gram in unique:\n",
    "            repeated_chars += len(n_gram)\n",
    "            idx += n\n",
    "        else:\n",
    "            unique.add(n_gram)\n",
    "            idx += 1\n",
    "    assert repeated_chars <= len(\"\".join(words))\n",
    "    return repeated_chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter(self, doc: Document) -> bool | tuple[bool, str]:\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    text = doc.text\n",
    "\n",
    "    paragraphs = self.paragraph_exp.split(text.strip())\n",
    "    paragraphs_duplicates, char_duplicates = find_duplicates(paragraphs)\n",
    "    if self.dup_para_frac and paragraphs_duplicates / len(paragraphs) > self.dup_para_frac:\n",
    "        return False, \"dup_para_frac\"\n",
    "    if self.dup_para_char_frac and char_duplicates / len(text) > self.dup_para_char_frac:\n",
    "        return False, \"dup_para_char_frac\"\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    line_duplicates, char_duplicates = find_duplicates(lines)\n",
    "    if self.dup_line_frac and line_duplicates / len(lines) > self.dup_line_frac:\n",
    "        return False, \"dup_line_frac\"\n",
    "    if self.dup_line_char_frac and char_duplicates / len(text) > self.dup_line_char_frac:\n",
    "        return False, \"dup_line_char_frac\"\n",
    "\n",
    "    words = word_tokenize(text, language=\"english\")  # TODO we should use language id filter\n",
    "\n",
    "    for n, n_frac in self.top_n_grams:\n",
    "        n_grams = get_n_grams(words, n)\n",
    "        if not n_grams:\n",
    "            continue\n",
    "        top_char_length = find_top_duplicate(n_grams)\n",
    "        if top_char_length / len(text) > n_frac:\n",
    "            return False, f\"top_{n}_gram\"\n",
    "\n",
    "    for n, n_frac in self.dup_n_grams:\n",
    "        n_duplicates_char = find_all_duplicate(words, n)\n",
    "        if n_duplicates_char / len(text) > n_frac:\n",
    "            return False, f\"duplicated_{n}_n_grams\"\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸\n",
    "è£½å“ãƒãƒ‹ãƒ¥ã‚¢ãƒ«\n",
    "ã‚ˆãã‚ã‚‹ã”è³ªå•[FAQ]\n",
    "ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n",
    "ã™ã¹ã¦ã‹ã‚‰æ¤œç´¢\n",
    "è£½å“ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‹ã‚‰æ¤œç´¢\n",
    "ã‚ˆãã‚ã‚‹è³ªå•[FAQ]ã‹ã‚‰æ¤œç´¢\n",
    "ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‹ã‚‰æ¤œç´¢\n",
    "ãŠçŸ¥ã‚‰ã›\n",
    "éšœå®³æƒ…å ±\n",
    "ãƒã‚¤ã‚µã‚¤ãƒˆã‚’ç·¨é›†\n",
    "ã‚µã‚¤ãƒˆã®çµ±åˆãƒ»ä¸€éƒ¨ãƒšãƒ¼ã‚¸ã®ç§»è¡Œãƒ»åˆ†å‰²ã¯ã§ãã¾ã™ã‹\n",
    "ï¼œï¼œçµ±åˆã‚„ç§»è¡Œã«ã¤ã„ã¦ï¼ï¼å„ã‚µã‚¤ãƒˆã”ã¨ã«ã‚µã‚¤ãƒˆã‚’æ§‹ç¯‰ãƒ»ç®¡ç†ã—ã¦ã„ã¾ã™ã®ã§ã€ã‚µã‚¤ãƒˆAã¨ã‚µã‚¤ãƒˆBã‚’çµ±åˆã—ãŸã‚Šã€å„ã‚µã‚¤ãƒˆã®ä¸€éƒ¨ã®ãƒšãƒ¼ã‚¸ã®ã¿ç§»è¡Œã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚ ï¼œï¼œåˆ†å‰²ã«ã¤ã„ã¦ï¼ï¼ åˆ†å‰²ã‚’è¡Œ...\n",
    "ã‚ˆãã‚ã‚‹è³ªå•\n",
    "å…¨èˆ¬\n",
    "ã”åˆ©ç”¨ç’°å¢ƒãƒ»ç·¨é›†é–¢é€£\n",
    "ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ‘ãƒãƒ«\n",
    "ã‚¢ãƒ—ãƒªã‚’èµ·å‹•\n",
    "å„ç¨®è¨­å®š\n",
    "ã‚µã‚¤ãƒˆã‚·ã‚¢ã‚¿ãƒ¼ç”»é¢\n",
    "ãƒã‚¤ã‚µã‚¤ãƒˆã‚’ç·¨é›†\n",
    "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é¸ã¶ãƒ»ãƒ–ãƒ©ãƒ³ã‚¯ã‚µã‚¤ãƒˆã‹ã‚‰ä½œæˆ\n",
    "ã‚µã‚¤ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "ã‚µã‚¤ãƒˆã‚¨ãƒ‡ã‚£ã‚¿ç”»é¢\n",
    "å…¨èˆ¬\n",
    "ã‚µã‚¤ãƒˆè¨­å®š\n",
    "ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»ã‚µã‚¤ãƒˆã‚’å…¬é–‹\n",
    "Dressãƒ»ã‚¦ã‚§ãƒ–ãƒ•ã‚©ãƒ³ãƒˆãƒ»ãƒšãƒ¼ã‚¸ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ\n",
    "ãƒ–ãƒ­ãƒƒã‚¯ã‚¨ãƒ‡ã‚£ã‚¿ç”»é¢\n",
    "ãƒ–ãƒ­ãƒƒã‚¯ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ\n",
    "ç·¨é›†\n",
    "è¨­å®š\n",
    "CSSè©³ç´°è¨­å®š\n",
    "å„ç¨®ãƒ‘ãƒ¼ãƒ„\n",
    "ç”»åƒã‚’é¸æŠ\n",
    "ç”»åƒã‚’ä½œæˆ(SiGN Pro)\n",
    "ã‚¹ãƒ©ã‚¤ãƒ‰ã‚·ãƒ§ãƒ¼\n",
    "ãƒªãƒ³ã‚¯\n",
    "ã‚¢ã‚¤ã‚³ãƒ³ãƒ»åŒºåˆ‡ã‚Šç·š\n",
    "è¡¨ ãƒ†ãƒ¼ãƒ–ãƒ«\n",
    "ãƒ•ã‚©ãƒ¼ãƒ \n",
    "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚°\n",
    "ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ç³»\n",
    "ã‚¦ã‚§ãƒ–ã‚µãƒ¼ãƒ“ã‚¹ç³»\n",
    "ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã‚«ãƒ¼ãƒˆãƒ‘ãƒ¼ãƒ„\n",
    "ãã®ä»–\n",
    "SYNC for WebLiFE*5\n",
    "ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ç³»/ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒœã‚¿ãƒ³ãƒ»Instagram\n",
    "ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ç³»/ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ï½¥Twitterãƒãƒƒã‚¸\n",
    "ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ç³»/Page Pluginãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæ¬„\n",
    "ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ç³»/YouTubeãƒ»Ustream\n",
    "ã‚¦ã‚§ãƒ–ã‚µãƒ¼ãƒ“ã‚¹ç³»/Googleãƒãƒƒãƒ—ãƒ»Yahoo!åœ°å›³\n",
    "ã‚¦ã‚§ãƒ–ã‚µãƒ¼ãƒ“ã‚¹ç³»/ã‚µã‚¤ãƒˆå†…æ¤œç´¢ãƒ»ç¿»è¨³\n",
    "ã‚¦ã‚§ãƒ–ã‚µãƒ¼ãƒ“ã‚¹ç³»/ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»ãƒ–ãƒ­ã‚°\n",
    "ã‚¦ã‚§ãƒ–ã‚µãƒ¼ãƒ“ã‚¹ç³»/ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\n",
    "ã‚µã‚¤ãƒˆãƒ»ã‚³ãƒ¼ãƒŠãƒ¼ãƒ»ãƒšãƒ¼ã‚¸è¨­å®š\n",
    "ã‚µã‚¤ãƒˆè¨­å®š\n",
    "ã‚³ãƒ¼ãƒŠãƒ¼è¨­å®š\n",
    "ãƒšãƒ¼ã‚¸è¨­å®š\n",
    "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»FTPè¨­å®š\n",
    "ã‚µãƒ¼ãƒãƒ¼é–¢é€£\n",
    "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–¢é€£\n",
    "é–²è¦§\n",
    "ãã®ä»–ãƒ»ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹\n",
    "å¯¾å¿œè¨€èª\n",
    "ãã®ä»–\n",
    "å„ç¨®è¨­å®š\n",
    "å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹é€£æº/Benchmark Email\n",
    "ã‚µã‚¤ãƒˆç®¡ç†/ã‹ã‚“ãŸã‚“ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰\n",
    "ã‚µã‚¤ãƒˆç®¡ç†/ç‹¬è‡ªãƒ‰ãƒ¡ã‚¤ãƒ³\n",
    "Copyright(C)BIGLOBE Inc. 1996-2016\n",
    "filter ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸\n",
    "è£½å“ãƒãƒ‹ãƒ¥ã‚¢ãƒ«\n",
    "ã‚ˆãã‚ã‚‹ã”è³ªå•[FAQ]\n",
    "ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n",
    "ã™ã¹ã¦ã‹ã‚‰æ¤œç´¢\n",
    "è£½å“ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‹ã‚‰æ¤œç´¢\n",
    "ã‚ˆãã‚ã‚‹è³ªå•[FAQ]ã‹ã‚‰æ¤œç´¢\n",
    "ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‹ã‚‰æ¤œç´¢\n",
    "ãŠçŸ¥ã‚‰ã›\n",
    "éšœå®³æƒ…å ±\n",
    "ãƒã‚¤ã‚µã‚¤ãƒˆã‚’ç·¨é›†\n",
    "ã‚µã‚¤ãƒˆã®çµ±åˆãƒ»ä¸€éƒ¨ãƒšãƒ¼ã‚¸ã®ç§»è¡Œãƒ»åˆ†å‰²ã¯ã§ãã¾ã™ã‹\n",
    "ï¼œï¼œçµ±åˆã‚„ç§»è¡Œã«ã¤ã„ã¦ï¼ï¼å„ã‚µã‚¤ãƒˆã”ã¨ã«ã‚µã‚¤ãƒˆã‚’æ§‹ç¯‰ãƒ»ç®¡ç†ã—ã¦ã„ã¾ã™ã®ã§ã€ã‚µã‚¤ãƒˆAã¨ã‚µã‚¤ãƒˆBã‚’çµ±åˆã—ãŸã‚Šã€å„ã‚µã‚¤ãƒˆã®ä¸€éƒ¨ã®ãƒšãƒ¼ã‚¸ã®ã¿ç§»è¡Œã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚ ï¼œï¼œåˆ†å‰²ã«ã¤ã„ã¦ï¼ï¼ åˆ†å‰²ã‚’è¡Œ...\n",
    "ã‚ˆãã‚ã‚‹è³ªå•\n",
    "å…¨èˆ¬\n",
    "ã”åˆ©ç”¨ç’°å¢ƒãƒ»ç·¨é›†é–¢é€£\n",
    "ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ‘ãƒãƒ«\n",
    "ã‚¢ãƒ—ãƒªã‚’èµ·å‹•\n",
    "å„ç¨®è¨­å®š\n",
    "ã‚µã‚¤ãƒˆã‚·ã‚¢ã‚¿ãƒ¼ç”»é¢\n",
    "ãƒã‚¤ã‚µã‚¤ãƒˆã‚’ç·¨é›†\n",
    "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é¸ã¶ãƒ»ãƒ–ãƒ©ãƒ³ã‚¯ã‚µã‚¤ãƒˆã‹ã‚‰ä½œæˆ\n",
    "ã‚µã‚¤ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "ã‚µã‚¤ãƒˆã‚¨ãƒ‡ã‚£ã‚¿ç”»é¢\n",
    "å…¨èˆ¬\n",
    "ã‚µã‚¤ãƒˆè¨­å®š\n",
    "ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»ã‚µã‚¤ãƒˆã‚’å…¬é–‹\n",
    "Dressãƒ»ã‚¦ã‚§ãƒ–ãƒ•ã‚©ãƒ³ãƒˆãƒ»ãƒšãƒ¼ã‚¸ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ\n",
    "ãƒ–ãƒ­ãƒƒã‚¯ã‚¨ãƒ‡ã‚£ã‚¿ç”»é¢\n",
    "ãƒ–ãƒ­ãƒƒã‚¯ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ\n",
    "ç·¨é›†\n",
    "è¨­å®š\n",
    "CSSè©³ç´°è¨­å®š\n",
    "å„ç¨®ãƒ‘ãƒ¼ãƒ„\n",
    "ç”»åƒã‚’é¸æŠ\n",
    "ç”»åƒã‚’ä½œæˆ(SiGN Pro)\n",
    "ã‚¹ãƒ©ã‚¤ãƒ‰ã‚·ãƒ§ãƒ¼\n",
    "ãƒªãƒ³ã‚¯\n",
    "ã‚¢ã‚¤ã‚³ãƒ³ãƒ»åŒºåˆ‡ã‚Šç·š\n",
    "è¡¨ ãƒ†ãƒ¼ãƒ–ãƒ«\n",
    "ãƒ•ã‚©ãƒ¼ãƒ \n",
    "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚°\n",
    "ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ç³»\n",
    "ã‚¦ã‚§ãƒ–ã‚µãƒ¼ãƒ“ã‚¹ç³»\n",
    "ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã‚«ãƒ¼ãƒˆãƒ‘ãƒ¼ãƒ„\n",
    "ãã®ä»–\n",
    "SYNC for WebLiFE*5\n",
    "ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ç³»/ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒœã‚¿ãƒ³ãƒ»Instagram\n",
    "ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ç³»/ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ï½¥Twitterãƒãƒƒã‚¸\n",
    "ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ç³»/Page Pluginãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæ¬„\n",
    "ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ç³»/YouTubeãƒ»Ustream\n",
    "ã‚¦ã‚§ãƒ–ã‚µãƒ¼ãƒ“ã‚¹ç³»/Googleãƒãƒƒãƒ—ãƒ»Yahoo!åœ°å›³\n",
    "ã‚¦ã‚§ãƒ–ã‚µãƒ¼ãƒ“ã‚¹ç³»/ã‚µã‚¤ãƒˆå†…æ¤œç´¢ãƒ»ç¿»è¨³\n",
    "ã‚¦ã‚§ãƒ–ã‚µãƒ¼ãƒ“ã‚¹ç³»/ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»ãƒ–ãƒ­ã‚°\n",
    "ã‚¦ã‚§ãƒ–ã‚µãƒ¼ãƒ“ã‚¹ç³»/ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\n",
    "ã‚µã‚¤ãƒˆãƒ»ã‚³ãƒ¼ãƒŠãƒ¼ãƒ»ãƒšãƒ¼ã‚¸è¨­å®š\n",
    "ã‚µã‚¤ãƒˆè¨­å®š\n",
    "ã‚³ãƒ¼ãƒŠãƒ¼è¨­å®š\n",
    "ãƒšãƒ¼ã‚¸è¨­å®š\n",
    "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»FTPè¨­å®š\n",
    "ã‚µãƒ¼ãƒãƒ¼é–¢é€£\n",
    "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–¢é€£\n",
    "é–²è¦§\n",
    "ãã®ä»–ãƒ»ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹\n",
    "å¯¾å¿œè¨€èª\n",
    "ãã®ä»–\n",
    "å„ç¨®è¨­å®š\n",
    "å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹é€£æº/Benchmark Email\n",
    "ã‚µã‚¤ãƒˆç®¡ç†/ã‹ã‚“ãŸã‚“ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰\n",
    "ã‚µã‚¤ãƒˆç®¡ç†/ç‹¬è‡ªãƒ‰ãƒ¡ã‚¤ãƒ³\n",
    "Copyright(C)BIGLOBE Inc. 1996-2016\n",
    "extract ä¸€é–¢å¸‚ã®åºƒå‘Šä»£ç†åº—FREE LINEï¼ˆãƒ•ãƒªãƒ¼ãƒ©ã‚¤ãƒ³ï¼‰ã¯ãŠå¾—ãªã‚¯ãƒ¼ãƒãƒ³æƒ…å ±ã‚’ãƒ•ãƒªãƒ¼ãƒšãƒ¼ãƒ‘ãƒ¼ã§ç´¹ä»‹ã—ã¦ãŠã‚Šã¾ã™\n",
    "åºƒå‘Šä»£ç†åº—FREE LINEï¼ˆãƒ•ãƒªãƒ¼ãƒ©ã‚¤ãƒ³ï¼‰ã§ã¯ä¸€é–¢å¸‚ã®ãƒ•ãƒªãƒ¼ãƒšãƒ¼ãƒ‘ãƒ¼ã‚’ç™ºè¡Œã—ã¦ãŠã‚Šã¾ã™ã€‚ãŠå¾—ãªæƒ…å ±æº€è¼‰ã§ã€çš†æ§˜ã®ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«ã‚’å¿œæ´ã„ãŸã—ã¾ã™ã€‚\n",
    "ä¸€é–¢å¸‚ã®ãƒ•ãƒªãƒ¼ãƒšãƒ¼ãƒ‘ãƒ¼ã«æƒ…å ±æ²è¼‰ã‚’ãŠè€ƒãˆãªã‚‰åºƒå‘Šä»£ç†åº—ã€ŒFREE LINEï¼ˆãƒ•ãƒªãƒ¼ãƒ©ã‚¤ãƒ³ï¼‰ã€ã¾ã§ã”é€£çµ¡ãã ã•ã„ï¼\n",
    "True\n",
    "filter \n",
    "extract Hair Scene023\n",
    "Creatorï¼šFetish Factory\n",
    "Formatï¼šdownload\n",
    "Scene timeï¼š00:11:54\n",
    "Price 999yen\n",
    "<ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ç‰¹åˆ¥ä¾¡æ ¼>\n",
    "\"é«ªå¤§å¥½ã\"ä¼¸ã°ã›ã‚‹ã¨ã“ã‚ã¾ã§ä¼¸ã°ã—ãŸã„ã¨ã„ã†é»’é«ªãƒ­ãƒ³ã‚°ã®ã‚†ã‹ã‚Šã•ã‚“ã€‚ã¡ã‚‡ã£ã¨ã‚µãƒ©ã‚µãƒ©ã®é»’é«ªãƒ­ãƒ³ã‚°ã‚’è§¦ã‚‰ã›ã¦ã‚‚ã‚‰ã„ã¾ã—ãŸã€‚ç€è¡£ã®ã¾ã¾ã§æ´—é«ªã®ãŠæ‰‹å…¥ã‚Œã€‚\n",
    "This is an image of woman's hair.\n",
    "ï¼œå½“ã‚µã‚¤ãƒˆå†…ã§ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ãŒã§ãã¾ã™ã€‚ï¼You can search keywords in this site.\n",
    "ï¼ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒˆãƒƒãƒ—ã¸æˆ»ã‚‹(Category top)\n",
    "ãƒ»ã”åˆ©ç”¨ã‚¬ã‚¤ãƒ‰\n",
    "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è²©å£²ã®ã‚·ã‚¹ãƒ†ãƒ ã¯XCREAMã«å§”è¨—ã—ã¦ãŠã‚Šã¾ã™ã€‚\n",
    "ãƒ»\n",
    "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«ã¤ã„ã¦\n",
    "ãƒ»\n",
    "Engloish FAQ\n",
    ">\n",
    "ä¸­æ–‡(ç°¡ä½“å­—)\n",
    ">\n",
    "ä¸­æ–‡(ç¹ä½“å­—)\n",
    ">\n",
    "í•œêµ­ì–´\n",
    "ãƒ»æ±ºæ¸ˆæ‰‹æ®µ\n",
    "ãŠæ”¯æ‰•ã¯ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã¨ãƒ“ãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ EXãŒã”åˆ©ç”¨ã§ãã¾ã™ã€‚\n",
    "æ ªå¼ä¼šç¤¾ã‚¼ã‚¦ã‚¹ãŒæä¾›ã™ã‚‹ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰æ±ºæ¸ˆã‚·ã‚¹ãƒ†ãƒ ã‚’å°å…¥ã€å®‰å¿ƒã—ã¦ãŠè²·ã„ç‰©ã„ãŸã ã‘ã¾ã™ã€‚\n",
    "Copyright(C) GAGON.COM All right reserved.\n",
    "filter Hair Scene023\n",
    "Creatorï¼šFetish Factory\n",
    "Formatï¼šdownload\n",
    "Scene timeï¼š00:11:54\n",
    "Price 999yen\n",
    "<ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ç‰¹åˆ¥ä¾¡æ ¼>\n",
    "\"é«ªå¤§å¥½ã\"ä¼¸ã°ã›ã‚‹ã¨ã“ã‚ã¾ã§ä¼¸ã°ã—ãŸã„ã¨ã„ã†é»’é«ªãƒ­ãƒ³ã‚°ã®ã‚†ã‹ã‚Šã•ã‚“ã€‚ã¡ã‚‡ã£ã¨ã‚µãƒ©ã‚µãƒ©ã®é»’é«ªãƒ­ãƒ³ã‚°ã‚’è§¦ã‚‰ã›ã¦ã‚‚ã‚‰ã„ã¾ã—ãŸã€‚ç€è¡£ã®ã¾ã¾ã§æ´—é«ªã®ãŠæ‰‹å…¥ã‚Œã€‚\n",
    "This is an image of woman's hair.\n",
    "ï¼œå½“ã‚µã‚¤ãƒˆå†…ã§ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ãŒã§ãã¾ã™ã€‚ï¼You can search keywords in this site.\n",
    "ï¼ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒˆãƒƒãƒ—ã¸æˆ»ã‚‹(Category top)\n",
    "ãƒ»ã”åˆ©ç”¨ã‚¬ã‚¤ãƒ‰\n",
    "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è²©å£²ã®ã‚·ã‚¹ãƒ†ãƒ ã¯XCREAMã«å§”è¨—ã—ã¦ãŠã‚Šã¾ã™ã€‚\n",
    "ãƒ»\n",
    "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«ã¤ã„ã¦\n",
    "ãƒ»\n",
    "Engloish FAQ\n",
    ">\n",
    "ä¸­æ–‡(ç°¡ä½“å­—)\n",
    ">\n",
    "ä¸­æ–‡(ç¹ä½“å­—)\n",
    ">\n",
    "í•œêµ­ì–´\n",
    "ãƒ»æ±ºæ¸ˆæ‰‹æ®µ\n",
    "ãŠæ”¯æ‰•ã¯ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã¨ãƒ“ãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ EXãŒã”åˆ©ç”¨ã§ãã¾ã™ã€‚\n",
    "æ ªå¼ä¼šç¤¾ã‚¼ã‚¦ã‚¹ãŒæä¾›ã™ã‚‹ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰æ±ºæ¸ˆã‚·ã‚¹ãƒ†ãƒ ã‚’å°å…¥ã€å®‰å¿ƒã—ã¦ãŠè²·ã„ç‰©ã„ãŸã ã‘ã¾ã™ã€‚\n",
    "Copyright(C) GAGON.COM All right reserved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å®Ÿé¨“\n",
    "- ç‰¹å®šã®å‡¦ç†ã§å®Ÿé¨“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: tqdm in /data/satori_hdd4/takumi/.local/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: joblib in /data/satori_hdd4/takumi/.local/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /data/satori_hdd4/takumi/.local/lib/python3.10/site-packages (from nltk) (2023.6.3)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  å°‘é‡ã®WARCå½¢å¼ã®ã‚‚ã®ã«å¯¾ã—ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from datatrove.utils.text import PUNCTUATION_SET\n",
    "\n",
    "PUNCTUATION = \"!/â€”â€:ï¼…ï¼‘ã€ˆ&(ã€â”\\\\ã€#%ã€Œã€ï¼Œã€‘ï¼›+^]~â€œã€Šâ€';â€™{|âˆ¶Â´[=-`*ï¼ï¼ˆâ€“ï¼Ÿï¼ï¼š$ï½Â«ã€‰,><ã€‹)?ï¼‰ã€‚â€¦@_.\\\"}â–ºÂ»\" + \"\".join(\n",
    "    map(\n",
    "        chr,\n",
    "        (x for a, b in ((0, 9), (11, 13), (13, 32), (127, 160)) for x in range(a, b)),\n",
    "    )\n",
    ")\n",
    "PUNCTUATION_SET = set(PUNCTUATION)\n",
    "\n",
    "\n",
    "\n",
    "STOP_WORDS = [\"the\", \"be\", \"to\", \"of\", \"and\", \"that\", \"have\", \"with\"]\n",
    "\n",
    "\n",
    "class GopherQualityFilter():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_doc_words: int | None = 50,\n",
    "        max_doc_words: int | None = 100000,\n",
    "        min_avg_word_length: int | None = 3,\n",
    "        max_avg_word_length: int | None = 10,\n",
    "        max_symbol_word_ratio: float | None = 0.1,\n",
    "        max_bullet_lines_ratio: float | None = 0.9,\n",
    "        max_ellipsis_lines_ratio: float | None = 0.3,\n",
    "        max_non_alpha_words_ratio: float | None = 0.8,\n",
    "        min_stop_words: int | None = 2,\n",
    "        stop_words: list[str] | None = None,\n",
    "        #exclusion_writer: DiskWriter = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Filter to apply Gopher's quality heuristic rules.\n",
    "        \"\"\"\n",
    "        self.min_doc_words = min_doc_words\n",
    "        self.max_doc_words = max_doc_words\n",
    "        self.min_avg_word_length = min_avg_word_length\n",
    "        self.max_avg_word_length = max_avg_word_length\n",
    "        self.max_symbol_word_ratio = max_symbol_word_ratio\n",
    "        self.max_bullet_lines_ratio = max_bullet_lines_ratio\n",
    "        self.max_ellipsis_lines_ratio = max_ellipsis_lines_ratio\n",
    "        self.max_non_alpha_words_ratio = max_non_alpha_words_ratio\n",
    "        self.min_stop_words = min_stop_words\n",
    "        self.stop_words = set(STOP_WORDS if stop_words is None else stop_words)\n",
    "\n",
    "    def filter(self, text: str) -> bool | tuple[bool, str]:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            doc: Applies the heuristics rules to decide if a document should be REMOVED\n",
    "\n",
    "        Returns: False if sample.text does not pass any of the the heuristic tests\n",
    "\n",
    "        \"\"\"\n",
    "        from nltk.tokenize import word_tokenize\n",
    "\n",
    "        words = word_tokenize(text)  # TODO we should use language id filter\n",
    "        n_words = len(words)\n",
    "        non_symbol_words = [w for w in words if any(ch not in PUNCTUATION_SET for ch in w)]\n",
    "        n_non_symbol_words_words = len(non_symbol_words)\n",
    "\n",
    "        # words < min_doc_words or words > max_doc_words\n",
    "        #if self.min_doc_words and n_non_symbol_words_words < self.min_doc_words:\n",
    "         #   print(text)\n",
    "           # return \"\",False, \"gopher_short_doc\"\n",
    "        if self.max_doc_words and n_non_symbol_words_words > self.max_doc_words:\n",
    "            return \"\",False, \"gopher_long_doc\"\n",
    "\n",
    "        # mean word length is outside the range of 3 to 10 characters\n",
    "        avg_n_words = np.mean([len(w) for w in non_symbol_words])\n",
    "        if self.min_avg_word_length and avg_n_words < self.min_avg_word_length:\n",
    "            return \"\",False, \"gopher_below_avg_threshold\"\n",
    "        if self.max_avg_word_length and avg_n_words > self.max_avg_word_length:\n",
    "            return \"\",False, \"gopher_above_avg_threshold\"\n",
    "\n",
    "        # symbol-to-word ratio greater than 0.1 for either the hash symbol or the ellipsis\n",
    "        if self.max_symbol_word_ratio and text.count(\"#\") / n_words > self.max_symbol_word_ratio:\n",
    "            return \"\",False, \"gopher_too_many_hashes\"\n",
    "        if self.max_symbol_word_ratio and (text.count(\"...\") + text.count(\"â€¦\")) / n_words > self.max_symbol_word_ratio:\n",
    "            return \"\",False, \"gopher_too_many_ellipsis\"\n",
    "\n",
    "        # any document with more than 90 % of lines starting with a bullet point,\n",
    "        # or more than 30 % ending with an ellipsis.\n",
    "        lines = text.splitlines()\n",
    "        if (\n",
    "            self.max_bullet_lines_ratio\n",
    "            and sum(s.lstrip().startswith(\"â€¢\") or s.lstrip().startswith(\"-\") for s in lines) / len(lines)\n",
    "            > self.max_bullet_lines_ratio\n",
    "        ):\n",
    "            return \"\",False, \"gopher_too_many_bullets\"\n",
    "        if (\n",
    "            self.max_ellipsis_lines_ratio\n",
    "            and sum(s.rstrip().endswith(\"...\") or s.rstrip().endswith(\"â€¦\") for s in lines) / len(lines)\n",
    "            > self.max_ellipsis_lines_ratio\n",
    "        ):\n",
    "            return \"\",False, \"gopher_too_many_end_ellipsis\"\n",
    "\n",
    "        # that 80 % of words in a document contain at least one alphabetic character\n",
    "        if (\n",
    "            self.max_non_alpha_words_ratio\n",
    "            and sum([any((c.isalpha() for c in w)) for w in words]) / n_words < self.max_non_alpha_words_ratio\n",
    "        ):\n",
    "            return \"\" ,False, \"gopher_below_alpha_threshold\"\n",
    "\n",
    "        # stop word filter\n",
    "        # if self.min_stop_words and sum(w in self.stop_words for w in words) < self.min_stop_words:\n",
    "        #     print(text)\n",
    "        #     return \"\",False, \"gopher_enough_stop_words\"\n",
    "\n",
    "        return text ,True ,\"ok\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "gopher_below_alpha_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_below_alpha_threshold\n",
      "gopher_above_avg_threshold\n",
      "ok\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "ok\n",
      "gopher_above_avg_threshold\n",
      "ok\n",
      "gopher_above_avg_threshold\n",
      "ok\n",
      "gopher_above_avg_threshold\n",
      "ok\n",
      "gopher_above_avg_threshold\n",
      "gopher_below_alpha_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_below_alpha_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_below_alpha_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "ok\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_below_alpha_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "ok\n",
      "ok\n",
      "gopher_above_avg_threshold\n",
      "ok\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n",
      "ok\n",
      "ok\n",
      "gopher_above_avg_threshold\n",
      "gopher_above_avg_threshold\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "quality_filter=GopherQualityFilter()\n",
    "new_texts2=[]\n",
    "for text in new_texts:\n",
    "    text, _ , m =quality_filter.filter(text)\n",
    "    print(m)\n",
    "    if text!=\"\":\n",
    "        new_texts2.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TOPãƒšãƒ¼ã‚¸\\næ–°è¦ç™»éŒ²\\nãƒã‚¤ãƒšãƒ¼ã‚¸\\nãƒ–ãƒ­ã‚°ãƒªãƒ¼ãƒ€ãƒ¼\\nåˆ©ç”¨è¦ç´„\\nåºƒå‘Šæ²è¼‰\\nãƒ˜ãƒ«ãƒ—\\nNEW\\nãƒ–ãƒ­ã‚¬ãƒ¼ã®ãŸã‚ã®SNSã€Œãƒ–ãƒ­ã‚°ã‚µãƒ¼ã‚¯ãƒ«ã€\\nãƒ–ãƒ­ã‚°ã«ç‰¹åŒ–ã—ãŸæ–°ã—ã„SNSãŒèª•ç”Ÿï¼ åŒã˜è¶£å‘³ã®ä»²é–“ã¨ã¤ãªãŒã£ãŸã‚Šã€è‡ªåˆ†ã®ãƒ–ãƒ­ã‚°ã‚’ã¿ã‚“ãªã«ã‚¢ãƒ”ãƒ¼ãƒ«ã—ã‚ˆã†ã€‚\\nä»Šã™ãç™»éŒ²\\nãƒ„ã‚¤ãƒ¼ãƒˆ\\næŠ•ç¥¨TOP\\næ–°ç€ãƒˆãƒ”ãƒƒã‚¯\\nè©±é¡Œã®ãƒˆãƒ”ãƒƒã‚¯\\nä½œæˆãƒ»ç®¡ç†\\nã‚«ãƒ†ã‚´ãƒª\\nã‚µãƒ¼ãƒ“ã‚¹ã‚¬ã‚¤ãƒ‰\\nï¼´ï¼¯ï¼°\\nä¹—ã‚Šç‰©\\né£›è¡Œæ©Ÿ\\nå—ä»˜æœŸé–“ : 2011å¹´06æœˆ11æ—¥ã€œç„¡æœŸé™\\nè‡ªè¡›éšŠèˆªç©ºæ©Ÿã§å¥½ããªæ©Ÿç¨®ã¯ï¼Ÿ\\nF-2\\nF-4 ãƒ•ã‚¡ãƒ³ãƒˆãƒ \\nF-15 ã‚¤ãƒ¼ã‚°ãƒ«\\nE-2C ãƒ›ãƒ¼ã‚¯ã‚¢ã‚¤\\nT-4 ï¼ˆãƒ–ãƒ«ãƒ¼ã‚‚ï¼‰\\nC-ï¼‘\\nYS-11\\nCH-47\\nUH-60\\nAH-1S ã‚³ãƒ–ãƒ©\\nUH-1H ã‚¤ãƒ­ã‚³ã‚¤\\nP-ï¼“C ã‚ªãƒ©ã‚¤ã‚ªãƒ³\\nC-130 ãƒãƒ¼ã‚­ãƒ¥ãƒªãƒ¼ã‚º\\nUS-1A\\nãªã„\\nãã®ä»–ï¼ˆéå»ã«æ‰€å±ã—ãŸæ©Ÿç¨®ã‚‚ï¼‰\\nã‚³ãƒ¡ãƒ³ãƒˆ\\næ€§åˆ¥ï¼š\\nã€æœªé¸æŠã€‘\\nç”·æ€§\\nå¥³æ€§\\nå¹´é½¢ï¼š\\nã€æœªé¸æŠã€‘\\n10æœªæº€\\n10æ­³ä»£\\n20æ­³ä»£\\n30æ­³ä»£\\n40æ­³ä»£\\n50æ­³ä»£\\n60æ­³ä»£\\n70æ­³ä»£\\n80æ­³ä»£\\n90æ­³ä»£\\n100ä»¥ä¸Š\\nåœ°åŸŸï¼š\\nã€æœªé¸æŠã€‘\\nåŒ—æµ·é“\\né’æ£®\\nå²©æ‰‹\\nç§‹ç”°\\nå®®åŸ\\nå±±å½¢\\nç¦å³¶\\næ–°æ½Ÿ\\næ ƒæœ¨\\nèŒ¨åŸ\\nç¾¤é¦¬\\nåŸ¼ç‰\\næ±äº¬\\nåƒè‘‰\\nç¥å¥ˆå·\\nå±±æ¢¨\\né™å²¡\\né•·é‡\\nå¯Œå±±\\nå²é˜œ\\næ„›çŸ¥\\nçŸ³å·\\nç¦äº•\\næ»‹è³€\\nä¸‰é‡\\näº¬éƒ½\\nå¥ˆè‰¯\\nå¤§é˜ª\\nå’Œæ­Œå±±\\nå…µåº«\\né³¥å–\\nå²¡å±±\\nå³¶æ ¹\\nåºƒå³¶\\nå±±å£\\né¦™å·\\nå¾³å³¶\\né«˜çŸ¥\\næ„›åª›\\nç¦å²¡\\nä½è³€\\nå¤§åˆ†\\né•·å´\\nç†Šæœ¬\\nå®®å´\\né¹¿å…å³¶\\næ²–ç¸„\\nã€æµ·å¤–ã€‘\\nÂ©\\nã•ã„ãŸã‹ã®ãƒ–ãƒ­ã‚°\\nã“ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’ãƒ–ãƒ­ã‚°ãƒ‘ãƒ¼ãƒ„ã«ã™ã‚‹\\nã‚µã‚¤ã‚ºé¸æŠ\\nè‡ªå‹•\\nå°ã‚µã‚¤ã‚º\\nä¸­ã‚µã‚¤ã‚º\\nå¤§ã‚µã‚¤ã‚º\\nç›´æ¥ãƒªãƒ³ã‚¯ã‚’å¼µã‚‹å ´åˆã®URL\\nãƒ‡ã‚¶ã‚¤ãƒ³ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹\\n(ç´°ã‹ã„ãƒ‡ã‚¶ã‚¤ãƒ³ã®å¤‰æ›´ãŒå¯èƒ½ã§ã™)\\næŠ•ç¥¨TOPã¸\\nï½œ\\nã€Œé£›è¡Œæ©Ÿã€ã‚«ãƒ†ã‚´ãƒªã¸\\nTOPãƒšãƒ¼ã‚¸\\næ–°è¦ç™»éŒ²\\nãƒã‚¤ãƒšãƒ¼ã‚¸\\nãƒ–ãƒ­ã‚°ãƒªãƒ¼ãƒ€ãƒ¼\\nåˆ©ç”¨è¦ç´„\\nåºƒå‘Šæ²è¼‰\\nãƒ˜ãƒ«ãƒ—\\nCopyright Â© 2004 - 2023 \"\\n@With\\n\" All rights reserved.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_texts2[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WETå½¢å¼ã®ã‚‚ã®ã«å¯¾ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "#mc4ã®èª­ã¿è¾¼ã¿\n",
    "dataset = load_dataset('mc4', 'ja',split='train', streaming=True)\n",
    "n_split=10**2\n",
    "import os\n",
    "\n",
    "corpus_dir=\"corpus/test\"\n",
    "if not os.path.exists(corpus_dir):\n",
    "    os.makedirs(corpus_dir)\n",
    "\n",
    "cnt=0\n",
    "record_id=-1\n",
    "for record in tqdm(dataset):\n",
    "    record_id+=1\n",
    "    \n",
    "    # if not url_filter(record['url']):\n",
    "    #     continue\n",
    "    \n",
    "    text=record['text']\n",
    "\n",
    "    #ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ³\n",
    "    text=clean_text(text)\n",
    "    if text==\"\":\n",
    "        continue\n",
    "\n",
    "    #è¨˜äº‹ã®åˆ¤å®š\n",
    "    # is_noise=annotator.predict(text)\n",
    "    # if is_noise==1:\n",
    "    #     continue\n",
    "\n",
    "    #ã‚¸ãƒ£ãƒ³ãƒ«ã®åˆ¤å®š\n",
    "    cnt+=1\n",
    "\n",
    "    d={\n",
    "        \"id\":record_id,\n",
    "        \"text\":text,\n",
    "    }\n",
    "    # file_name=f\"{corpus_dir}/{cnt//n_split}.txt\"\n",
    "    # with open(file_name, \"a\") as f:\n",
    "    #     f.write(str(d)+\"\\n\")\n",
    "\n",
    "    if cnt>1000:\n",
    "        break\n",
    "    \n",
    "print(f\"åœ§ç¸®ç‡ï¼š{cnt/record_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®æ€§èƒ½è©•ä¾¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä»–ã®å‡¦ç†\n",
    "- Refinedwebã‚’å‚è€ƒã«ã—ã¦æ”¹è‰¯ã—ãŸã„ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
