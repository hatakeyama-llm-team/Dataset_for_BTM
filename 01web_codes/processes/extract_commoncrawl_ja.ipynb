{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfQDVEC-MLnY"
      },
      "source": [
        "# _CommonCrawl日本語データの分割抽出\n",
        "\n",
        " このNotebookは, CommonCrawl日本語データ分割抽出を行うものです.\n",
        "\n",
        "GENIACプロジェクトのTeam Hatakeyama(仮)ではCommonCrawlのデータを\n",
        "\n",
        "質良く加工して日本語LLMの学習に役立つコーパス作成を進めています.\n",
        "\n",
        "そのコーパス作成で問題となるポイントとして,\n",
        "\n",
        "CommonCrawlのデータが多すぎることがあります(なんと100TB程度!).\n",
        "\n",
        "\n",
        " 多くの言語が集まるデータから日本語のデータのみを抜く工程\n",
        "\n",
        "でもデータ数の多さからチームメンバーだけで行うことは困難です.\n",
        "\n",
        "そのため, チーム内外でCommonCrawlからの日本語データを分割で抽出し,\n",
        "\n",
        "最終的に統合するということを目指しています.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9qxJjbgOpJL"
      },
      "source": [
        "## 手順 (Google Colab)..\n",
        "\n",
        "今回CommonCrawlにおける90000個のアーカイブデータ(warc)を分割で処理します.\n",
        "\n",
        "90000個を10個ずつのバッチに分けて処理, その結果をGoogleDriveに格納することで,\n",
        "\n",
        "90000個すべての処理を目指します.\n",
        "\n",
        "そして, その上で行っていただきたいのは,\n",
        "\n",
        "1)このセル以降のセルを上から順番に実行していく.\n",
        "\n",
        "2)最後のセルにあるbatch_numberを変更して, セルを実行\n",
        "https://colab.research.google.com/drive/1Gq8HQ0iyASH5iOAkosclJEQTwYJvYRmy#scrollTo=UawI0uZgAjz6&line=3&uniqifier=1\n",
        "\n",
        "3)ファイルメニューに現れる/submit/に保存されている{batch_number}.gzファイルを\n",
        "ダウンロード\n",
        "\n",
        "4)バッチの処理が終わったことをgenaic slackなどで共有いただき, アップロードするDriveの場所の指示を受けてください.\n",
        "\n",
        "60分程度実行すると, 1バッチ終了します.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz4XELbxhNHu"
      },
      "source": [
        "### 手順 (個別環境 linux)\n",
        "\n",
        "1)このGoogle Colab Notebookをダウンロードしてください.\n",
        "\n",
        "2)jupyterが使用できるpython環境で実行してください."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX7yN4hEuzfS"
      },
      "outputs": [],
      "source": [
        "#for Google Colab\n",
        "import sys\n",
        "from google.colab import drive\n",
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 100})'''))\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "###---- ROOT_PATH を記載 ----###\n",
        "ROOT_PATH = '/content/drive/MyDrive/ColabNotebooks/work/GENIAC/'\n",
        "sys.path.append(ROOT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZT3LGB5AwHQ"
      },
      "source": [
        "## Environment\n",
        "\n",
        "pythonのライブラリをインストールします.\n",
        "\n",
        "※初回だけ実行してください"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOm4205s95Jg"
      },
      "outputs": [],
      "source": [
        "!pip install warcio\n",
        "!pip install beautifulsoup4\n",
        "!pip install trafilatura"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLdgfw9H98Af"
      },
      "source": [
        "### Function - File Utils\n",
        "\n",
        "今回の処理に必要な関数(ファイル処理関係)をインストールします.\n",
        "\n",
        "※初回だけ実行してください"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a73YzpM795GV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import gzip\n",
        "import shutil\n",
        "import requests\n",
        "\n",
        "base_url = \"https://data.commoncrawl.org/\"\n",
        "os.makedirs(\"data/gz\", exist_ok=True)\n",
        "os.makedirs(\"data/warc\", exist_ok=True)\n",
        "\n",
        "def download_file(url, save_path):\n",
        "\n",
        "    response = requests.get(url, stream=True)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        with open(save_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=128):\n",
        "                f.write(chunk)\n",
        "        print(f\"ファイルが正常にダウンロードされました: {save_path}\")\n",
        "    else:\n",
        "        print(f\"ファイルのダウンロードに失敗しました。ステータスコード: {response.status_code}\")\n",
        "\n",
        "\n",
        "def decompress_gz(gz_path, output_path, remove_gz=True, fill_blank_gz=False):\n",
        "    with gzip.open(gz_path, 'rb') as f_in:\n",
        "        with open(output_path, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "    print(f\"{gz_path}が解凍され、{output_path}に保存されました。\")\n",
        "    if remove_gz:\n",
        "        os.remove(gz_path)\n",
        "\n",
        "    if fill_blank_gz:\n",
        "        with open(gz_path, 'w') as f:\n",
        "            f.write(\"\")\n",
        "\n",
        "def get_cc_path_list(path_dir=\"data/path_list/*\"):\n",
        "    path_list = []\n",
        "    for file_path in glob.glob(path_dir):\n",
        "        print(file_path)\n",
        "        with open(file_path, \"r\") as f:\n",
        "            temp_path_list = f.readlines()\n",
        "\n",
        "        temp_path_list = [path.strip() for path in temp_path_list]\n",
        "\n",
        "        path_list += temp_path_list\n",
        "\n",
        "    return path_list\n",
        "\n",
        "\n",
        "def cc_path_to_urls(cc_path):\n",
        "    url = base_url+cc_path\n",
        "    filename = cc_path.replace(\"/\", \"_\")\n",
        "    gz_path = f\"data/gz/{filename}\"\n",
        "    warc_path = f\"data/warc/{filename}\".replace(\".gz\", \"\")\n",
        "\n",
        "    return url, gz_path, warc_path\n",
        "\n",
        "\n",
        "def download_warc_file(path):\n",
        "    url, gz_path, warc_path = cc_path_to_urls(path)\n",
        "\n",
        "    if os.path.exists(warc_path):\n",
        "        print(f\"warc_pathにはファイルが存在しています\")\n",
        "        return warc_path\n",
        "    try:\n",
        "        if os.path.exists(gz_path):\n",
        "            print(f\"gz_pathがすでに存在します: {gz_path}\")\n",
        "        else:\n",
        "            print(\"downloading \"+url)\n",
        "            download_file(url, gz_path)\n",
        "        print(\"decompressing \"+gz_path)\n",
        "        decompress_gz(gz_path, warc_path,\n",
        "                      remove_gz=False, fill_blank_gz=True)\n",
        "        return warc_path\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(\"fail loading \"+url)\n",
        "        return warc_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcoYu9_JDDsq"
      },
      "source": [
        "### Function - Process Utils\n",
        "\n",
        "今回の処理に必要な関数(warcファイルの処理)をインストールします.\n",
        "\n",
        "※初回だけ実行してください"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dy6U0kGeDEEC"
      },
      "outputs": [],
      "source": [
        "from warcio.archiveiterator import ArchiveIterator\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "from trafilatura import fetch_url, extract\n",
        "\n",
        "def halfwidth_ratio(s):\n",
        "    if len(s) == 0:  # 空の文字列の場合は0を返す\n",
        "        return 0\n",
        "    halfwidth_count = sum(\n",
        "        1 for char in s\n",
        "        if '\\u0020' <= char <= '\\u007E' or  # 基本的なASCII範囲\n",
        "           '\\uFF61' <= char <= '\\uFF9F' or  # 半角カタカナ\n",
        "           char in ('\\u0009', '\\u000A', '\\u000D')  # タブ、改行、復帰\n",
        "    )\n",
        "    return halfwidth_count / len(s)\n",
        "\n",
        "\n",
        "def pre_clean(soup):\n",
        "    texts_with_tags = []\n",
        "    for tag in soup.find_all(True):\n",
        "        # 特定のタグを除外する場合\n",
        "        # if tag.name not in ['html', 'body', 'ul']:\n",
        "        text = tag.get_text(separator=\"\\n\", strip=True)\n",
        "        spl_text = text.split(\"\\n\")\n",
        "        spl_text = [i.strip() for i in spl_text if i.strip()]  # 空の文字列を除外\n",
        "        for item in spl_text:\n",
        "            if tag.name == \"script\" or tag.name == \"style\":\n",
        "                continue\n",
        "            texts_with_tags.append((item, tag.name))  # テキストとタグの名前をタプルとして追加\n",
        "    return texts_with_tags\n",
        "\n",
        "\n",
        "def extract_japanese_from_warc(path,\n",
        "                               save_dir=\"json\",\n",
        "                               max_num=10**10,\n",
        "                               ):\n",
        "    ja_soup_list = []\n",
        "    path = path.replace(\"\\\\\", \"/\")  # for windows env\n",
        "    filename = path.split(\"/\")[-1].replace(\".warc\", \".json\")\n",
        "    if os.path.exists(f\"{save_dir}/{filename}\"):\n",
        "        print(\"already done\")\n",
        "        return\n",
        "    # 途中から再開する用の位置情報の取得\n",
        "    if len(ja_soup_list) > 0:\n",
        "        fin_record_id = ja_soup_list[-1][\"record_id\"]\n",
        "    else:\n",
        "        fin_record_id = 0\n",
        "    # WARCファイルを開く\n",
        "    record_id = 0\n",
        "    with open(path, 'rb') as stream:\n",
        "        for record in tqdm(ArchiveIterator(stream)):\n",
        "            record_id += 1\n",
        "            if record_id <= fin_record_id:\n",
        "                continue\n",
        "            if record.rec_type == 'response':\n",
        "                if record.http_headers.get_header('Content-Type') == 'text/html':\n",
        "                    content = record.content_stream().read()\n",
        "                    soup = BeautifulSoup(content, 'html.parser')\n",
        "                    # <html>タグからlang属性を取得\n",
        "                    html_tag = soup.find('html')\n",
        "                    if html_tag and html_tag.has_attr('lang'):\n",
        "                        lang = html_tag['lang']\n",
        "                        texts = pre_clean(soup)\n",
        "\n",
        "                        if len(texts) == 0:\n",
        "                            continue\n",
        "                        if lang == \"ja\":\n",
        "                            if soup.title is not None:\n",
        "                                title = soup.title.string\n",
        "                            else:\n",
        "                                title = \"\"\n",
        "                            texts = extract(content, include_tables=False,target_lang='ja',favour_precision=True) #trafilaturaでテキスト抽出\n",
        "                            print(texts)\n",
        "                            d = {\n",
        "                                \"record_id\": record_id,\n",
        "                                \"url\": record.rec_headers.get_header('WARC-Target-URI'),\n",
        "                                \"title\": title,\n",
        "                                \"timestamp\": record.rec_headers.get_header('WARC-Date'),\n",
        "                                \"text\": texts,\n",
        "                            }\n",
        "                            ja_soup_list.append(d)\n",
        "                        if len(ja_soup_list) > max_num:\n",
        "                            break\n",
        "    return ja_soup_list\n",
        "\n",
        "def download_and_parse(cc_path, base_dir=None):\n",
        "    # warcファイルのダウンロード\n",
        "    warc_path = download_warc_file(cc_path)\n",
        "    # ファイル関連の処理\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "    # パス関連の処理\n",
        "    file_name = os.path.basename(warc_path)\n",
        "    base_name = os.path.splitext(file_name)[0]\n",
        "    file_base_name = \"_\".join(base_name.split(\"_\")[2:])\n",
        "    if base_dir is None:\n",
        "        base_dir = \"/tmp/\"\n",
        "    save_gz_path = f\"{base_dir}/{file_base_name}_japanese.json.gz\"\n",
        "    try:\n",
        "        tag_records = extract_japanese_from_warc(warc_path)\n",
        "        is_error = False\n",
        "        error_text = \"\"\n",
        "    except Exception as e:\n",
        "        tag_records = []\n",
        "        is_error = True\n",
        "        print(e)\n",
        "        error_text = str(e)\n",
        "    # 保存用のdictを作製\n",
        "    save_dict = {\n",
        "      \"tag_records\" : tag_records,\n",
        "      \"is_error\" : is_error,\n",
        "      \"cc_path\" : cc_path,\n",
        "      \"warc_path\" : warc_path,\n",
        "      \"error_text\" : error_text\n",
        "    }\n",
        "    with gzip.open(save_gz_path, 'wt', encoding=\"utf-8\") as zipfile:\n",
        "       json.dump(save_dict, zipfile, indent=2, ensure_ascii=False)\n",
        "    return\n",
        "\n",
        "def curation(batch_number, submit_dir=\"/content/submit\", is_debug=False):\n",
        "    cc_path_list = get_cc_path_list()\n",
        "    if is_debug:\n",
        "        n_batch = 3\n",
        "    else:\n",
        "        n_batch = 10\n",
        "    start_idx, end_idx = batch_number * n_batch, (batch_number+1) * n_batch\n",
        "    target_path_list  = cc_path_list[start_idx:end_idx]\n",
        "    for cc_path in tqdm(target_path_list):\n",
        "        download_and_parse(cc_path, f\"process/batch{batch_number}\")\n",
        "    shutil.make_archive(f'{submit_dir}/{batch_number}',\n",
        "                        format='zip', root_dir=f\"process/batch{batch_number}\")\n",
        "\n",
        "    shutil.rmtree(\"process/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1NfsB8D-Cp8"
      },
      "source": [
        "## Data Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wO9eBqd-JA8"
      },
      "source": [
        "### Step1 Download CommonCrawl Paths\n",
        "\n",
        "CommonCrawlにおけるアーカイブ(warc)が保存されているパスの文字列が\n",
        "\n",
        "圧縮されたファイル(gz)で保存されている.\n",
        "\n",
        "このファイルをダウンロードして, 解凍, data/data_list配下に保存する\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T59_l8OQ-CAz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "download path list from commoncrawl\n",
        "\"\"\"\n",
        "# Parameter\n",
        "# 今回処理するwarcのパスリストが圧縮されているURL\n",
        "# CC-MAIN-2023-50以外にも存在するが, 一旦このURLのみで行う\n",
        "path_urls = [\n",
        "    \"https://data.commoncrawl.org/crawl-data/CC-MAIN-2023-50/warc.paths.gz\",\n",
        "]\n",
        "# パスリストをダウンロードするフォルダの作成\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "os.makedirs(\"data/path_list\", exist_ok=True)\n",
        "\n",
        "# Process\n",
        "# Parameterで指定したURLからパス(gz)をダウンロードし,解凍する\n",
        "for url in path_urls:\n",
        "    file_name = url.split(\"/\")[-2]+\".gz\"\n",
        "    try:\n",
        "        # パスリストが格納されているgzファイルをdata_list配下に保存\n",
        "        download_file(url, f\"data/path_list/{file_name}\")\n",
        "        # 保存されたgzファイルを解凍する\n",
        "        decompress_gz(f\"data/path_list/{file_name}\",\n",
        "                      f\"data/path_list/{os.path.splitext(file_name)[0]}\")\n",
        "    except:\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wklkGS4h_QCu"
      },
      "source": [
        "### step2 Check Step1 Result\n",
        "\n",
        "warcのパスの文字列が保存されているかを確認する\n",
        "\n",
        "※step1で異常があった際に実行してください"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MlgowuXAIUS"
      },
      "outputs": [],
      "source": [
        "# Process\n",
        "# 保存されているwarcファイルのパスのリストを取得\n",
        "cc_paths = get_cc_path_list(path_dir=\"data/path_list/*\")\n",
        "# 表示\n",
        "cc_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv-KdQKGVez6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC9QykzqAjgE"
      },
      "source": [
        "### Step3 : Download Warc and Extract Japanese Site Data\n",
        "\n",
        "warc.gzファイルのURI(その一部)を元にwarc.gzファイルをダウンロード/解凍(->warc)する.\n",
        "\n",
        "warcファイルを読み込み, 内部に保存されている日本語サイトのデータのみを抽出する.\n",
        "\n",
        "抽出した結果をjson.gz形式で圧縮して保存.\n",
        "\n",
        "warcファイルは90000個ほど存在するため, この一部のみを処理していく必要あり,\n",
        "\n",
        "そのため, 処理するファイルのパスをいくつかのbatchに分けている.\n",
        "\n",
        "このbatchの番号(batch_number)を指定し, そのbatchにおける日本語ページを取得\n",
        "\n",
        "取得した結果をまとめたzipファイルがsubmit/{batch_number}.zipに保存される.\n",
        "\n",
        "保存されている内容を指定のGoogleDriveに配置ください\n",
        "\n",
        "---\n",
        "\n",
        "#### 行っていただく内容\n",
        "\n",
        "- is_debugをTrueにして動くかを確認 (初回のみ)\n",
        "\n",
        "- batch_numberを変更 (取りくむbatchをご指定ください)\n",
        "\n",
        "- is_debugをFalseにしてデータ抽出/加工スタート\n",
        "\n",
        "#### 2024/03/04 更新\n",
        "\n",
        "結果のzipファイルを自動的にダウンロードするように変更いたしました\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9B209H5SAcr"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "# 処理結果を自動的にダウンロードするように変更\n",
        "\n",
        "start=500\n",
        "end=500\n",
        "\n",
        "for batch_number in range(start, end+1):\n",
        "    is_debug = False\n",
        "\n",
        "    # 保存用ディレクトリの指定\n",
        "    # submit_dir = \"submit\"\n",
        "    # もしdriveがマウントできれば,上のsubmit_dirをコメントアウト, 以下をコードにしてください.\n",
        "    submit_dir = ROOT_PATH + \"submit\"\n",
        "\n",
        "    # batchの番号に従って,データの処理\n",
        "    curation(batch_number, submit_dir=submit_dir, is_debug=is_debug)\n",
        "\n",
        "    # ファイルのダウンロード\n",
        "    # files.download(f\"./submit/{batch_number}.zip\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBJhwCeyL5Lb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
