{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RefinedWebの処理MDRの実装をしたfineweb\n",
    "* URL filter：\n",
    "\n",
    "* repetition：\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple\n",
    "\n",
    "from datatrove.data import Document, DocumentsPipeline\n",
    "from datatrove.pipeline.base import PipelineStep\n",
    "from datatrove.pipeline.writers.disk_base import DiskWriter\n",
    "from datatrove.utils.typeshelper import StatHints\n",
    "\n",
    "\n",
    "def get_filter_result(res):\n",
    "    result, reason = res, None\n",
    "    if isinstance(result, tuple):\n",
    "        result, reason = res\n",
    "    return result, reason\n",
    "\n",
    "\n",
    "class BaseFilter(PipelineStep, ABC):\n",
    "    \"\"\"Base module for Filters. Filters remove documents.\n",
    "\n",
    "    Args:\n",
    "        exclusion_writer: optionally pass in a writer that will save the dropped documents\n",
    "    \"\"\"\n",
    "\n",
    "    type = \"🔻 - FILTER\"\n",
    "\n",
    "    def __init__(self, exclusion_writer: DiskWriter = None):\n",
    "        super().__init__()\n",
    "        self.exclusion_writer = exclusion_writer\n",
    "\n",
    "    @abstractmethod\n",
    "    def filter(self, doc: Document) -> bool | Tuple[bool, str]:\n",
    "        \"\"\"Filter modules main method.\n",
    "        Returns true if a sample should be KEPT, false if it should be REMOVED.\n",
    "\n",
    "        Args:\n",
    "            doc: sample to filter\n",
    "\n",
    "        Returns:\n",
    "            bool - whether the doc should be kept\n",
    "            or (False, str), to drop with a specific reason\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run(self, data: DocumentsPipeline, rank: int = 0, world_size: int = 1) -> DocumentsPipeline:\n",
    "        with self.exclusion_writer if self.exclusion_writer else contextlib.nullcontext() as writer:\n",
    "            for doc in data:\n",
    "                self.stat_update(StatHints.total)\n",
    "                with self.track_time():\n",
    "                    filter_result, reason = get_filter_result(self.filter(doc))\n",
    "                    if filter_result:\n",
    "                        self.stat_update(StatHints.forwarded)\n",
    "                        self.update_doc_stats(doc)\n",
    "                    else:\n",
    "                        self.stat_update(StatHints.dropped)\n",
    "                        if reason:\n",
    "                            self.stat_update(f\"dropped_{reason}\")\n",
    "                        if self.exclusion_writer:\n",
    "                            if reason:\n",
    "                                doc.metadata[\"filter_reason\"] = reason\n",
    "                            writer.write(doc, rank)\n",
    "                        continue\n",
    "                yield doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quality filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datatrove.pipeline.filters.base_filter import BaseFilter\n",
    "\n",
    "class FWQualityFilter(BaseFilter):\n",
    "    name = \"fineweb quality filter\"\n",
    "    _requires_dependencies = [\"nltk\"]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            exclusion_writer,\n",
    "            line_punct_thr: float = 0.12,\n",
    "            line_punct_exclude_zero=False,\n",
    "            short_line_thr: float = 0.67,\n",
    "            short_line_length: int = 30,\n",
    "            char_duplicates_ratio: float = 0.01\n",
    "        ):\n",
    "        super().__init__(exclusion_writer)\n",
    "        self.line_punct_thr = line_punct_thr\n",
    "        self.line_punct_exclude_zero = line_punct_exclude_zero\n",
    "        self.short_line_threshold = short_line_thr\n",
    "        self.short_line_length = short_line_length\n",
    "        self.char_duplicates_ratio = char_duplicates_ratio\n",
    "\n",
    "    def filter(self, doc) -> bool | tuple[bool, str]:\n",
    "        from datatrove.pipeline.filters.gopher_repetition_filter import find_duplicates\n",
    "\n",
    "        def remove_empty_lines(lines: list[str]):\n",
    "            return [l for l in lines if l.strip() != \"\"]\n",
    "\n",
    "        stop_chars = (\".\", \"'\", '\"', \"!\", \"?\")\n",
    "\n",
    "        lines = doc.text.split(\"\\n\")\n",
    "        ratio = sum(1 for line in lines if line.endswith(stop_chars)) / len(lines)\n",
    "        if ratio <= self.line_punct_thr and not (ratio == 0 and self.line_punct_exclude_zero):\n",
    "            return False, \"line_punct_ratio\"\n",
    "\n",
    "        ratio = sum(1 for line in lines if len(line) <= self.short_line_length) / len(\n",
    "            lines\n",
    "        )\n",
    "        if ratio >= self.short_line_threshold:\n",
    "            return False, \"short_line_ratio\"\n",
    "\n",
    "        ratio = find_duplicates(remove_empty_lines(lines))[1] / len(doc.text.replace(\"\\n\", \"\"))\n",
    "\n",
    "        if ratio >= self.char_duplicates_ratio:\n",
    "            return False, \"char_dup_ratio\"\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datatrove.data import Document\n",
    "from datatrove.pipeline.filters.base_filter import BaseFilter\n",
    "from datatrove.pipeline.writers.disk_base import DiskWriter\n",
    "from datatrove.utils.text import PUNCTUATION_SET\n",
    "\n",
    "\n",
    "STOP_WORDS = [\"the\", \"be\", \"to\", \"of\", \"and\", \"that\", \"have\", \"with\"]\n",
    "\n",
    "\n",
    "class GopherQualityFilter(BaseFilter):\n",
    "    name = \"🥇 Gopher Quality\"\n",
    "    _requires_dependencies = [\"nltk\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_doc_words: int | None = 50,\n",
    "        max_doc_words: int | None = 100000,\n",
    "        min_avg_word_length: int | None = 3,\n",
    "        max_avg_word_length: int | None = 10,\n",
    "        max_symbol_word_ratio: float | None = 0.1,\n",
    "        max_bullet_lines_ratio: float | None = 0.9,\n",
    "        max_ellipsis_lines_ratio: float | None = 0.3,\n",
    "        max_non_alpha_words_ratio: float | None = 0.8,\n",
    "        min_stop_words: int | None = 2,\n",
    "        stop_words: list[str] | None = None,\n",
    "        exclusion_writer: DiskWriter = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Filter to apply Gopher's quality heuristic rules.\n",
    "        Reference: https://arxiv.org/pdf/2112.11446.pdf\n",
    "\n",
    "        Args:\n",
    "            min_doc_words:\n",
    "            max_doc_words:\n",
    "            min_avg_word_length:\n",
    "            max_avg_word_length:\n",
    "            max_symbol_word_ratio:\n",
    "            max_bullet_lines_ratio:\n",
    "            max_ellipsis_lines_ratio:\n",
    "            max_non_alpha_words_ratio:\n",
    "            min_stop_words:\n",
    "            stop_words:\n",
    "            exclusion_writer:\n",
    "        \"\"\"\n",
    "        super().__init__(exclusion_writer)\n",
    "        self.min_doc_words = min_doc_words\n",
    "        self.max_doc_words = max_doc_words\n",
    "        self.min_avg_word_length = min_avg_word_length\n",
    "        self.max_avg_word_length = max_avg_word_length\n",
    "        self.max_symbol_word_ratio = max_symbol_word_ratio\n",
    "        self.max_bullet_lines_ratio = max_bullet_lines_ratio\n",
    "        self.max_ellipsis_lines_ratio = max_ellipsis_lines_ratio\n",
    "        self.max_non_alpha_words_ratio = max_non_alpha_words_ratio\n",
    "        self.min_stop_words = min_stop_words\n",
    "        self.stop_words = set(STOP_WORDS if stop_words is None else stop_words)\n",
    "\n",
    "    def filter(self, doc: Document) -> bool | tuple[bool, str]:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            doc: Applies the heuristics rules to decide if a document should be REMOVED\n",
    "\n",
    "\n",
    "        Returns: False if sample.text does not pass any of the the heuristic tests\n",
    "\n",
    "        \"\"\"\n",
    "        from nltk.tokenize import word_tokenize\n",
    "\n",
    "        text = doc.text\n",
    "        words = word_tokenize(text)  # TODO we should use language id filter\n",
    "        n_words = len(words)\n",
    "\n",
    "        non_symbol_words = [w for w in words if any(ch not in PUNCTUATION_SET for ch in w)]\n",
    "        n_non_symbol_words_words = len(non_symbol_words)\n",
    "\n",
    "        # words < min_doc_words or words > max_doc_words\n",
    "        if self.min_doc_words and n_non_symbol_words_words < self.min_doc_words:\n",
    "            return False, \"gopher_short_doc\"\n",
    "        if self.max_doc_words and n_non_symbol_words_words > self.max_doc_words:\n",
    "            return False, \"gopher_long_doc\"\n",
    "\n",
    "        # mean word length is outside the range of 3 to 10 characters\n",
    "        avg_n_words = np.mean([len(w) for w in non_symbol_words])\n",
    "        if self.min_avg_word_length and avg_n_words < self.min_avg_word_length:\n",
    "            return False, \"gopher_below_avg_threshold\"\n",
    "        if self.max_avg_word_length and avg_n_words > self.max_avg_word_length:\n",
    "            return False, \"gopher_above_avg_threshold\"\n",
    "\n",
    "        # symbol-to-word ratio greater than 0.1 for either the hash symbol or the ellipsis\n",
    "        if self.max_symbol_word_ratio and text.count(\"#\") / n_words > self.max_symbol_word_ratio:\n",
    "            return False, \"gopher_too_many_hashes\"\n",
    "        if self.max_symbol_word_ratio and (text.count(\"...\") + text.count(\"…\")) / n_words > self.max_symbol_word_ratio:\n",
    "            return False, \"gopher_too_many_ellipsis\"\n",
    "\n",
    "        # any document with more than 90 % of lines starting with a bullet point,\n",
    "        # or more than 30 % ending with an ellipsis.\n",
    "        lines = text.splitlines()\n",
    "        if (\n",
    "            self.max_bullet_lines_ratio\n",
    "            and sum(s.lstrip().startswith(\"•\") or s.lstrip().startswith(\"-\") for s in lines) / len(lines)\n",
    "            > self.max_bullet_lines_ratio\n",
    "        ):\n",
    "            return False, \"gopher_too_many_bullets\"\n",
    "        if (\n",
    "            self.max_ellipsis_lines_ratio\n",
    "            and sum(s.rstrip().endswith(\"...\") or s.rstrip().endswith(\"…\") for s in lines) / len(lines)\n",
    "            > self.max_ellipsis_lines_ratio\n",
    "        ):\n",
    "            return False, \"gopher_too_many_end_ellipsis\"\n",
    "\n",
    "        # that 80 % of words in a document contain at least one alphabetic character\n",
    "        if (\n",
    "            self.max_non_alpha_words_ratio\n",
    "            and sum([any((c.isalpha() for c in w)) for w in words]) / n_words < self.max_non_alpha_words_ratio\n",
    "        ):\n",
    "            return False, \"gopher_below_alpha_threshold\"\n",
    "\n",
    "        # stop word filter\n",
    "        if self.min_stop_words and sum(w in self.stop_words for w in words) < self.min_stop_words:\n",
    "            return False, \"gopher_enough_stop_words\"\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from datatrove.utils.text import PUNCTUATION_SET\n",
    "\n",
    "PUNCTUATION = \"!/—”:％１〈&(、━\\\\【#%「」，】；+^]~“《„';’{|∶´[=-`*．（–？！：$～«〉,><》)?）。…@_.\\\"}►»\" + \"\".join(\n",
    "    map(\n",
    "        chr,\n",
    "        (x for a, b in ((0, 9), (11, 13), (13, 32), (127, 160)) for x in range(a, b)),\n",
    "    )\n",
    ")\n",
    "PUNCTUATION_SET = set(PUNCTUATION)\n",
    "\n",
    "\n",
    "\n",
    "STOP_WORDS = [\"the\", \"be\", \"to\", \"of\", \"and\", \"that\", \"have\", \"with\"]\n",
    "\n",
    "\n",
    "class GopherQualityFilter():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_doc_words: int | None = 50,\n",
    "        max_doc_words: int | None = 100000,\n",
    "        min_avg_word_length: int | None = 3,\n",
    "        max_avg_word_length: int | None = 10,\n",
    "        max_symbol_word_ratio: float | None = 0.1,\n",
    "        max_bullet_lines_ratio: float | None = 0.9,\n",
    "        max_ellipsis_lines_ratio: float | None = 0.3,\n",
    "        max_non_alpha_words_ratio: float | None = 0.8,\n",
    "        min_stop_words: int | None = 2,\n",
    "        stop_words: list[str] | None = None,\n",
    "        #exclusion_writer: DiskWriter = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Filter to apply Gopher's quality heuristic rules.\n",
    "        Reference: https://arxiv.org/pdf/2112.11446.pdf\n",
    "\n",
    "        Args:\n",
    "            min_doc_words:\n",
    "            max_doc_words:\n",
    "            min_avg_word_length:\n",
    "            max_avg_word_length:\n",
    "            max_symbol_word_ratio:\n",
    "            max_bullet_lines_ratio:\n",
    "            max_ellipsis_lines_ratio:\n",
    "            max_non_alpha_words_ratio:\n",
    "            min_stop_words:\n",
    "            stop_words:\n",
    "            exclusion_writer:\n",
    "        \"\"\"\n",
    "        self.min_doc_words = min_doc_words\n",
    "        self.max_doc_words = max_doc_words\n",
    "        self.min_avg_word_length = min_avg_word_length\n",
    "        self.max_avg_word_length = max_avg_word_length\n",
    "        self.max_symbol_word_ratio = max_symbol_word_ratio\n",
    "        self.max_bullet_lines_ratio = max_bullet_lines_ratio\n",
    "        self.max_ellipsis_lines_ratio = max_ellipsis_lines_ratio\n",
    "        self.max_non_alpha_words_ratio = max_non_alpha_words_ratio\n",
    "        self.min_stop_words = min_stop_words\n",
    "        self.stop_words = set(STOP_WORDS if stop_words is None else stop_words)\n",
    "\n",
    "    def filter(self, text: str) -> bool | tuple[bool, str]:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            doc: Applies the heuristics rules to decide if a document should be REMOVED\n",
    "\n",
    "\n",
    "        Returns: False if sample.text does not pass any of the the heuristic tests\n",
    "\n",
    "        \"\"\"\n",
    "        from nltk.tokenize import word_tokenize\n",
    "\n",
    "        words = word_tokenize(text)  # TODO we should use language id filter\n",
    "        n_words = len(words)\n",
    "        non_symbol_words = [w for w in words if any(ch not in PUNCTUATION_SET for ch in w)]\n",
    "        n_non_symbol_words_words = len(non_symbol_words)\n",
    "\n",
    "        # words < min_doc_words or words > max_doc_words\n",
    "        if self.min_doc_words and n_non_symbol_words_words < self.min_doc_words:\n",
    "            return \"\",False, \"gopher_short_doc\"\n",
    "        if self.max_doc_words and n_non_symbol_words_words > self.max_doc_words:\n",
    "            return \"\",False, \"gopher_long_doc\"\n",
    "\n",
    "        # mean word length is outside the range of 3 to 10 characters\n",
    "        avg_n_words = np.mean([len(w) for w in non_symbol_words])\n",
    "        if self.min_avg_word_length and avg_n_words < self.min_avg_word_length:\n",
    "            return \"\",False, \"gopher_below_avg_threshold\"\n",
    "        if self.max_avg_word_length and avg_n_words > self.max_avg_word_length:\n",
    "            return \"\",False, \"gopher_above_avg_threshold\"\n",
    "\n",
    "        # symbol-to-word ratio greater than 0.1 for either the hash symbol or the ellipsis\n",
    "        if self.max_symbol_word_ratio and text.count(\"#\") / n_words > self.max_symbol_word_ratio:\n",
    "            return \"\",False, \"gopher_too_many_hashes\"\n",
    "        if self.max_symbol_word_ratio and (text.count(\"...\") + text.count(\"…\")) / n_words > self.max_symbol_word_ratio:\n",
    "            return \"\",False, \"gopher_too_many_ellipsis\"\n",
    "\n",
    "        # any document with more than 90 % of lines starting with a bullet point,\n",
    "        # or more than 30 % ending with an ellipsis.\n",
    "        lines = text.splitlines()\n",
    "        if (\n",
    "            self.max_bullet_lines_ratio\n",
    "            and sum(s.lstrip().startswith(\"•\") or s.lstrip().startswith(\"-\") for s in lines) / len(lines)\n",
    "            > self.max_bullet_lines_ratio\n",
    "        ):\n",
    "            return \"\",False, \"gopher_too_many_bullets\"\n",
    "        if (\n",
    "            self.max_ellipsis_lines_ratio\n",
    "            and sum(s.rstrip().endswith(\"...\") or s.rstrip().endswith(\"…\") for s in lines) / len(lines)\n",
    "            > self.max_ellipsis_lines_ratio\n",
    "        ):\n",
    "            return \"\",False, \"gopher_too_many_end_ellipsis\"\n",
    "\n",
    "        # that 80 % of words in a document contain at least one alphabetic character\n",
    "        if (\n",
    "            self.max_non_alpha_words_ratio\n",
    "            and sum([any((c.isalpha() for c in w)) for w in words]) / n_words < self.max_non_alpha_words_ratio\n",
    "        ):\n",
    "            return \"\" ,False, \"gopher_below_alpha_threshold\"\n",
    "\n",
    "        # stop word filter\n",
    "        if self.min_stop_words and sum(w in self.stop_words for w in words) < self.min_stop_words:\n",
    "            return \"\",False, \"gopher_enough_stop_words\"\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c4\n",
    "import heapq\n",
    "import re\n",
    "\n",
    "from datatrove.data import Document\n",
    "from datatrove.pipeline.filters.base_filter import BaseFilter\n",
    "from datatrove.pipeline.writers.disk_base import DiskWriter\n",
    "\n",
    "\n",
    "CITATION_REGEX = re.compile(r\"\\[\\d*]|\\[edit]|\\[citation needed]\")\n",
    "END_PUNCTUATION = (\".\", \"?\", \"!\", '\"', \"'\")\n",
    "ELLIPSIS = \"...\"\n",
    "POLICY_SUBSTRINGS = [\n",
    "    \"terms of use\",\n",
    "    \"privacy policy\",\n",
    "    \"cookie policy\",\n",
    "    \"uses cookies\",\n",
    "    \"use of cookies\",\n",
    "    \"use cookies\",\n",
    "]\n",
    "\n",
    "\n",
    "class C4QualityFilter(BaseFilter):\n",
    "    \"\"\"Applies heuristic rules from C4 https://jmlr.org/papers/volume21/20-074/20-074.pdf\n",
    "\n",
    "    - We only retained lines that ended in a terminal punctuation mark (! . \" ?)\n",
    "    - We discarded any page with fewer than 5 sentences and only retained lines that contained at least 3 words\n",
    "    - [NOT IMPLEMENTED] We removed any page that contained any word on the “List of Dirty, Naughty, Obscene or Otherwise Bad Words”\n",
    "    - We removed any line with the word Javascript.\n",
    "    - We removed any page where the phrase “lorem ipsum” appeared\n",
    "    - We removed any pages that contained a curly bracket\n",
    "    Additional filters not mentioned on the list from the paper but on the code:\n",
    "    - Remove lines with one word over 1000 chars\n",
    "    - Remove lines with cookies and terms of use keywords\n",
    "\n",
    "    Reference implementation: https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/text/c4_utils.py#L197\n",
    "    Args:\n",
    "        exclusion_writer: optionally pass in a writer that will save the dropped documents\n",
    "        tokenizer_language: load a diff language specific punkt tokenizer from nltk\n",
    "        split_paragraph: by default (as in the paper) split on \"\\n\".\n",
    "            Set to \"False\" to apply the filters to each sentence instead of to each line\n",
    "        remove_citations: remove wikipedia style citations from the text\n",
    "        filter_no_terminal_punct: remove lines without terminal punctuation marks\n",
    "        min_num_sentences: remove documents that do not have at least this number of sentences (after line filtering).\n",
    "            set to -1 to disable\n",
    "        min_words_per_line: drop lines without this min number of words\n",
    "        max_word_length: drop lines where at least one word has more than this number of characters\n",
    "        filter_lorem_ipsum: drop documents that contain \"lorem ipsum\"\n",
    "        filter_javascript: drop lines mentioning \"javascript\"\n",
    "        filter_curly_bracket: drop documents containing {\n",
    "        filter_policy: drop lines containing any of the phrases in POLICY_SUBSTRINGS\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"⛰ C4 Quality\"\n",
    "    _requires_dependencies = [\"nltk\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        exclusion_writer: DiskWriter = None,\n",
    "        tokenizer_language: str = \"english\",\n",
    "        split_paragraph: bool = True,  # default as used on c4. Set to \"False\" to split with sent_tokenize\n",
    "        remove_citations: bool = True,\n",
    "        filter_no_terminal_punct: bool = True,\n",
    "        min_num_sentences: int = 5,  # set to -1 to disable\n",
    "        min_words_per_line: int = 3,  # set to -1 to disable\n",
    "        max_word_length: int = 1000,  # set to -1 to disable\n",
    "        filter_lorem_ipsum: bool = True,\n",
    "        filter_javascript: bool = True,\n",
    "        filter_curly_bracket: bool = True,\n",
    "        filter_policy: bool = True,\n",
    "    ):\n",
    "        super().__init__(exclusion_writer)\n",
    "        self.tokenizer_language = tokenizer_language\n",
    "        self.split_paragraph = split_paragraph\n",
    "        self.remove_citations = remove_citations\n",
    "        self.filter_no_terminal_punct = filter_no_terminal_punct\n",
    "        self.min_num_sentences = min_num_sentences\n",
    "        self.min_words_per_line = min_words_per_line\n",
    "        self.max_word_length = max_word_length\n",
    "        self.filter_lorem_ipsum = filter_lorem_ipsum\n",
    "        self.filter_javascript = filter_javascript\n",
    "        self.filter_curly_bracket = filter_curly_bracket\n",
    "        self.filter_policy = filter_policy\n",
    "\n",
    "    def filter(self, doc: Document) -> bool | tuple[bool, str]:\n",
    "        from nltk.tokenize import sent_tokenize\n",
    "\n",
    "        lines = (\n",
    "            doc.text.splitlines()\n",
    "            if self.split_paragraph\n",
    "            else sent_tokenize(doc.text, language=self.tokenizer_language)\n",
    "        )\n",
    "\n",
    "        num_sentences = 0\n",
    "        kept_lines = []\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            words = line.split()\n",
    "            self.stat_update(\"line-total\")\n",
    "            # check line has too long word\n",
    "            if self.max_word_length != -1 and any(len(word) > self.max_word_length for word in words):\n",
    "                self.stat_update(\"line-filter-too_long_word\")\n",
    "                continue\n",
    "            # remove citation\n",
    "            if self.remove_citations:\n",
    "                line = CITATION_REGEX.sub(\"\", line)\n",
    "            # end punctuation\n",
    "            if self.filter_no_terminal_punct and (not line.endswith(END_PUNCTUATION) or line.endswith(ELLIPSIS)):\n",
    "                self.stat_update(\"line-filter-no_terminal_punc\")\n",
    "                continue\n",
    "            # min words per line\n",
    "            if len(words) < self.min_words_per_line:\n",
    "                self.stat_update(\"line-filter-too_few_words\")\n",
    "                continue\n",
    "            line_l = line.lower()\n",
    "            # lorem ipsum\n",
    "            if self.filter_lorem_ipsum and \"lorem ipsum\" in line_l:\n",
    "                return False, \"lorem_ipsum\"  # drop entire doc\n",
    "            # javascript\n",
    "            if self.filter_javascript and \"javascript\" in line_l:\n",
    "                self.stat_update(\"line-filter-javascript\")\n",
    "                continue\n",
    "            # bracket\n",
    "            if self.filter_curly_bracket and \"{\" in line:\n",
    "                return False, \"curly_bracket\"  # drop entire doc\n",
    "            # policy\n",
    "            if self.filter_policy and any(p in line_l for p in POLICY_SUBSTRINGS):\n",
    "                self.stat_update(\"line-filter-policy\")\n",
    "                continue\n",
    "            num_sentences += len(sent_tokenize(line, language=self.tokenizer_language)) if self.split_paragraph else 1\n",
    "            kept_lines.append(line)\n",
    "            self.stat_update(\"line-kept\")\n",
    "        if num_sentences < self.min_num_sentences:\n",
    "            return False, \"too_few_sentences\"\n",
    "\n",
    "        doc.text = (\"\\n\" if self.split_paragraph else \" \").join(kept_lines).strip()\n",
    "        return True\n",
    "\n",
    "\n",
    "class C4ParagraphFilter(BaseFilter):\n",
    "    \"\"\"Applies paragraph filtering from mC4\n",
    "\n",
    "    https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/text/c4_utils.py#L551\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"⛰ C4 Paragraph\"\n",
    "\n",
    "    def __init__(self, exclusion_writer: DiskWriter = None):\n",
    "        super().__init__(exclusion_writer)\n",
    "\n",
    "        self.min_paragraphs = 3\n",
    "        self.min_paragraph_len = 200\n",
    "        self.line_delimiter = \"\\n\"\n",
    "\n",
    "    def paragraph_filter(self, page):\n",
    "        \"\"\"Returns False iff a page has too few or too short paragraphs.\"\"\"\n",
    "        lines = page.split(self.line_delimiter)\n",
    "        # Filter out docs that don't have at least three \"paragraphs\"\n",
    "        # (lines >= `min_paragraph_len` chars).\n",
    "        if (\n",
    "            len(lines) < self.min_paragraphs\n",
    "            or min(heapq.nlargest(3, [len(line) for line in lines])) < self.min_paragraph_len\n",
    "        ):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def filter(self, doc: Document) -> bool | tuple[bool, str]:\n",
    "        if not self.paragraph_filter(doc.text):\n",
    "            return False, f\"< {self.min_paragraphs} paragraphs\"\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# repetition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repetition \n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from datatrove.data import Document\n",
    "from datatrove.pipeline.filters.base_filter import BaseFilter\n",
    "from datatrove.pipeline.writers.disk_base import DiskWriter\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Table A1 from https://arxiv.org/pdf/2112.11446.pd\n",
    "    duplicate line fraction                 0.30\n",
    "    duplicate paragraph fraction            0.30\n",
    "    duplicate line character fraction       0.20\n",
    "    duplicate paragraph character fraction  0.20\n",
    "\n",
    "    top 2-gram character fraction           0.20\n",
    "    top 3-gram character fraction           0.18\n",
    "    top 4-gram character fraction           0.16\n",
    "\n",
    "    duplicate 5-gram character fraction     0.15\n",
    "    duplicate 6-gram character fraction     0.14\n",
    "    duplicate 7-gram character fraction     0.13\n",
    "    duplicate 8-gram character fraction     0.12\n",
    "    duplicate 9-gram character fraction     0.11\n",
    "    duplicate 10-gram character fraction    0.10\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_n_grams(words: list[str], n: int) -> list[str]:\n",
    "    return [\" \".join(words[i : i + n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "\n",
    "def find_duplicates(x: list[str]) -> tuple[int, int]:\n",
    "    unique_x = set()\n",
    "    duplicate_chars = 0\n",
    "    duplicate_elements = 0\n",
    "    for element in x:\n",
    "        if element in unique_x:\n",
    "            duplicate_chars += len(element)\n",
    "            duplicate_elements += 1\n",
    "\n",
    "        else:\n",
    "            unique_x.add(element)\n",
    "    return duplicate_elements, duplicate_chars\n",
    "\n",
    "def find_top_duplicate(x: list[str]) -> int:\n",
    "    counter = Counter()\n",
    "    for element in x:\n",
    "        counter[element] += 1\n",
    "    top_n_gram = counter.most_common(1)[0]\n",
    "    return len(top_n_gram[0]) * top_n_gram[1]\n",
    "\n",
    "\n",
    "def find_all_duplicate(words: list[str], n: int) -> int:\n",
    "    n_words = len(words)\n",
    "    unique = set()\n",
    "    repeated_chars, idx = 0, 0\n",
    "    while idx < n_words - n + 1:\n",
    "        n_gram = \"\".join(words[idx : idx + n])\n",
    "        if n_gram in unique:\n",
    "            repeated_chars += len(n_gram)\n",
    "            idx += n\n",
    "        else:\n",
    "            unique.add(n_gram)\n",
    "            idx += 1\n",
    "    assert repeated_chars <= len(\"\".join(words))\n",
    "    return repeated_chars\n",
    "\n",
    "\n",
    "class GopherRepetitionFilter(BaseFilter):\n",
    "    name = \"👯 Gopher Repetition\"\n",
    "    _requires_dependencies = [\"nltk\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dup_line_frac: float | None = 0.3,\n",
    "        dup_para_frac: float | None = 0.3,\n",
    "        dup_line_char_frac: float | None = 0.2,\n",
    "        dup_para_char_frac: float | None = 0.2,\n",
    "        top_n_grams: tuple[tuple[int, float]] = ((2, 0.2), (3, 0.18), (4, 0.16)),\n",
    "        dup_n_grams: tuple[tuple[int, float]] = ((5, 0.15), (6, 0.14), (7, 0.13), (8, 0.12), (9, 0.11), (10, 0.10)),\n",
    "        exclusion_writer: DiskWriter = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            dup_line_frac:\n",
    "            dup_para_frac:\n",
    "            dup_line_char_frac:\n",
    "            dup_para_char_frac:\n",
    "            top_n_grams:\n",
    "            dup_n_grams:\n",
    "            exclusion_writer:\n",
    "        \"\"\"\n",
    "        super().__init__(exclusion_writer)\n",
    "\n",
    "        self.dup_line_frac = dup_line_frac\n",
    "        self.dup_para_frac = dup_para_frac\n",
    "        self.dup_line_char_frac = dup_line_char_frac\n",
    "        self.dup_para_char_frac = dup_para_char_frac\n",
    "        self.top_n_grams = top_n_grams\n",
    "        self.dup_n_grams = dup_n_grams\n",
    "        self.paragraph_exp = re.compile(r\"\\n{2,}\")\n",
    "\n",
    "    def filter(self, doc: Document) -> bool | tuple[bool, str]:\n",
    "        from nltk.tokenize import word_tokenize\n",
    "\n",
    "        text = doc.text\n",
    "\n",
    "        paragraphs = self.paragraph_exp.split(text.strip())\n",
    "        paragraphs_duplicates, char_duplicates = find_duplicates(paragraphs)\n",
    "        if self.dup_para_frac and paragraphs_duplicates / len(paragraphs) > self.dup_para_frac:\n",
    "            return False, \"dup_para_frac\"\n",
    "        if self.dup_para_char_frac and char_duplicates / len(text) > self.dup_para_char_frac:\n",
    "            return False, \"dup_para_char_frac\"\n",
    "\n",
    "        lines = text.splitlines()\n",
    "        line_duplicates, char_duplicates = find_duplicates(lines)\n",
    "        if self.dup_line_frac and line_duplicates / len(lines) > self.dup_line_frac:\n",
    "            return False, \"dup_line_frac\"\n",
    "        if self.dup_line_char_frac and char_duplicates / len(text) > self.dup_line_char_frac:\n",
    "            return False, \"dup_line_char_frac\"\n",
    "\n",
    "        words = word_tokenize(text, language=\"english\")  # TODO we should use language id filter\n",
    "\n",
    "        for n, n_frac in self.top_n_grams:\n",
    "            n_grams = get_n_grams(words, n)\n",
    "            if not n_grams:\n",
    "                continue\n",
    "            top_char_length = find_top_duplicate(n_grams)\n",
    "            if top_char_length / len(text) > n_frac:\n",
    "                return False, f\"top_{n}_gram\"\n",
    "\n",
    "        for n, n_frac in self.dup_n_grams:\n",
    "            n_duplicates_char = find_all_duplicate(words, n)\n",
    "            if n_duplicates_char / len(text) > n_frac:\n",
    "                return False, f\"duplicated_{n}_n_grams\"\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datatrove.data import Document\n",
    "from datatrove.io import cached_asset_path_or_download\n",
    "from datatrove.pipeline.filters.base_filter import BaseFilter\n",
    "from datatrove.pipeline.writers.disk_base import DiskWriter\n",
    "from datatrove.utils.text import SPLIT_TEXT_DOCUMENTS, split_into_parts\n",
    "\n",
    "\n",
    "class FastTextClassifierFilter(BaseFilter):\n",
    "    \"\"\"\n",
    "    Only keeps documents that have\n",
    "    - AT LEAST ONE of the labels in `keep_labels` with a score above the configured threshold, or\n",
    "    - NONE of the labels in `remove_labels` with a score above the configured threshold.\n",
    "\n",
    "    You can only supply one of these, to avoid conflicts. Use multiple filters if you need to. If you supply\n",
    "    neither, the block will simply annotate each document with the labels (set `save_labels_in_metadata=True`)\n",
    "\n",
    "    Example:\n",
    "        for `keep_labels=[(\"math\", 0.9)]` will only keep samples with a score on __label__math of at least 0.9\n",
    "        for `remove_labels=[(\"math\", 0.9)]` will remove samples with a score on __label__math of at least 0.9\n",
    "\n",
    "    Info to train your own classifier: https://fasttext.cc/docs/en/supervised-tutorial.html\n",
    "\n",
    "    Args:\n",
    "        model_url: url to download the model from or local path\n",
    "        keep_labels: tuple of (label name without \"__label__\", min score) (or list of such tuples)\n",
    "        remove_labels: tuple of (label name without \"__label__\", min score) (or list of such tuples)\n",
    "        save_labels_in_metadata: whether to save all the label scores in the document metadata\n",
    "        newline_replacement: str to replace \\n with before predicting scores\n",
    "        filter_mode: predict and filter on DOCUMENT, PARAGRAPH or SENTENCE level\n",
    "        exclusion_writer:\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"🤖 fastText\"\n",
    "    _requires_dependencies = [(\"fasttext\", \"fasttext-wheel\"), \"fasteners\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_url: str,\n",
    "        keep_labels: Tuple[str, float] | list[Tuple[str, float]] | None = None,\n",
    "        remove_labels: Tuple[str, float] | list[Tuple[str, float]] | None = None,\n",
    "        save_labels_in_metadata: bool = True,\n",
    "        exclusion_writer: DiskWriter | None = None,\n",
    "        newline_replacement=\"\",\n",
    "        filter_mode: str = SPLIT_TEXT_DOCUMENTS,\n",
    "    ):\n",
    "        super().__init__(exclusion_writer)\n",
    "        self.model_url = model_url\n",
    "        self.keep_labels = keep_labels\n",
    "        self.remove_labels = remove_labels\n",
    "        self.filter_mode = filter_mode\n",
    "        if keep_labels and remove_labels:\n",
    "            raise ValueError(\"You can only supply one of `keep_labels` or `remove_labels`.\")\n",
    "        self.newline_replacement = newline_replacement\n",
    "        if keep_labels and isinstance(keep_labels[0], str):\n",
    "            self.keep_labels = [keep_labels]\n",
    "        if remove_labels and isinstance(remove_labels[0], str):\n",
    "            self.remove_labels = [remove_labels]\n",
    "        self.save_labels_in_metadata = save_labels_in_metadata\n",
    "        self._model = None\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        if not self._model:\n",
    "            from fasttext.FastText import _FastText\n",
    "\n",
    "            model_file = cached_asset_path_or_download(\n",
    "                self.model_url, namespace=\"filters\", subfolder=\"fasttext\", desc=\"fast-text model\"\n",
    "            )\n",
    "            self._model = _FastText(model_file)\n",
    "            # check label values\n",
    "            available_labels = [x.removeprefix(\"__label__\") for x in self._model.labels]\n",
    "            for label, _ in self.keep_labels or [] + self.remove_labels or []:\n",
    "                if label not in available_labels:\n",
    "                    raise ValueError(\n",
    "                        f\"Label '{label}' passed as keep_labels or remove_labels is not available in this \"\n",
    "                        f\"FastText model. Available labels: {available_labels}\"\n",
    "                    )\n",
    "        return self._model\n",
    "\n",
    "    def filter(self, doc: Document) -> bool:\n",
    "        def check_label_scores(unit_scores):\n",
    "            if self.keep_labels:\n",
    "                return any(\n",
    "                    unit_scores.get(f\"__label__{label}\", -9e9) >= min_score for label, min_score in self.keep_labels\n",
    "                )\n",
    "            else:\n",
    "                return not self.remove_labels or not any(\n",
    "                    unit_scores.get(f\"__label__{label}\", -9e9) >= min_score for label, min_score in self.remove_labels\n",
    "                )\n",
    "\n",
    "        units = split_into_parts(doc.text, mode=self.filter_mode)\n",
    "        kept_spans = []\n",
    "        label_scores = defaultdict(list)\n",
    "        for unit in units:\n",
    "            labels, scores = self.model.predict(unit.strip().replace(\"\\n\", self.newline_replacement), k=-1)\n",
    "            if self.save_labels_in_metadata:\n",
    "                for label, score in zip(labels, scores):\n",
    "                    label_scores[label].append(score)\n",
    "            if check_label_scores(dict(zip(labels, scores))):\n",
    "                kept_spans.append(unit)\n",
    "                self.stat_update(\"kept_span\")\n",
    "            else:\n",
    "                self.stat_update(\"removed_span\")\n",
    "        doc.text = \"\".join(kept_spans)\n",
    "        if self.save_labels_in_metadata:\n",
    "            doc.metadata.update({label: np.mean(scores).item() for label, scores in label_scores.items()})\n",
    "        return not not doc.text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lambda filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from datatrove.data import Document\n",
    "from datatrove.pipeline.filters.base_filter import BaseFilter\n",
    "from datatrove.pipeline.writers.disk_base import DiskWriter\n",
    "\n",
    "class LambdaFilter(BaseFilter):\n",
    "    name = \"👤 Lambda\"\n",
    "\n",
    "    def __init__(self, filter_function: Callable[[Document], bool], exclusion_writer: DiskWriter = None):\n",
    "        \"\"\"\n",
    "        filters documents triggering the given filter_function with respect to a specific metadata key.\n",
    "\n",
    "        Args:\n",
    "            filter_function:\n",
    "            exclusion_writer:\n",
    "        \"\"\"\n",
    "        super().__init__(exclusion_writer)\n",
    "        self.filter_function = filter_function\n",
    "\n",
    "    def filter(self, doc: Document) -> bool:\n",
    "        \"\"\"Args:\n",
    "            doc: document\n",
    "\n",
    "        Returns:\n",
    "            is_filter\n",
    "        \"\"\"\n",
    "        return self.filter_function(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from datatrove.data import Document\n",
    "from datatrove.pipeline.filters.base_filter import BaseFilter\n",
    "from datatrove.pipeline.writers.disk_base import DiskWriter\n",
    "\n",
    "\n",
    "class RegexFilter(BaseFilter):\n",
    "    name = \"🕵 Regex\"\n",
    "\n",
    "    def __init__(self, regex_exp: str, exclusion_writer: DiskWriter = None):\n",
    "        \"\"\"\n",
    "        filters if regex finds at least one match\n",
    "\n",
    "        Args:\n",
    "            regex_exp: regex expression\n",
    "            exclusion_writer:\n",
    "        \"\"\"\n",
    "        super().__init__(exclusion_writer)\n",
    "        self.regex = re.compile(regex_exp)\n",
    "\n",
    "    def filter(self, doc: Document) -> bool:\n",
    "        \"\"\"Args:\n",
    "            doc: document\n",
    "\n",
    "        Returns:\n",
    "            is_filter\n",
    "        \"\"\"\n",
    "        return not self.regex.search(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "from typing import Iterable\n",
    "\n",
    "from huggingface_hub import cached_assets_path\n",
    "from loguru import logger\n",
    "\n",
    "from datatrove.data import Document\n",
    "from datatrove.io import safely_create_file\n",
    "from datatrove.utils._import_utils import ASSETS_PATH\n",
    "\n",
    "from ..writers.disk_base import DiskWriter\n",
    "from .base_filter import BaseFilter\n",
    "\n",
    "\n",
    "normalizer = re.compile(r\"[^a-zA-Z0-9]+\")\n",
    "\n",
    "\n",
    "def normalize(text, replace=\"\"):\n",
    "    return normalizer.sub(replace, text).lower()\n",
    "\n",
    "\n",
    "def parse_list(line, do_normalize=True):\n",
    "    return {normalize(x) if do_normalize else x.strip() for x in line if x[0] != \"#\"}\n",
    "\n",
    "\n",
    "def get_list(abs_path: str, file_name: str, extra: set = None, do_normalize: bool = True):\n",
    "    with open(os.path.join(abs_path, file_name)) as f:\n",
    "        return parse_list(f, do_normalize).union(set(parse_list(extra, do_normalize)) if extra else set())\n",
    "\n",
    "\n",
    "class URLFilter(BaseFilter):\n",
    "    \"\"\"\n",
    "    Performs filtering based on samples urls.\n",
    "    Samples are removed if:\n",
    "    - their domain is present on `block_listed_domains`\n",
    "    - if their subdomain is present on `block_listed_domains`\n",
    "    - if the full url is present on `block_listed_url`\n",
    "    - if any word from `banned_words` is in the url\n",
    "    - if there are at least `soft_word_threshold` words from `soft_banned_words` in the url\n",
    "    - if any word from `banned_subwords` is a substring of the url\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"😈 Url-filter\"\n",
    "    _requires_dependencies = [\"tldextract\", \"fasteners\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        soft_word_threshold: int = 2,\n",
    "        extra_domains: Iterable = None,\n",
    "        extra_urls: Iterable = None,\n",
    "        banned_words: Iterable = None,\n",
    "        banned_subwords: Iterable = None,\n",
    "        soft_banned_words: Iterable = None,\n",
    "        exclusion_writer: DiskWriter = None,\n",
    "    ):\n",
    "        from tldextract import TLDExtract\n",
    "\n",
    "        super().__init__(exclusion_writer)\n",
    "        self.soft_word_threshold = soft_word_threshold\n",
    "        self.block_listed_domains = extra_domains\n",
    "        self.block_listed_url = extra_urls\n",
    "        self.banned_words = banned_words\n",
    "        self.banned_subwords = banned_subwords\n",
    "        self.soft_banned_words = soft_banned_words\n",
    "        self._downloaded = False\n",
    "        self.tldextractor = TLDExtract()\n",
    "\n",
    "    def download_data(self):\n",
    "        if self._downloaded:\n",
    "            return\n",
    "        download_dir = cached_assets_path(library_name=\"datatrove\", namespace=\"filters\", subfolder=\"url_filter\")\n",
    "        file_to_lock = os.path.join(download_dir, \"url_filterblacklists.tar.gz\")\n",
    "\n",
    "        def do_extract():\n",
    "            logger.info(\"💥 Extracting url filter blacklists...\")\n",
    "            with tarfile.open(os.path.join(ASSETS_PATH, \"url_filterblacklists.tar.gz\"), \"r:gz\") as tar:\n",
    "                tar.extractall(download_dir)\n",
    "            logger.info(\"💥 Extracted url filter blacklists.\")\n",
    "\n",
    "        safely_create_file(file_to_lock, do_extract)\n",
    "\n",
    "        self.block_listed_domains = get_list(\n",
    "            download_dir, \"adult/domains\", self.block_listed_domains, do_normalize=False\n",
    "        )\n",
    "        self.block_listed_url = get_list(download_dir, \"adult/urls\", self.block_listed_url, do_normalize=False)\n",
    "        self.banned_words = get_list(ASSETS_PATH, \"banned_words.txt\", self.banned_words)\n",
    "        self.banned_subwords = get_list(ASSETS_PATH, \"banned_subwords.txt\", self.banned_subwords)\n",
    "        self.soft_banned_words = get_list(ASSETS_PATH, \"soft_banned_words.txt\", self.soft_banned_words)\n",
    "        self._downloaded = True\n",
    "\n",
    "    def filter(self, document: Document) -> bool | tuple[bool, str]:\n",
    "        self.download_data()\n",
    "        url = document.metadata.get(\"url\")\n",
    "\n",
    "        assert url, \"Document does not have url in its metadata\"\n",
    "        url_info = self.tldextractor(url)\n",
    "\n",
    "        if url_info.registered_domain in self.block_listed_domains:\n",
    "            return False, \"domain\"\n",
    "\n",
    "        if url_info.fqdn in self.block_listed_domains:\n",
    "            return False, \"subdomain\"\n",
    "\n",
    "        if url in self.block_listed_url:\n",
    "            return False, \"url\"\n",
    "\n",
    "        url_words = set(normalizer.split(url))\n",
    "        if any(word in url_words for word in self.banned_words):\n",
    "            return False, \"hard_blacklisted\"\n",
    "\n",
    "        nb_soft_words = sum([word in url_words for word in self.soft_banned_words])\n",
    "        if nb_soft_words >= self.soft_word_threshold:\n",
    "            return False, \"soft_blacklisted\"\n",
    "\n",
    "        normalized_space = normalize(url)\n",
    "        if any(word in normalized_space for word in self.banned_subwords):\n",
    "            return False, \"blacklisted_subword\"\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unigram\n",
    "クラスのドキュメント文字列で、このフィルタが英語の単語頻度データに基づいて単語のログ確率の平均を計算し、その平均が閾値よりも高いかどうかをチェックすることを説明しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "from huggingface_hub import cached_assets_path\n",
    "from loguru import logger\n",
    "\n",
    "from datatrove.data import Document\n",
    "from datatrove.pipeline.filters.base_filter import BaseFilter\n",
    "from datatrove.pipeline.writers.disk_base import DiskWriter\n",
    "\n",
    "\n",
    "UNIGRAM_DOWNLOAD = \"https://ai2-s2-research-public.s3-us-west-2.amazonaws.com/lucas/google-1T-unigram/unigram_freq.csv\"\n",
    "\n",
    "\n",
    "class UnigramLogProbFilter(BaseFilter):\n",
    "    \"\"\"\n",
    "    Computes average unigram log probability based on word frequencies from\n",
    "    https://www.kaggle.com/datasets/rtatman/english-word-frequency\n",
    "\n",
    "    Idea taken from https://huggingface.co/datasets/allenai/peS2o\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"🧑‍🍳 Unigram log-prob filter\"\n",
    "    _requires_dependencies = [\"nltk\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        logprobs_threshold: float = -10,\n",
    "        exclusion_writer: DiskWriter = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            logprobs_threshold: the minimum average unigram logprobs needed to keep a document\n",
    "            exclusion_writer:\n",
    "        \"\"\"\n",
    "        super().__init__(exclusion_writer)\n",
    "        self.logprobs_threshold = logprobs_threshold\n",
    "        self.unigram_frequencies = self.get_frequencies()\n",
    "\n",
    "    def get_frequencies(self):\n",
    "        download_dir = cached_assets_path(\n",
    "            library_name=\"datatrove\", namespace=\"filters\", subfolder=\"unigram_logprob_filter\"\n",
    "        )\n",
    "        unigram_freq_file = os.path.join(download_dir, \"unigram_freq.csv\")\n",
    "        if not os.path.isfile(unigram_freq_file):\n",
    "            logger.info(\"⬇️ Downloading unigram-frequencies ...\")\n",
    "            urllib.request.urlretrieve(UNIGRAM_DOWNLOAD, unigram_freq_file)\n",
    "\n",
    "        words = []\n",
    "        counts = []\n",
    "        with open(unigram_freq_file, encoding=\"utf-8\", newline=\"\") as f:\n",
    "            csv_reader = csv.DictReader(f)\n",
    "            for row in csv_reader:\n",
    "                words.append(row[\"word\"])\n",
    "                counts.append(int(row[\"count\"]))\n",
    "        total_count = sum(counts)\n",
    "        return {word: count / total_count for word, count in zip(words, counts)}\n",
    "\n",
    "    def get_logprob(self, doc):\n",
    "        from nltk.tokenize import word_tokenize\n",
    "\n",
    "        words = word_tokenize(doc.text)\n",
    "        freqs = [self.unigram_frequencies.get(word.lower(), 1e-9) for word in words]\n",
    "\n",
    "        if len(freqs) == 0:\n",
    "            return 0\n",
    "        return sum([np.log(f) for f in freqs]) / len(freqs)\n",
    "\n",
    "    def filter(self, doc: Document) -> bool:\n",
    "        \"\"\"\n",
    "            Checks if the average unigram probability is above the threshold. This assumes the text is in english.\n",
    "        Args:\n",
    "            doc:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        return self.get_logprob(doc) > self.logprobs_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# language filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatrove.data import Document\n",
    "from datatrove.io import cached_asset_path_or_download\n",
    "from datatrove.pipeline.filters.base_filter import BaseFilter\n",
    "from datatrove.pipeline.writers.disk_base import DiskWriter\n",
    "from datatrove.utils.typeshelper import Languages\n",
    "\n",
    "\n",
    "LANGUAGE_ID_MODEL_URL = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n",
    "\n",
    "\n",
    "class LanguageFilter(BaseFilter):\n",
    "    name = \"🌍 Language ID\"\n",
    "    _requires_dependencies = [(\"fasttext\", \"fasttext-wheel\"), \"fasteners\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        languages: tuple = (Languages.english,),\n",
    "        language_threshold: float = 0.65,\n",
    "        exclusion_writer: DiskWriter = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        filters if the predicted language is not among given language or if the language score is below language\n",
    "        language_threshold\n",
    "\n",
    "        Args:\n",
    "            languages: list of languages to keep\n",
    "            language_threshold: language_threshold minimum score to accept a document\n",
    "            exclusion_writer:\n",
    "        \"\"\"\n",
    "        super().__init__(exclusion_writer)\n",
    "        self.language_threshold = language_threshold\n",
    "        self.languages = languages\n",
    "        self._model = None\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        if not self._model:\n",
    "            from fasttext.FastText import _FastText\n",
    "\n",
    "            model_file = cached_asset_path_or_download(\n",
    "                LANGUAGE_ID_MODEL_URL,\n",
    "                namespace=\"filters\",\n",
    "                subfolder=\"language_filter\",\n",
    "                desc=\"fast-text language identifier model\",\n",
    "            )\n",
    "            self._model = _FastText(model_file)\n",
    "        return self._model\n",
    "\n",
    "    def filter(self, doc: Document) -> bool:\n",
    "        \"\"\"Args:\n",
    "            doc: document\n",
    "\n",
    "        Returns:\n",
    "            is_filter\n",
    "        \"\"\"\n",
    "\n",
    "        language, score = self.model.predict(doc.text.replace(\"\\n\", \"\"))\n",
    "        # language label is given in the form __label__<language_id>\n",
    "        language = language[0].split(\"__\")[2]\n",
    "        doc.metadata[\"language\"] = language\n",
    "        doc.metadata[\"language_score\"] = score[0]\n",
    "        return score > self.language_threshold and language in self.languages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
