{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RefinedWebã®å‡¦ç†MDRã®æ¨¡å€£å®Ÿè£…ã‚’ã—ãŸfinewebã®å‡¦ç†æ¦‚è¦\n",
    "[finedwebã®ã‚³ãƒ¼ãƒ‰](https://github.com/huggingface/datatrove/blob/main/examples/fineweb.py)ã‚’å‚è€ƒã«ã—ãŸ\n",
    "\n",
    "*  WarcReader\n",
    "* URLFilter\n",
    "* Trafilatura\n",
    "* LanguageFilte\n",
    "* GopherRepetitionFilter\n",
    "* GopherQualityFilte\n",
    "* C4QualityFilter\n",
    "* FineWebQualityFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple\n",
    "\n",
    "from datatrove.data import Document, DocumentsPipeline\n",
    "from datatrove.pipeline.base import PipelineStep\n",
    "from datatrove.pipeline.writers.disk_base import DiskWriter\n",
    "from datatrove.utils.typeshelper import StatHints\n",
    "\n",
    "\n",
    "def get_filter_result(res):\n",
    "    result, reason = res, None\n",
    "    if isinstance(result, tuple):\n",
    "        result, reason = res\n",
    "    return result, reason\n",
    "\n",
    "\n",
    "class BaseFilter(PipelineStep, ABC):\n",
    "    \"\"\"Base module for Filters. Filters remove documents.\n",
    "\n",
    "    Args:\n",
    "        exclusion_writer: optionally pass in a writer that will save the dropped documents\n",
    "    \"\"\"\n",
    "\n",
    "    type = \"ðŸ”» - FILTER\"\n",
    "\n",
    "    def __init__(self, exclusion_writer: DiskWriter = None):\n",
    "        super().__init__()\n",
    "        self.exclusion_writer = exclusion_writer\n",
    "\n",
    "    @abstractmethod\n",
    "    def filter(self, doc: Document) -> bool | Tuple[bool, str]:\n",
    "        \"\"\"Filter modules main method.\n",
    "        Returns true if a sample should be KEPT, false if it should be REMOVED.\n",
    "\n",
    "        Args:\n",
    "            doc: sample to filter\n",
    "\n",
    "        Returns:\n",
    "            bool - whether the doc should be kept\n",
    "            or (False, str), to drop with a specific reason\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run(self, data: DocumentsPipeline, rank: int = 0, world_size: int = 1) -> DocumentsPipeline:\n",
    "        with self.exclusion_writer if self.exclusion_writer else contextlib.nullcontext() as writer:\n",
    "            for doc in data:\n",
    "                self.stat_update(StatHints.total)\n",
    "                with self.track_time():\n",
    "                    filter_result, reason = get_filter_result(self.filter(doc))\n",
    "                    if filter_result:\n",
    "                        self.stat_update(StatHints.forwarded)\n",
    "                        self.update_doc_stats(doc)\n",
    "                    else:\n",
    "                        self.stat_update(StatHints.dropped)\n",
    "                        if reason:\n",
    "                            self.stat_update(f\"dropped_{reason}\")\n",
    "                        if self.exclusion_writer:\n",
    "                            if reason:\n",
    "                                doc.metadata[\"filter_reason\"] = reason\n",
    "                            writer.write(doc, rank)\n",
    "                        continue\n",
    "                yield doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quality filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finedweb_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datatrove.pipeline.filters.base_filter import BaseFilter\n",
    "\n",
    "class FWQualityFilter(BaseFilter):\n",
    "    name = \"fineweb quality filter\"\n",
    "    _requires_dependencies = [\"nltk\"]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            exclusion_writer,\n",
    "            line_punct_thr: float = 0.12,\n",
    "            line_punct_exclude_zero=False,\n",
    "            short_line_thr: float = 0.67,\n",
    "            short_line_length: int = 30,\n",
    "            char_duplicates_ratio: float = 0.01\n",
    "        ):\n",
    "        super().__init__(exclusion_writer)\n",
    "        self.line_punct_thr = line_punct_thr\n",
    "        self.line_punct_exclude_zero = line_punct_exclude_zero\n",
    "        self.short_line_threshold = short_line_thr\n",
    "        self.short_line_length = short_line_length\n",
    "        self.char_duplicates_ratio = char_duplicates_ratio\n",
    "\n",
    "    def filter(self, doc) -> bool | tuple[bool, str]:\n",
    "        from datatrove.pipeline.filters.gopher_repetition_filter import find_duplicates\n",
    "\n",
    "        def remove_empty_lines(lines: list[str]):\n",
    "            return [l for l in lines if l.strip() != \"\"]\n",
    "\n",
    "        stop_chars = (\".\", \"'\", '\"', \"!\", \"?\")\n",
    "\n",
    "        lines = doc.text.split(\"\\n\")\n",
    "        ratio = sum(1 for line in lines if line.endswith(stop_chars)) / len(lines)\n",
    "        if ratio <= self.line_punct_thr and not (ratio == 0 and self.line_punct_exclude_zero):\n",
    "            return False, \"line_punct_ratio\"\n",
    "\n",
    "        ratio = sum(1 for line in lines if len(line) <= self.short_line_length) / len(\n",
    "            lines\n",
    "        )\n",
    "        if ratio >= self.short_line_threshold:\n",
    "            return False, \"short_line_ratio\"\n",
    "\n",
    "        ratio = find_duplicates(remove_empty_lines(lines))[1] / len(doc.text.replace(\"\\n\", \"\"))\n",
    "\n",
    "        if ratio >= self.char_duplicates_ratio:\n",
    "            return False, \"char_dup_ratio\"\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¿®æ­£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fined_quality_filter(text, line_punct_thr: float = 0.12,\n",
    "                         line_punct_exclude_zero=False,\n",
    "                         short_line_thr: float = 0.67,\n",
    "                         short_line_length: int = 30,\n",
    "                         char_duplicates_ratio: float = 0.01):\n",
    "\n",
    "    # Helper functions\n",
    "    def remove_empty_lines(lines):\n",
    "        return [l for l in lines if l.strip()]\n",
    "\n",
    "    def find_duplicates(lines):\n",
    "        # Simplified example of a duplicate finder\n",
    "        from collections import Counter\n",
    "        char_count = Counter(''.join(lines))\n",
    "        total_chars = sum(char_count.values())\n",
    "        return 0, sum(v - 1 for v in char_count.values() if v > 1)\n",
    "\n",
    "    # Remove empty lines\n",
    "    lines = text.split(\"\\n\")\n",
    "    non_empty_lines = remove_empty_lines(lines)\n",
    "\n",
    "    # Calculate line punctuation ratio\n",
    "    stop_chars = (\".\", \"'\", '\"', \"!\", \"?\")\n",
    "    ratio = sum(1 for line in lines if line.endswith(stop_chars)) / max(len(lines), 1)\n",
    "    if ratio <= line_punct_thr and not (ratio == 0 and line_punct_exclude_zero):\n",
    "        return \"\"\n",
    "\n",
    "    # Calculate short line ratio\n",
    "    ratio = sum(1 for line in lines if len(line) <= short_line_length) / max(len(lines), 1)\n",
    "    if ratio >= short_line_thr:\n",
    "        return \"\"\n",
    "\n",
    "    # Calculate character duplicates ratio\n",
    "    _, duplicate_chars = find_duplicates(non_empty_lines)\n",
    "    ratio = duplicate_chars / max(len(text.replace(\"\\n\", \"\")), 1)\n",
    "    if ratio >= char_duplicates_ratio:\n",
    "        return \"\"\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gopher_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datatrove.data import Document\n",
    "from datatrove.pipeline.filters.base_filter import BaseFilter\n",
    "from datatrove.pipeline.writers.disk_base import DiskWriter\n",
    "from datatrove.utils.text import PUNCTUATION_SET\n",
    "\n",
    "\n",
    "STOP_WORDS = [\"the\", \"be\", \"to\", \"of\", \"and\", \"that\", \"have\", \"with\"]\n",
    "\n",
    "\n",
    "class GopherQualityFilter(BaseFilter):\n",
    "    name = \"ðŸ¥‡ Gopher Quality\"\n",
    "    _requires_dependencies = [\"nltk\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_doc_words: int | None = 50,\n",
    "        max_doc_words: int | None = 100000,\n",
    "        min_avg_word_length: int | None = 3,\n",
    "        max_avg_word_length: int | None = 10,\n",
    "        max_symbol_word_ratio: float | None = 0.1,\n",
    "        max_bullet_lines_ratio: float | None = 0.9,\n",
    "        max_ellipsis_lines_ratio: float | None = 0.3,\n",
    "        max_non_alpha_words_ratio: float | None = 0.8,\n",
    "        min_stop_words: int | None = 2,\n",
    "        stop_words: list[str] | None = None,\n",
    "        exclusion_writer: DiskWriter = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Filter to apply Gopher's quality heuristic rules.\n",
    "        Reference: https://arxiv.org/pdf/2112.11446.pdf\n",
    "\n",
    "        Args:\n",
    "            min_doc_words:\n",
    "            max_doc_words:\n",
    "            min_avg_word_length:\n",
    "            max_avg_word_length:\n",
    "            max_symbol_word_ratio:\n",
    "            max_bullet_lines_ratio:\n",
    "            max_ellipsis_lines_ratio:\n",
    "            max_non_alpha_words_ratio:\n",
    "            min_stop_words:\n",
    "            stop_words:\n",
    "            exclusion_writer:\n",
    "        \"\"\"\n",
    "        super().__init__(exclusion_writer)\n",
    "        self.min_doc_words = min_doc_words\n",
    "        self.max_doc_words = max_doc_words\n",
    "        self.min_avg_word_length = min_avg_word_length\n",
    "        self.max_avg_word_length = max_avg_word_length\n",
    "        self.max_symbol_word_ratio = max_symbol_word_ratio\n",
    "        self.max_bullet_lines_ratio = max_bullet_lines_ratio\n",
    "        self.max_ellipsis_lines_ratio = max_ellipsis_lines_ratio\n",
    "        self.max_non_alpha_words_ratio = max_non_alpha_words_ratio\n",
    "        self.min_stop_words = min_stop_words\n",
    "        self.stop_words = set(STOP_WORDS if stop_words is None else stop_words)\n",
    "\n",
    "    def filter(self, doc: Document) -> bool | tuple[bool, str]:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            doc: Applies the heuristics rules to decide if a document should be REMOVED\n",
    "\n",
    "\n",
    "        Returns: False if sample.text does not pass any of the the heuristic tests\n",
    "\n",
    "        \"\"\"\n",
    "        from nltk.tokenize import word_tokenize\n",
    "\n",
    "        text = doc.text\n",
    "        words = word_tokenize(text)  # TODO we should use language id filter\n",
    "        n_words = len(words)\n",
    "\n",
    "        non_symbol_words = [w for w in words if any(ch not in PUNCTUATION_SET for ch in w)]\n",
    "        n_non_symbol_words_words = len(non_symbol_words)\n",
    "\n",
    "        # words < min_doc_words or words > max_doc_words\n",
    "        if self.min_doc_words and n_non_symbol_words_words < self.min_doc_words:\n",
    "            return False, \"gopher_short_doc\"\n",
    "        if self.max_doc_words and n_non_symbol_words_words > self.max_doc_words:\n",
    "            return False, \"gopher_long_doc\"\n",
    "\n",
    "        # mean word length is outside the range of 3 to 10 characters\n",
    "        avg_n_words = np.mean([len(w) for w in non_symbol_words])\n",
    "        if self.min_avg_word_length and avg_n_words < self.min_avg_word_length:\n",
    "            return False, \"gopher_below_avg_threshold\"\n",
    "        if self.max_avg_word_length and avg_n_words > self.max_avg_word_length:\n",
    "            return False, \"gopher_above_avg_threshold\"\n",
    "\n",
    "        # symbol-to-word ratio greater than 0.1 for either the hash symbol or the ellipsis\n",
    "        if self.max_symbol_word_ratio and text.count(\"#\") / n_words > self.max_symbol_word_ratio:\n",
    "            return False, \"gopher_too_many_hashes\"\n",
    "        if self.max_symbol_word_ratio and (text.count(\"...\") + text.count(\"â€¦\")) / n_words > self.max_symbol_word_ratio:\n",
    "            return False, \"gopher_too_many_ellipsis\"\n",
    "\n",
    "        # any document with more than 90 % of lines starting with a bullet point,\n",
    "        # or more than 30 % ending with an ellipsis.\n",
    "        lines = text.splitlines()\n",
    "        if (\n",
    "            self.max_bullet_lines_ratio\n",
    "            and sum(s.lstrip().startswith(\"â€¢\") or s.lstrip().startswith(\"-\") for s in lines) / len(lines)\n",
    "            > self.max_bullet_lines_ratio\n",
    "        ):\n",
    "            return False, \"gopher_too_many_bullets\"\n",
    "        if (\n",
    "            self.max_ellipsis_lines_ratio\n",
    "            and sum(s.rstrip().endswith(\"...\") or s.rstrip().endswith(\"â€¦\") for s in lines) / len(lines)\n",
    "            > self.max_ellipsis_lines_ratio\n",
    "        ):\n",
    "            return False, \"gopher_too_many_end_ellipsis\"\n",
    "\n",
    "        # that 80 % of words in a document contain at least one alphabetic character\n",
    "        if (\n",
    "            self.max_non_alpha_words_ratio\n",
    "            and sum([any((c.isalpha() for c in w)) for w in words]) / n_words < self.max_non_alpha_words_ratio\n",
    "        ):\n",
    "            return False, \"gopher_below_alpha_threshold\"\n",
    "\n",
    "        # stop word filter\n",
    "        if self.min_stop_words and sum(w in self.stop_words for w in words) < self.min_stop_words:\n",
    "            return False, \"gopher_enough_stop_words\"\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¿®æ­£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Global configuration for punctuation and stop words\n",
    "PUNCTUATION_SET = {\"!\", \",\", \".\", \":\", \";\", \"?\", \"-\"}\n",
    "STOP_WORDS = [\"the\", \"be\", \"to\", \"of\", \"and\", \"that\", \"have\", \"with\"]\n",
    "\n",
    "def gopher_quality_filter(\n",
    "    text,\n",
    "    min_doc_words=50,\n",
    "    max_doc_words=100000,\n",
    "    min_avg_word_length=3,\n",
    "    max_avg_word_length=10,\n",
    "    max_symbol_word_ratio=0.1,\n",
    "    max_bullet_lines_ratio=0.9,\n",
    "    max_ellipsis_lines_ratio=0.3,\n",
    "    max_non_alpha_words_ratio=0.8,\n",
    "    min_stop_words=2,\n",
    "    stop_words=None\n",
    "):\n",
    "    if stop_words is None:\n",
    "        stop_words = set(STOP_WORDS)\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    n_words = len(words)\n",
    "\n",
    "    non_symbol_words = [w for w in words if any(ch not in PUNCTUATION_SET for ch in w)]\n",
    "    n_non_symbol_words = len(non_symbol_words)\n",
    "\n",
    "    if (min_doc_words and n_non_symbol_words < min_doc_words) or \\\n",
    "       (max_doc_words and n_non_symbol_words > max_doc_words):\n",
    "        return \"\"\n",
    "\n",
    "    avg_word_length = np.mean([len(w) for w in non_symbol_words])\n",
    "    if (min_avg_word_length and avg_word_length < min_avg_word_length) or \\\n",
    "       (max_avg_word_length and avg_word_length > max_avg_word_length):\n",
    "        return \"\"\n",
    "\n",
    "    symbol_count = text.count(\"#\") + text.count(\"...\") + text.count(\"â€¦\")\n",
    "    if max_symbol_word_ratio and symbol_count / n_words > max_symbol_word_ratio:\n",
    "        return \"\"\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    bullet_lines_ratio = sum(s.lstrip().startswith(\"â€¢\") or s.lstrip().startswith(\"-\") for s in lines) / max(len(lines), 1)\n",
    "    ellipsis_lines_ratio = sum(s.rstrip().endswith(\"...\") or s.rstrip().endswith(\"â€¦\") for s in lines) / max(len(lines), 1)\n",
    "    if (max_bullet_lines_ratio and bullet_lines_ratio > max_bullet_lines_ratio) or \\\n",
    "       (max_ellipsis_lines_ratio and ellipsis_lines_ratio > max_ellipsis_lines_ratio):\n",
    "        return \"\"\n",
    "\n",
    "    non_alpha_words_ratio = sum(any(c.isalpha() for c in w) for w in words) / n_words\n",
    "    if max_non_alpha_words_ratio and non_alpha_words_ratio < max_non_alpha_words_ratio:\n",
    "        return \"\"\n",
    "\n",
    "    stop_word_count = sum(w.lower() in stop_words for w in words)\n",
    "    if min_stop_words and stop_word_count < min_stop_words:\n",
    "        return \"\"\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from datatrove.utils.text import PUNCTUATION_SET\n",
    "\n",
    "PUNCTUATION = \"!/â€”â€:ï¼…ï¼‘ã€ˆ&(ã€â”\\\\ã€#%ã€Œã€ï¼Œã€‘ï¼›+^]~â€œã€Šâ€ž';â€™{|âˆ¶Â´[=-`*ï¼Žï¼ˆâ€“ï¼Ÿï¼ï¼š$ï½žÂ«ã€‰,><ã€‹)?ï¼‰ã€‚â€¦@_.\\\"}â–ºÂ»\" + \"\".join(\n",
    "    map(\n",
    "        chr,\n",
    "        (x for a, b in ((0, 9), (11, 13), (13, 32), (127, 160)) for x in range(a, b)),\n",
    "    )\n",
    ")\n",
    "PUNCTUATION_SET = set(PUNCTUATION)\n",
    "\n",
    "\n",
    "\n",
    "STOP_WORDS = [\"the\", \"be\", \"to\", \"of\", \"and\", \"that\", \"have\", \"with\"]\n",
    "\n",
    "\n",
    "class GopherQualityFilter():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_doc_words: int | None = 50,\n",
    "        max_doc_words: int | None = 100000,\n",
    "        min_avg_word_length: int | None = 3,\n",
    "        max_avg_word_length: int | None = 10,\n",
    "        max_symbol_word_ratio: float | None = 0.1,\n",
    "        max_bullet_lines_ratio: float | None = 0.9,\n",
    "        max_ellipsis_lines_ratio: float | None = 0.3,\n",
    "        max_non_alpha_words_ratio: float | None = 0.8,\n",
    "        min_stop_words: int | None = 2,\n",
    "        stop_words: list[str] | None = None,\n",
    "        #exclusion_writer: DiskWriter = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Filter to apply Gopher's quality heuristic rules.\n",
    "        Reference: https://arxiv.org/pdf/2112.11446.pdf\n",
    "\n",
    "        Args:\n",
    "            min_doc_words:\n",
    "            max_doc_words:\n",
    "            min_avg_word_length:\n",
    "            max_avg_word_length:\n",
    "            max_symbol_word_ratio:\n",
    "            max_bullet_lines_ratio:\n",
    "            max_ellipsis_lines_ratio:\n",
    "            max_non_alpha_words_ratio:\n",
    "            min_stop_words:\n",
    "            stop_words:\n",
    "            exclusion_writer:\n",
    "        \"\"\"\n",
    "        self.min_doc_words = min_doc_words\n",
    "        self.max_doc_words = max_doc_words\n",
    "        self.min_avg_word_length = min_avg_word_length\n",
    "        self.max_avg_word_length = max_avg_word_length\n",
    "        self.max_symbol_word_ratio = max_symbol_word_ratio\n",
    "        self.max_bullet_lines_ratio = max_bullet_lines_ratio\n",
    "        self.max_ellipsis_lines_ratio = max_ellipsis_lines_ratio\n",
    "        self.max_non_alpha_words_ratio = max_non_alpha_words_ratio\n",
    "        self.min_stop_words = min_stop_words\n",
    "        self.stop_words = set(STOP_WORDS if stop_words is None else stop_words)\n",
    "\n",
    "    def filter(self, text: str) -> bool | tuple[bool, str]:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            doc: Applies the heuristics rules to decide if a document should be REMOVED\n",
    "\n",
    "\n",
    "        Returns: False if sample.text does not pass any of the the heuristic tests\n",
    "\n",
    "        \"\"\"\n",
    "        from nltk.tokenize import word_tokenize\n",
    "\n",
    "        words = word_tokenize(text)  # TODO we should use language id filter\n",
    "        n_words = len(words)\n",
    "        non_symbol_words = [w for w in words if any(ch not in PUNCTUATION_SET for ch in w)]\n",
    "        n_non_symbol_words_words = len(non_symbol_words)\n",
    "\n",
    "        # words < min_doc_words or words > max_doc_words\n",
    "        if self.min_doc_words and n_non_symbol_words_words < self.min_doc_words:\n",
    "            return \"\",False, \"gopher_short_doc\"\n",
    "        if self.max_doc_words and n_non_symbol_words_words > self.max_doc_words:\n",
    "            return \"\",False, \"gopher_long_doc\"\n",
    "\n",
    "        # mean word length is outside the range of 3 to 10 characters\n",
    "        avg_n_words = np.mean([len(w) for w in non_symbol_words])\n",
    "        if self.min_avg_word_length and avg_n_words < self.min_avg_word_length:\n",
    "            return \"\",False, \"gopher_below_avg_threshold\"\n",
    "        if self.max_avg_word_length and avg_n_words > self.max_avg_word_length:\n",
    "            return \"\",False, \"gopher_above_avg_threshold\"\n",
    "\n",
    "        # symbol-to-word ratio greater than 0.1 for either the hash symbol or the ellipsis\n",
    "        if self.max_symbol_word_ratio and text.count(\"#\") / n_words > self.max_symbol_word_ratio:\n",
    "            return \"\",False, \"gopher_too_many_hashes\"\n",
    "        if self.max_symbol_word_ratio and (text.count(\"...\") + text.count(\"â€¦\")) / n_words > self.max_symbol_word_ratio:\n",
    "            return \"\",False, \"gopher_too_many_ellipsis\"\n",
    "\n",
    "        # any document with more than 90 % of lines starting with a bullet point,\n",
    "        # or more than 30 % ending with an ellipsis.\n",
    "        lines = text.splitlines()\n",
    "        if (\n",
    "            self.max_bullet_lines_ratio\n",
    "            and sum(s.lstrip().startswith(\"â€¢\") or s.lstrip().startswith(\"-\") for s in lines) / len(lines)\n",
    "            > self.max_bullet_lines_ratio\n",
    "        ):\n",
    "            return \"\",False, \"gopher_too_many_bullets\"\n",
    "        if (\n",
    "            self.max_ellipsis_lines_ratio\n",
    "            and sum(s.rstrip().endswith(\"...\") or s.rstrip().endswith(\"â€¦\") for s in lines) / len(lines)\n",
    "            > self.max_ellipsis_lines_ratio\n",
    "        ):\n",
    "            return \"\",False, \"gopher_too_many_end_ellipsis\"\n",
    "\n",
    "        # that 80 % of words in a document contain at least one alphabetic character\n",
    "        if (\n",
    "            self.max_non_alpha_words_ratio\n",
    "            and sum([any((c.isalpha() for c in w)) for w in words]) / n_words < self.max_non_alpha_words_ratio\n",
    "        ):\n",
    "            return \"\" ,False, \"gopher_below_alpha_threshold\"\n",
    "\n",
    "        # stop word filter\n",
    "        if self.min_stop_words and sum(w in self.stop_words for w in words) < self.min_stop_words:\n",
    "            return \"\",False, \"gopher_enough_stop_words\"\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c4_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c4\n",
    "import heapq\n",
    "import re\n",
    "\n",
    "from datatrove.data import Document\n",
    "from datatrove.pipeline.filters.base_filter import BaseFilter\n",
    "from datatrove.pipeline.writers.disk_base import DiskWriter\n",
    "\n",
    "\n",
    "CITATION_REGEX = re.compile(r\"\\[\\d*]|\\[edit]|\\[citation needed]\")\n",
    "END_PUNCTUATION = (\".\", \"?\", \"!\", '\"', \"'\")\n",
    "ELLIPSIS = \"...\"\n",
    "POLICY_SUBSTRINGS = [\n",
    "    \"terms of use\",\n",
    "    \"privacy policy\",\n",
    "    \"cookie policy\",\n",
    "    \"uses cookies\",\n",
    "    \"use of cookies\",\n",
    "    \"use cookies\",\n",
    "]\n",
    "\n",
    "\n",
    "class C4QualityFilter(BaseFilter):\n",
    "    \"\"\"Applies heuristic rules from C4 https://jmlr.org/papers/volume21/20-074/20-074.pdf\n",
    "\n",
    "    - We only retained lines that ended in a terminal punctuation mark (! . \" ?)\n",
    "    - We discarded any page with fewer than 5 sentences and only retained lines that contained at least 3 words\n",
    "    - [NOT IMPLEMENTED] We removed any page that contained any word on the â€œList of Dirty, Naughty, Obscene or Otherwise Bad Wordsâ€\n",
    "    - We removed any line with the word Javascript.\n",
    "    - We removed any page where the phrase â€œlorem ipsumâ€ appeared\n",
    "    - We removed any pages that contained a curly bracket\n",
    "    Additional filters not mentioned on the list from the paper but on the code:\n",
    "    - Remove lines with one word over 1000 chars\n",
    "    - Remove lines with cookies and terms of use keywords\n",
    "\n",
    "    Reference implementation: https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/text/c4_utils.py#L197\n",
    "    Args:\n",
    "        exclusion_writer: optionally pass in a writer that will save the dropped documents\n",
    "        tokenizer_language: load a diff language specific punkt tokenizer from nltk\n",
    "        split_paragraph: by default (as in the paper) split on \"\\n\".\n",
    "            Set to \"False\" to apply the filters to each sentence instead of to each line\n",
    "        remove_citations: remove wikipedia style citations from the text\n",
    "        filter_no_terminal_punct: remove lines without terminal punctuation marks\n",
    "        min_num_sentences: remove documents that do not have at least this number of sentences (after line filtering).\n",
    "            set to -1 to disable\n",
    "        min_words_per_line: drop lines without this min number of words\n",
    "        max_word_length: drop lines where at least one word has more than this number of characters\n",
    "        filter_lorem_ipsum: drop documents that contain \"lorem ipsum\"\n",
    "        filter_javascript: drop lines mentioning \"javascript\"\n",
    "        filter_curly_bracket: drop documents containing {\n",
    "        filter_policy: drop lines containing any of the phrases in POLICY_SUBSTRINGS\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"â›° C4 Quality\"\n",
    "    _requires_dependencies = [\"nltk\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        exclusion_writer: DiskWriter = None,\n",
    "        tokenizer_language: str = \"english\",\n",
    "        split_paragraph: bool = True,  # default as used on c4. Set to \"False\" to split with sent_tokenize\n",
    "        remove_citations: bool = True,\n",
    "        filter_no_terminal_punct: bool = True,\n",
    "        min_num_sentences: int = 5,  # set to -1 to disable\n",
    "        min_words_per_line: int = 3,  # set to -1 to disable\n",
    "        max_word_length: int = 1000,  # set to -1 to disable\n",
    "        filter_lorem_ipsum: bool = True,\n",
    "        filter_javascript: bool = True,\n",
    "        filter_curly_bracket: bool = True,\n",
    "        filter_policy: bool = True,\n",
    "    ):\n",
    "        super().__init__(exclusion_writer)\n",
    "        self.tokenizer_language = tokenizer_language\n",
    "        self.split_paragraph = split_paragraph\n",
    "        self.remove_citations = remove_citations\n",
    "        self.filter_no_terminal_punct = filter_no_terminal_punct\n",
    "        self.min_num_sentences = min_num_sentences\n",
    "        self.min_words_per_line = min_words_per_line\n",
    "        self.max_word_length = max_word_length\n",
    "        self.filter_lorem_ipsum = filter_lorem_ipsum\n",
    "        self.filter_javascript = filter_javascript\n",
    "        self.filter_curly_bracket = filter_curly_bracket\n",
    "        self.filter_policy = filter_policy\n",
    "\n",
    "    def filter(self, doc: Document) -> bool | tuple[bool, str]:\n",
    "        from nltk.tokenize import sent_tokenize\n",
    "\n",
    "        lines = (\n",
    "            doc.text.splitlines()\n",
    "            if self.split_paragraph\n",
    "            else sent_tokenize(doc.text, language=self.tokenizer_language)\n",
    "        )\n",
    "\n",
    "        num_sentences = 0\n",
    "        kept_lines = []\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            words = line.split()\n",
    "            self.stat_update(\"line-total\")\n",
    "            # check line has too long word\n",
    "            if self.max_word_length != -1 and any(len(word) > self.max_word_length for word in words):\n",
    "                self.stat_update(\"line-filter-too_long_word\")\n",
    "                continue\n",
    "            # remove citation\n",
    "            if self.remove_citations:\n",
    "                line = CITATION_REGEX.sub(\"\", line)\n",
    "            # end punctuation\n",
    "            if self.filter_no_terminal_punct and (not line.endswith(END_PUNCTUATION) or line.endswith(ELLIPSIS)):\n",
    "                self.stat_update(\"line-filter-no_terminal_punc\")\n",
    "                continue\n",
    "            # min words per line\n",
    "            if len(words) < self.min_words_per_line:\n",
    "                self.stat_update(\"line-filter-too_few_words\")\n",
    "                continue\n",
    "            line_l = line.lower()\n",
    "            # lorem ipsum\n",
    "            if self.filter_lorem_ipsum and \"lorem ipsum\" in line_l:\n",
    "                return False, \"lorem_ipsum\"  # drop entire doc\n",
    "            # javascript\n",
    "            if self.filter_javascript and \"javascript\" in line_l:\n",
    "                self.stat_update(\"line-filter-javascript\")\n",
    "                continue\n",
    "            # bracket\n",
    "            if self.filter_curly_bracket and \"{\" in line:\n",
    "                return False, \"curly_bracket\"  # drop entire doc\n",
    "            # policy\n",
    "            if self.filter_policy and any(p in line_l for p in POLICY_SUBSTRINGS):\n",
    "                self.stat_update(\"line-filter-policy\")\n",
    "                continue\n",
    "            num_sentences += len(sent_tokenize(line, language=self.tokenizer_language)) if self.split_paragraph else 1\n",
    "            kept_lines.append(line)\n",
    "            self.stat_update(\"line-kept\")\n",
    "        if num_sentences < self.min_num_sentences:\n",
    "            return False, \"too_few_sentences\"\n",
    "\n",
    "        doc.text = (\"\\n\" if self.split_paragraph else \" \").join(kept_lines).strip()\n",
    "        return True\n",
    "\n",
    "\n",
    "class C4ParagraphFilter(BaseFilter):\n",
    "    \"\"\"Applies paragraph filtering from mC4\n",
    "\n",
    "    https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/text/c4_utils.py#L551\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"â›° C4 Paragraph\"\n",
    "\n",
    "    def __init__(self, exclusion_writer: DiskWriter = None):\n",
    "        super().__init__(exclusion_writer)\n",
    "\n",
    "        self.min_paragraphs = 3\n",
    "        self.min_paragraph_len = 200\n",
    "        self.line_delimiter = \"\\n\"\n",
    "\n",
    "    def paragraph_filter(self, page):\n",
    "        \"\"\"Returns False iff a page has too few or too short paragraphs.\"\"\"\n",
    "        lines = page.split(self.line_delimiter)\n",
    "        # Filter out docs that don't have at least three \"paragraphs\"\n",
    "        # (lines >= `min_paragraph_len` chars).\n",
    "        if (\n",
    "            len(lines) < self.min_paragraphs\n",
    "            or min(heapq.nlargest(3, [len(line) for line in lines])) < self.min_paragraph_len\n",
    "        ):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def filter(self, doc: Document) -> bool | tuple[bool, str]:\n",
    "        if not self.paragraph_filter(doc.text):\n",
    "            return False, f\"< {self.min_paragraphs} paragraphs\"\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¿®æ­£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Regular expressions and constants for C4 filtering\n",
    "CITATION_REGEX = re.compile(r\"\\[\\d*]|\\[edit]|\\[citation needed]\")\n",
    "END_PUNCTUATION = (\".\", \"?\", \"!\", '\"', \"'\")\n",
    "ELLIPSIS = \"...\"\n",
    "POLICY_SUBSTRINGS = [\n",
    "    \"terms of use\",\n",
    "    \"privacy policy\",\n",
    "    \"cookie policy\",\n",
    "    \"uses cookies\",\n",
    "    \"use of cookies\",\n",
    "    \"use cookies\",\n",
    "]\n",
    "\n",
    "def c4_quality_filter(\n",
    "    text,\n",
    "    tokenizer_language=\"english\",\n",
    "    split_paragraph=True,\n",
    "    remove_citations=True,\n",
    "    filter_no_terminal_punct=True,\n",
    "    min_num_sentences=5,\n",
    "    min_words_per_line=3,\n",
    "    max_word_length=1000,\n",
    "    filter_lorem_ipsum=True,\n",
    "    filter_javascript=True,\n",
    "    filter_curly_bracket=True,\n",
    "    filter_policy=True\n",
    "):\n",
    "    lines = text.splitlines() if split_paragraph else sent_tokenize(text, language=tokenizer_language)\n",
    "    num_sentences = 0\n",
    "    kept_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        words = line.split()\n",
    "        # Check line has too long word\n",
    "        if max_word_length != -1 and any(len(word) > max_word_length for word in words):\n",
    "            continue\n",
    "        # Remove citation\n",
    "        if remove_citations:\n",
    "            line = CITATION_REGEX.sub(\"\", line)\n",
    "        # End punctuation\n",
    "        if filter_no_terminal_punct and (not line.endswith(END_PUNCTUATION) or line.endswith(ELLIPSIS)):\n",
    "            continue\n",
    "        # Min words per line\n",
    "        if len(words) < min_words_per_line:\n",
    "            continue\n",
    "        line_lower = line.lower()\n",
    "        # Lorem ipsum\n",
    "        if filter_lorem_ipsum and \"lorem ipsum\" in line_lower:\n",
    "            return \"\"\n",
    "        # JavaScript\n",
    "        if filter_javascript and \"javascript\" in line_lower:\n",
    "            continue\n",
    "        # Bracket\n",
    "        if filter_curly_bracket and \"{\" in line:\n",
    "            return \"\"\n",
    "        # Policy\n",
    "        if filter_policy and any(p in line_lower for p in POLICY_SUBSTRINGS):\n",
    "            continue\n",
    "        # Count sentences\n",
    "        num_sentences += len(sent_tokenize(line, language=tokenizer_language)) if split_paragraph else 1\n",
    "        kept_lines.append(line)\n",
    "\n",
    "    if num_sentences < min_num_sentences:\n",
    "        return \"\"\n",
    "\n",
    "    return \"\\n\".join(kept_lines).strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# repetition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repetition \n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from datatrove.data import Document\n",
    "from datatrove.pipeline.filters.base_filter import BaseFilter\n",
    "from datatrove.pipeline.writers.disk_base import DiskWriter\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Table A1 from https://arxiv.org/pdf/2112.11446.pd\n",
    "    duplicate line fraction                 0.30\n",
    "    duplicate paragraph fraction            0.30\n",
    "    duplicate line character fraction       0.20\n",
    "    duplicate paragraph character fraction  0.20\n",
    "\n",
    "    top 2-gram character fraction           0.20\n",
    "    top 3-gram character fraction           0.18\n",
    "    top 4-gram character fraction           0.16\n",
    "\n",
    "    duplicate 5-gram character fraction     0.15\n",
    "    duplicate 6-gram character fraction     0.14\n",
    "    duplicate 7-gram character fraction     0.13\n",
    "    duplicate 8-gram character fraction     0.12\n",
    "    duplicate 9-gram character fraction     0.11\n",
    "    duplicate 10-gram character fraction    0.10\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_n_grams(words: list[str], n: int) -> list[str]:\n",
    "    return [\" \".join(words[i : i + n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "\n",
    "def find_duplicates(x: list[str]) -> tuple[int, int]:\n",
    "    unique_x = set()\n",
    "    duplicate_chars = 0\n",
    "    duplicate_elements = 0\n",
    "    for element in x:\n",
    "        if element in unique_x:\n",
    "            duplicate_chars += len(element)\n",
    "            duplicate_elements += 1\n",
    "\n",
    "        else:\n",
    "            unique_x.add(element)\n",
    "    return duplicate_elements, duplicate_chars\n",
    "\n",
    "def find_top_duplicate(x: list[str]) -> int:\n",
    "    counter = Counter()\n",
    "    for element in x:\n",
    "        counter[element] += 1\n",
    "    top_n_gram = counter.most_common(1)[0]\n",
    "    return len(top_n_gram[0]) * top_n_gram[1]\n",
    "\n",
    "\n",
    "def find_all_duplicate(words: list[str], n: int) -> int:\n",
    "    n_words = len(words)\n",
    "    unique = set()\n",
    "    repeated_chars, idx = 0, 0\n",
    "    while idx < n_words - n + 1:\n",
    "        n_gram = \"\".join(words[idx : idx + n])\n",
    "        if n_gram in unique:\n",
    "            repeated_chars += len(n_gram)\n",
    "            idx += n\n",
    "        else:\n",
    "            unique.add(n_gram)\n",
    "            idx += 1\n",
    "    assert repeated_chars <= len(\"\".join(words))\n",
    "    return repeated_chars\n",
    "\n",
    "\n",
    "class GopherRepetitionFilter(BaseFilter):\n",
    "    name = \"ðŸ‘¯ Gopher Repetition\"\n",
    "    _requires_dependencies = [\"nltk\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dup_line_frac: float | None = 0.3,\n",
    "        dup_para_frac: float | None = 0.3,\n",
    "        dup_line_char_frac: float | None = 0.2,\n",
    "        dup_para_char_frac: float | None = 0.2,\n",
    "        top_n_grams: tuple[tuple[int, float]] = ((2, 0.2), (3, 0.18), (4, 0.16)),\n",
    "        dup_n_grams: tuple[tuple[int, float]] = ((5, 0.15), (6, 0.14), (7, 0.13), (8, 0.12), (9, 0.11), (10, 0.10)),\n",
    "        exclusion_writer: DiskWriter = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            dup_line_frac:\n",
    "            dup_para_frac:\n",
    "            dup_line_char_frac:\n",
    "            dup_para_char_frac:\n",
    "            top_n_grams:\n",
    "            dup_n_grams:\n",
    "            exclusion_writer:\n",
    "        \"\"\"\n",
    "        super().__init__(exclusion_writer)\n",
    "\n",
    "        self.dup_line_frac = dup_line_frac\n",
    "        self.dup_para_frac = dup_para_frac\n",
    "        self.dup_line_char_frac = dup_line_char_frac\n",
    "        self.dup_para_char_frac = dup_para_char_frac\n",
    "        self.top_n_grams = top_n_grams\n",
    "        self.dup_n_grams = dup_n_grams\n",
    "        self.paragraph_exp = re.compile(r\"\\n{2,}\")\n",
    "\n",
    "    def filter(self, doc: Document) -> bool | tuple[bool, str]:\n",
    "        from nltk.tokenize import word_tokenize\n",
    "\n",
    "        text = doc.text\n",
    "\n",
    "        paragraphs = self.paragraph_exp.split(text.strip())\n",
    "        paragraphs_duplicates, char_duplicates = find_duplicates(paragraphs)\n",
    "        if self.dup_para_frac and paragraphs_duplicates / len(paragraphs) > self.dup_para_frac:\n",
    "            return False, \"dup_para_frac\"\n",
    "        if self.dup_para_char_frac and char_duplicates / len(text) > self.dup_para_char_frac:\n",
    "            return False, \"dup_para_char_frac\"\n",
    "\n",
    "        lines = text.splitlines()\n",
    "        line_duplicates, char_duplicates = find_duplicates(lines)\n",
    "        if self.dup_line_frac and line_duplicates / len(lines) > self.dup_line_frac:\n",
    "            return False, \"dup_line_frac\"\n",
    "        if self.dup_line_char_frac and char_duplicates / len(text) > self.dup_line_char_frac:\n",
    "            return False, \"dup_line_char_frac\"\n",
    "\n",
    "        words = word_tokenize(text, language=\"english\")  # TODO we should use language id filter\n",
    "\n",
    "        for n, n_frac in self.top_n_grams:\n",
    "            n_grams = get_n_grams(words, n)\n",
    "            if not n_grams:\n",
    "                continue\n",
    "            top_char_length = find_top_duplicate(n_grams)\n",
    "            if top_char_length / len(text) > n_frac:\n",
    "                return False, f\"top_{n}_gram\"\n",
    "\n",
    "        for n, n_frac in self.dup_n_grams:\n",
    "            n_duplicates_char = find_all_duplicate(words, n)\n",
    "            if n_duplicates_char / len(text) > n_frac:\n",
    "                return False, f\"duplicated_{n}_n_grams\"\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¿®æ­£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_n_grams(words, n):\n",
    "    return [\" \".join(words[i:i + n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "def find_duplicates(items):\n",
    "    seen = set()\n",
    "    duplicates = 0\n",
    "    duplicate_chars = 0\n",
    "    for item in items:\n",
    "        if item in seen:\n",
    "            duplicates += 1\n",
    "            duplicate_chars += len(item)\n",
    "        else:\n",
    "            seen.add(item)\n",
    "    return duplicates, duplicate_chars\n",
    "\n",
    "def find_top_duplicate(n_grams):\n",
    "    counter = Counter(n_grams)\n",
    "    if counter:\n",
    "        top, count = counter.most_common(1)[0]\n",
    "        return len(top) * count\n",
    "    return 0\n",
    "\n",
    "def find_all_duplicate(words, n):\n",
    "    seen = set()\n",
    "    repeated_chars = 0\n",
    "    n_words = len(words)\n",
    "    for i in range(n_words - n + 1):\n",
    "        n_gram = \" \".join(words[i:i + n])\n",
    "        if n_gram in seen:\n",
    "            repeated_chars += len(n_gram.replace(\" \", \"\"))\n",
    "        else:\n",
    "            seen.add(n_gram)\n",
    "    return repeated_chars\n",
    "\n",
    "def repetition_filter(text, dup_line_frac=0.3, dup_para_frac=0.3, dup_line_char_frac=0.2, dup_para_char_frac=0.2,\n",
    "                      top_n_grams=((2, 0.2), (3, 0.18), (4, 0.16)), dup_n_grams=((5, 0.15), (6, 0.14), (7, 0.13), (8, 0.12), (9, 0.11), (10, 0.10))):\n",
    "    paragraphs = re.split(r'\\n{2,}', text.strip())\n",
    "    paragraphs_duplicates, char_duplicates = find_duplicates(paragraphs)\n",
    "    if paragraphs_duplicates / len(paragraphs) > dup_para_frac or char_duplicates / len(text) > dup_para_char_frac:\n",
    "        return \"\"\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    line_duplicates, char_duplicates = find_duplicates(lines)\n",
    "    if line_duplicates / len(lines) > dup_line_frac or char_duplicates / len(text) > dup_line_char_frac:\n",
    "        return \"\"\n",
    "\n",
    "    words = word_tokenize(text, language=\"english\")\n",
    "    for n, frac in top_n_grams:\n",
    "        n_grams = get_n_grams(words, n)\n",
    "        if n_grams and find_top_duplicate(n_grams) / len(text) > frac:\n",
    "            return \"\"\n",
    "\n",
    "    for n, frac in dup_n_grams:\n",
    "        if find_all_duplicate(words, n) / len(text) > frac:\n",
    "            return \"\"\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datatrove.data import Document\n",
    "from datatrove.io import cached_asset_path_or_download\n",
    "from datatrove.pipeline.filters.base_filter import BaseFilter\n",
    "from datatrove.pipeline.writers.disk_base import DiskWriter\n",
    "from datatrove.utils.text import SPLIT_TEXT_DOCUMENTS, split_into_parts\n",
    "\n",
    "\n",
    "class FastTextClassifierFilter(BaseFilter):\n",
    "    \"\"\"\n",
    "    Only keeps documents that have\n",
    "    - AT LEAST ONE of the labels in `keep_labels` with a score above the configured threshold, or\n",
    "    - NONE of the labels in `remove_labels` with a score above the configured threshold.\n",
    "\n",
    "    You can only supply one of these, to avoid conflicts. Use multiple filters if you need to. If you supply\n",
    "    neither, the block will simply annotate each document with the labels (set `save_labels_in_metadata=True`)\n",
    "\n",
    "    Example:\n",
    "        for `keep_labels=[(\"math\", 0.9)]` will only keep samples with a score on __label__math of at least 0.9\n",
    "        for `remove_labels=[(\"math\", 0.9)]` will remove samples with a score on __label__math of at least 0.9\n",
    "\n",
    "    Info to train your own classifier: https://fasttext.cc/docs/en/supervised-tutorial.html\n",
    "\n",
    "    Args:\n",
    "        model_url: url to download the model from or local path\n",
    "        keep_labels: tuple of (label name without \"__label__\", min score) (or list of such tuples)\n",
    "        remove_labels: tuple of (label name without \"__label__\", min score) (or list of such tuples)\n",
    "        save_labels_in_metadata: whether to save all the label scores in the document metadata\n",
    "        newline_replacement: str to replace \\n with before predicting scores\n",
    "        filter_mode: predict and filter on DOCUMENT, PARAGRAPH or SENTENCE level\n",
    "        exclusion_writer:\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"ðŸ¤– fastText\"\n",
    "    _requires_dependencies = [(\"fasttext\", \"fasttext-wheel\"), \"fasteners\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_url: str,\n",
    "        keep_labels: Tuple[str, float] | list[Tuple[str, float]] | None = None,\n",
    "        remove_labels: Tuple[str, float] | list[Tuple[str, float]] | None = None,\n",
    "        save_labels_in_metadata: bool = True,\n",
    "        exclusion_writer: DiskWriter | None = None,\n",
    "        newline_replacement=\"\",\n",
    "        filter_mode: str = SPLIT_TEXT_DOCUMENTS,\n",
    "    ):\n",
    "        super().__init__(exclusion_writer)\n",
    "        self.model_url = model_url\n",
    "        self.keep_labels = keep_labels\n",
    "        self.remove_labels = remove_labels\n",
    "        self.filter_mode = filter_mode\n",
    "        if keep_labels and remove_labels:\n",
    "            raise ValueError(\"You can only supply one of `keep_labels` or `remove_labels`.\")\n",
    "        self.newline_replacement = newline_replacement\n",
    "        if keep_labels and isinstance(keep_labels[0], str):\n",
    "            self.keep_labels = [keep_labels]\n",
    "        if remove_labels and isinstance(remove_labels[0], str):\n",
    "            self.remove_labels = [remove_labels]\n",
    "        self.save_labels_in_metadata = save_labels_in_metadata\n",
    "        self._model = None\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        if not self._model:\n",
    "            from fasttext.FastText import _FastText\n",
    "\n",
    "            model_file = cached_asset_path_or_download(\n",
    "                self.model_url, namespace=\"filters\", subfolder=\"fasttext\", desc=\"fast-text model\"\n",
    "            )\n",
    "            self._model = _FastText(model_file)\n",
    "            # check label values\n",
    "            available_labels = [x.removeprefix(\"__label__\") for x in self._model.labels]\n",
    "            for label, _ in self.keep_labels or [] + self.remove_labels or []:\n",
    "                if label not in available_labels:\n",
    "                    raise ValueError(\n",
    "                        f\"Label '{label}' passed as keep_labels or remove_labels is not available in this \"\n",
    "                        f\"FastText model. Available labels: {available_labels}\"\n",
    "                    )\n",
    "        return self._model\n",
    "\n",
    "    def filter(self, doc: Document) -> bool:\n",
    "        def check_label_scores(unit_scores):\n",
    "            if self.keep_labels:\n",
    "                return any(\n",
    "                    unit_scores.get(f\"__label__{label}\", -9e9) >= min_score for label, min_score in self.keep_labels\n",
    "                )\n",
    "            else:\n",
    "                return not self.remove_labels or not any(\n",
    "                    unit_scores.get(f\"__label__{label}\", -9e9) >= min_score for label, min_score in self.remove_labels\n",
    "                )\n",
    "\n",
    "        units = split_into_parts(doc.text, mode=self.filter_mode)\n",
    "        kept_spans = []\n",
    "        label_scores = defaultdict(list)\n",
    "        for unit in units:\n",
    "            labels, scores = self.model.predict(unit.strip().replace(\"\\n\", self.newline_replacement), k=-1)\n",
    "            if self.save_labels_in_metadata:\n",
    "                for label, score in zip(labels, scores):\n",
    "                    label_scores[label].append(score)\n",
    "            if check_label_scores(dict(zip(labels, scores))):\n",
    "                kept_spans.append(unit)\n",
    "                self.stat_update(\"kept_span\")\n",
    "            else:\n",
    "                self.stat_update(\"removed_span\")\n",
    "        doc.text = \"\".join(kept_spans)\n",
    "        if self.save_labels_in_metadata:\n",
    "            doc.metadata.update({label: np.mean(scores).item() for label, scores in label_scores.items()})\n",
    "        return not not doc.text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¿®æ­£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    # This is a very simple sentence splitter\n",
    "    return re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "\n",
    "def fast_text_filter(\n",
    "    text: str,\n",
    "    model_path: str,\n",
    "    keep_labels: List[Tuple[str, float]] = None,\n",
    "    remove_labels: List[Tuple[str, float]] = None,\n",
    "    newline_replacement: str = \"\",\n",
    "    filter_mode: str = 'document'\n",
    ") -> str:\n",
    "    # Load fastText model\n",
    "    model = fasttext.load_model(model_path)\n",
    "\n",
    "    # Prepare text according to the filter mode\n",
    "    if filter_mode == 'document':\n",
    "        parts = [text]\n",
    "    elif filter_mode == 'paragraph':\n",
    "        parts = text.split('\\n\\n')\n",
    "    elif filter_mode == 'sentence':\n",
    "        parts = split_into_sentences(text)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported filter mode: use 'document', 'paragraph', or 'sentence'.\")\n",
    "\n",
    "    kept_parts = []\n",
    "\n",
    "    for part in parts:\n",
    "        prepared_text = part.replace('\\n', newline_replacement)\n",
    "        labels, probabilities = model.predict(prepared_text, k=-1)\n",
    "        labels = [label.replace(\"__label__\", \"\") for label in labels]\n",
    "        label_prob = dict(zip(labels, probabilities))\n",
    "\n",
    "        # Check against keep and remove conditions\n",
    "        keep = not keep_labels or any(label_prob.get(label, 0) >= score for label, score in keep_labels)\n",
    "        remove = remove_labels and any(label_prob.get(label, 0) >= score for label, score in remove_labels)\n",
    "        \n",
    "        if keep and not remove:\n",
    "            kept_parts.append(part)\n",
    "\n",
    "    # Combine kept parts back into text\n",
    "    return \"\\n\\n\".join(kept_parts) if kept_parts else \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lambda filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from datatrove.data import Document\n",
    "from datatrove.pipeline.filters.base_filter import BaseFilter\n",
    "from datatrove.pipeline.writers.disk_base import DiskWriter\n",
    "\n",
    "class LambdaFilter(BaseFilter):\n",
    "    name = \"ðŸ‘¤ Lambda\"\n",
    "\n",
    "    def __init__(self, filter_function: Callable[[Document], bool], exclusion_writer: DiskWriter = None):\n",
    "        \"\"\"\n",
    "        filters documents triggering the given filter_function with respect to a specific metadata key.\n",
    "\n",
    "        Args:\n",
    "            filter_function:\n",
    "            exclusion_writer:\n",
    "        \"\"\"\n",
    "        super().__init__(exclusion_writer)\n",
    "        self.filter_function = filter_function\n",
    "\n",
    "    def filter(self, doc: Document) -> bool:\n",
    "        \"\"\"Args:\n",
    "            doc: document\n",
    "\n",
    "        Returns:\n",
    "            is_filter\n",
    "        \"\"\"\n",
    "        return self.filter_function(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¿®æ­£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an example text that contains the keyword.\n",
      "Filter result: True\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def lambda_filter(text: str, filter_function: Callable[[str], bool]) -> bool:\n",
    "    \"\"\"\n",
    "    A standalone filter function that applies a lambda or custom function to a given text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to filter.\n",
    "        filter_function (Callable[[str], bool]): A function that takes a string and returns a boolean.\n",
    "            It should return True if the text meets the filter criteria, and False otherwise.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the text passes the filter, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply the provided filter function to the text\n",
    "    return filter_function(text)\n",
    "\n",
    "# Example usage:\n",
    "# Define a simple filter function that checks if the text contains the word 'example'\n",
    "def example_filter(text: str) -> bool:\n",
    "    return 'example' in text.lower()\n",
    "\n",
    "# Apply the filter to a sample text\n",
    "sample_text = \"This is an example text that contains the keyword.\"\n",
    "result = lambda_filter(sample_text, example_filter)\n",
    "print(\"Filter result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from datatrove.data import Document\n",
    "from datatrove.pipeline.filters.base_filter import BaseFilter\n",
    "from datatrove.pipeline.writers.disk_base import DiskWriter\n",
    "\n",
    "\n",
    "class RegexFilter(BaseFilter):\n",
    "    name = \"ðŸ•µ Regex\"\n",
    "\n",
    "    def __init__(self, regex_exp: str, exclusion_writer: DiskWriter = None):\n",
    "        \"\"\"\n",
    "        filters if regex finds at least one match\n",
    "\n",
    "        Args:\n",
    "            regex_exp: regex expression\n",
    "            exclusion_writer:\n",
    "        \"\"\"\n",
    "        super().__init__(exclusion_writer)\n",
    "        self.regex = re.compile(regex_exp)\n",
    "\n",
    "    def filter(self, doc: Document) -> bool:\n",
    "        \"\"\"Args:\n",
    "            doc: document\n",
    "\n",
    "        Returns:\n",
    "            is_filter\n",
    "        \"\"\"\n",
    "        return not self.regex.search(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¿®æ­£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter result: \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def regex_filter(text: str, regex_exp: str) -> bool:\n",
    "    \"\"\"\n",
    "    Applies a regex filter to the given text. If the regex finds at least one match in the text,\n",
    "    it returns True, otherwise False.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to filter.\n",
    "        regex_exp (str): The regex expression to apply.\n",
    "\n",
    "    Returns:\n",
    "        bool: False if the regex finds at least one match, True otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compile the regex expression\n",
    "    regex = re.compile(regex_exp)\n",
    "\n",
    "    # Search the text for matches\n",
    "    if regex.search(text):\n",
    "        return False  # The document should be filtered out if there is a match\n",
    "    return True # No matches found, do not filter out the document\n",
    "\n",
    "# Example usage:\n",
    "sample_text = \"This is a sample text with a phone number 123-456-7890.\"\n",
    "regex_expression = r\"\\d{3}-\\d{3}-\\d{4}\"  # Matches a simple phone number format\n",
    "result = regex_filter(sample_text, regex_expression)\n",
    "print(\"Filter result:\", result)  # Expected: False, since there is a match\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "from typing import Iterable\n",
    "\n",
    "from huggingface_hub import cached_assets_path\n",
    "from loguru import logger\n",
    "\n",
    "from datatrove.data import Document\n",
    "from datatrove.io import safely_create_file\n",
    "from datatrove.utils._import_utils import ASSETS_PATH\n",
    "\n",
    "from ..writers.disk_base import DiskWriter\n",
    "from .base_filter import BaseFilter\n",
    "\n",
    "\n",
    "normalizer = re.compile(r\"[^a-zA-Z0-9]+\")\n",
    "\n",
    "\n",
    "def normalize(text, replace=\"\"):\n",
    "    return normalizer.sub(replace, text).lower()\n",
    "\n",
    "\n",
    "def parse_list(line, do_normalize=True):\n",
    "    return {normalize(x) if do_normalize else x.strip() for x in line if x[0] != \"#\"}\n",
    "\n",
    "\n",
    "def get_list(abs_path: str, file_name: str, extra: set = None, do_normalize: bool = True):\n",
    "    with open(os.path.join(abs_path, file_name)) as f:\n",
    "        return parse_list(f, do_normalize).union(set(parse_list(extra, do_normalize)) if extra else set())\n",
    "\n",
    "\n",
    "class URLFilter(BaseFilter):\n",
    "    \"\"\"\n",
    "    Performs filtering based on samples urls.\n",
    "    Samples are removed if:\n",
    "    - their domain is present on `block_listed_domains`\n",
    "    - if their subdomain is present on `block_listed_domains`\n",
    "    - if the full url is present on `block_listed_url`\n",
    "    - if any word from `banned_words` is in the url\n",
    "    - if there are at least `soft_word_threshold` words from `soft_banned_words` in the url\n",
    "    - if any word from `banned_subwords` is a substring of the url\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"ðŸ˜ˆ Url-filter\"\n",
    "    _requires_dependencies = [\"tldextract\", \"fasteners\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        soft_word_threshold: int = 2,\n",
    "        extra_domains: Iterable = None,\n",
    "        extra_urls: Iterable = None,\n",
    "        banned_words: Iterable = None,\n",
    "        banned_subwords: Iterable = None,\n",
    "        soft_banned_words: Iterable = None,\n",
    "        exclusion_writer: DiskWriter = None,\n",
    "    ):\n",
    "        from tldextract import TLDExtract\n",
    "\n",
    "        super().__init__(exclusion_writer)\n",
    "        self.soft_word_threshold = soft_word_threshold\n",
    "        self.block_listed_domains = extra_domains\n",
    "        self.block_listed_url = extra_urls\n",
    "        self.banned_words = banned_words\n",
    "        self.banned_subwords = banned_subwords\n",
    "        self.soft_banned_words = soft_banned_words\n",
    "        self._downloaded = False\n",
    "        self.tldextractor = TLDExtract()\n",
    "\n",
    "    def download_data(self):\n",
    "        if self._downloaded:\n",
    "            return\n",
    "        download_dir = cached_assets_path(library_name=\"datatrove\", namespace=\"filters\", subfolder=\"url_filter\")\n",
    "        file_to_lock = os.path.join(download_dir, \"url_filterblacklists.tar.gz\")\n",
    "\n",
    "        def do_extract():\n",
    "            logger.info(\"ðŸ’¥ Extracting url filter blacklists...\")\n",
    "            with tarfile.open(os.path.join(ASSETS_PATH, \"url_filterblacklists.tar.gz\"), \"r:gz\") as tar:\n",
    "                tar.extractall(download_dir)\n",
    "            logger.info(\"ðŸ’¥ Extracted url filter blacklists.\")\n",
    "\n",
    "        safely_create_file(file_to_lock, do_extract)\n",
    "\n",
    "        self.block_listed_domains = get_list(\n",
    "            download_dir, \"adult/domains\", self.block_listed_domains, do_normalize=False\n",
    "        )\n",
    "        self.block_listed_url = get_list(download_dir, \"adult/urls\", self.block_listed_url, do_normalize=False)\n",
    "        self.banned_words = get_list(ASSETS_PATH, \"banned_words.txt\", self.banned_words)\n",
    "        self.banned_subwords = get_list(ASSETS_PATH, \"banned_subwords.txt\", self.banned_subwords)\n",
    "        self.soft_banned_words = get_list(ASSETS_PATH, \"soft_banned_words.txt\", self.soft_banned_words)\n",
    "        self._downloaded = True\n",
    "\n",
    "    def filter(self, document: Document) -> bool | tuple[bool, str]:\n",
    "        self.download_data()\n",
    "        url = document.metadata.get(\"url\")\n",
    "\n",
    "        assert url, \"Document does not have url in its metadata\"\n",
    "        url_info = self.tldextractor(url)\n",
    "\n",
    "        if url_info.registered_domain in self.block_listed_domains:\n",
    "            return False, \"domain\"\n",
    "\n",
    "        if url_info.fqdn in self.block_listed_domains:\n",
    "            return False, \"subdomain\"\n",
    "\n",
    "        if url in self.block_listed_url:\n",
    "            return False, \"url\"\n",
    "\n",
    "        url_words = set(normalizer.split(url))\n",
    "        if any(word in url_words for word in self.banned_words):\n",
    "            return False, \"hard_blacklisted\"\n",
    "\n",
    "        nb_soft_words = sum([word in url_words for word in self.soft_banned_words])\n",
    "        if nb_soft_words >= self.soft_word_threshold:\n",
    "            return False, \"soft_blacklisted\"\n",
    "\n",
    "        normalized_space = normalize(url)\n",
    "        if any(word in normalized_space for word in self.banned_subwords):\n",
    "            return False, \"blacklisted_subword\"\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¿®æ­£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Set\n",
    "\n",
    "# Regular expression to split words in URLs\n",
    "normalizer = re.compile(r\"[^a-zA-Z0-9]+\")\n",
    "\n",
    "def normalize(text: str, replace: str = \"\") -> str:\n",
    "    \"\"\"Normalize text by removing non-alphanumeric characters.\"\"\"\n",
    "    return normalizer.sub(replace, text).lower()\n",
    "\n",
    "def url_filter(url: str, block_listed_domains: Set[str], block_listed_urls: Set[str],\n",
    "               banned_words: Set[str], banned_subwords: Set[str], \n",
    "               soft_banned_words: Set[str], soft_word_threshold: int = 2) -> bool:\n",
    "    \"\"\"\n",
    "    Filters URLs based on specified blocking criteria.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to check.\n",
    "        block_listed_domains (Set[str]): Domains that are outright blocked.\n",
    "        block_listed_urls (Set[str]): Full URLs that are outright blocked.\n",
    "        banned_words (Set[str]): Words that, if found in the URL, result in blocking.\n",
    "        banned_subwords (Set[str]): Subwords that, if found as substrings in the URL, result in blocking.\n",
    "        soft_banned_words (Set[str]): Words that contribute to a soft ban if they appear too frequently.\n",
    "        soft_word_threshold (int): The threshold for triggering a soft ban based on the frequency of soft-banned words.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the URL passes all filters (i.e., should not be blocked), False otherwise.\n",
    "    \"\"\"\n",
    "    # Check against blocked domains and full URLs\n",
    "    if url in block_listed_urls:\n",
    "        return False\n",
    "    domain = url.split('/')[2] if '/' in url else url  # Simplistic domain extraction\n",
    "    if domain in block_listed_domains:\n",
    "        return False\n",
    "\n",
    "    # Prepare words from the URL for word-based checks\n",
    "    url_words = set(normalize(url).split())\n",
    "\n",
    "    # Check against banned words and soft banned words\n",
    "    if any(word in url_words for word in banned_words):\n",
    "        return False\n",
    "    if sum(word in url_words for word in soft_banned_words) >= soft_word_threshold:\n",
    "        return False\n",
    "\n",
    "    # Check for banned subwords in the normalized URL\n",
    "    normalized_url = normalize(url)\n",
    "    if any(subword in normalized_url for subword in banned_subwords):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Example usage:\n",
    "block_domains = {\"example.com\"}\n",
    "block_urls = {\"http://example.com/badpath\"}\n",
    "banned = {\"badword\"}\n",
    "banned_subwords = {\"bad\"}\n",
    "soft_banned = {\"test\"}\n",
    "url_to_test = \"http://example.com/badpath?query=badword\"\n",
    "\n",
    "# Should print: False (blocked by full URL match)\n",
    "print(url_filter(url_to_test, block_domains, block_urls, banned, banned_subwords, soft_banned))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unigram\n",
    "ã‚¯ãƒ©ã‚¹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ–‡å­—åˆ—ã§ã€ã“ã®ãƒ•ã‚£ãƒ«ã‚¿ãŒè‹±èªžã®å˜èªžé »åº¦ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦å˜èªžã®ãƒ­ã‚°ç¢ºçŽ‡ã®å¹³å‡ã‚’è¨ˆç®—ã—ã€ãã®å¹³å‡ãŒé–¾å€¤ã‚ˆã‚Šã‚‚é«˜ã„ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã“ã¨ã‚’èª¬æ˜Žã—ã¦ã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "from huggingface_hub import cached_assets_path\n",
    "from loguru import logger\n",
    "\n",
    "from datatrove.data import Document\n",
    "from datatrove.pipeline.filters.base_filter import BaseFilter\n",
    "from datatrove.pipeline.writers.disk_base import DiskWriter\n",
    "\n",
    "\n",
    "UNIGRAM_DOWNLOAD = \"https://ai2-s2-research-public.s3-us-west-2.amazonaws.com/lucas/google-1T-unigram/unigram_freq.csv\"\n",
    "\n",
    "\n",
    "class UnigramLogProbFilter(BaseFilter):\n",
    "    \"\"\"\n",
    "    Computes average unigram log probability based on word frequencies from\n",
    "    https://www.kaggle.com/datasets/rtatman/english-word-frequency\n",
    "\n",
    "    Idea taken from https://huggingface.co/datasets/allenai/peS2o\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"ðŸ§‘â€ðŸ³ Unigram log-prob filter\"\n",
    "    _requires_dependencies = [\"nltk\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        logprobs_threshold: float = -10,\n",
    "        exclusion_writer: DiskWriter = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            logprobs_threshold: the minimum average unigram logprobs needed to keep a document\n",
    "            exclusion_writer:\n",
    "        \"\"\"\n",
    "        super().__init__(exclusion_writer)\n",
    "        self.logprobs_threshold = logprobs_threshold\n",
    "        self.unigram_frequencies = self.get_frequencies()\n",
    "\n",
    "    def get_frequencies(self):\n",
    "        download_dir = cached_assets_path(\n",
    "            library_name=\"datatrove\", namespace=\"filters\", subfolder=\"unigram_logprob_filter\"\n",
    "        )\n",
    "        unigram_freq_file = os.path.join(download_dir, \"unigram_freq.csv\")\n",
    "        if not os.path.isfile(unigram_freq_file):\n",
    "            logger.info(\"â¬‡ï¸ Downloading unigram-frequencies ...\")\n",
    "            urllib.request.urlretrieve(UNIGRAM_DOWNLOAD, unigram_freq_file)\n",
    "\n",
    "        words = []\n",
    "        counts = []\n",
    "        with open(unigram_freq_file, encoding=\"utf-8\", newline=\"\") as f:\n",
    "            csv_reader = csv.DictReader(f)\n",
    "            for row in csv_reader:\n",
    "                words.append(row[\"word\"])\n",
    "                counts.append(int(row[\"count\"]))\n",
    "        total_count = sum(counts)\n",
    "        return {word: count / total_count for word, count in zip(words, counts)}\n",
    "\n",
    "    def get_logprob(self, doc):\n",
    "        from nltk.tokenize import word_tokenize\n",
    "\n",
    "        words = word_tokenize(doc.text)\n",
    "        freqs = [self.unigram_frequencies.get(word.lower(), 1e-9) for word in words]\n",
    "\n",
    "        if len(freqs) == 0:\n",
    "            return 0\n",
    "        return sum([np.log(f) for f in freqs]) / len(freqs)\n",
    "\n",
    "    def filter(self, doc: Document) -> bool:\n",
    "        \"\"\"\n",
    "            Checks if the average unigram probability is above the threshold. This assumes the text is in english.\n",
    "        Args:\n",
    "            doc:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        return self.get_logprob(doc) > self.logprobs_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¿®æ­£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Constants and helper functions\n",
    "UNIGRAM_DOWNLOAD_URL = \"https://ai2-s2-research-public.s3-us-west-2.amazonaws.com/lucas/google-1T-unigram/unigram_freq.csv\"\n",
    "UNIGRAM_FREQ_FILE = \"unigram_freq.csv\"\n",
    "\n",
    "def download_unigram_frequencies(file_path: str):\n",
    "    \"\"\" Download the unigram frequencies CSV if it's not already present. \"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(\"Downloading unigram frequencies...\")\n",
    "        urlretrieve(UNIGRAM_DOWNLOAD_URL, file_path)\n",
    "\n",
    "def load_unigram_frequencies(file_path: str) -> dict:\n",
    "    \"\"\" Load unigram frequencies from a CSV file into a dictionary. \"\"\"\n",
    "    words, counts = [], []\n",
    "    with open(file_path, mode='r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            words.append(row['word'])\n",
    "            counts.append(int(row['count']))\n",
    "    total_count = sum(counts)\n",
    "    return {word: count / total_count for word, count in zip(words, counts)}\n",
    "\n",
    "def unigram_filter(text: str, unigram_freqs: dict, logprobs_threshold: float = -10) -> bool:\n",
    "    \"\"\" \n",
    "    Filter text based on the average unigram log probability.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to filter.\n",
    "        unigram_freqs (dict): Dictionary of unigram frequencies.\n",
    "        logprobs_threshold (float): Threshold for the average unigram log probability to keep the text.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the text meets the log probability threshold, False otherwise.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text.lower())\n",
    "    if not words:\n",
    "        return False  # Empty text should probably be filtered out\n",
    "    log_probs = [np.log(unigram_freqs.get(word, 1e-9)) for word in words]\n",
    "    average_log_prob = sum(log_probs) / len(words)\n",
    "    return average_log_prob > logprobs_threshold\n",
    "\n",
    "# Example of how to use this function\n",
    "def main():\n",
    "    # Path to save or load the unigram frequency file\n",
    "    unigram_freq_file_path = UNIGRAM_FREQ_FILE\n",
    "    download_unigram_frequencies(unigram_freq_file_path)\n",
    "    unigram_freqs = load_unigram_frequencies(unigram_freq_file_path)\n",
    "    \n",
    "    # Example text\n",
    "    example_text = \"This is a simple test text with common words and some uncommon ones.\"\n",
    "\n",
    "    # Apply the filter\n",
    "    result = unigram_filter(example_text, unigram_freqs)\n",
    "    print(\"Text is kept:\", result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# language filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatrove.data import Document\n",
    "from datatrove.io import cached_asset_path_or_download\n",
    "from datatrove.pipeline.filters.base_filter import BaseFilter\n",
    "from datatrove.pipeline.writers.disk_base import DiskWriter\n",
    "from datatrove.utils.typeshelper import Languages\n",
    "\n",
    "\n",
    "LANGUAGE_ID_MODEL_URL = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n",
    "\n",
    "\n",
    "class LanguageFilter(BaseFilter):\n",
    "    name = \"ðŸŒ Language ID\"\n",
    "    _requires_dependencies = [(\"fasttext\", \"fasttext-wheel\"), \"fasteners\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        languages: tuple = (Languages.english,),\n",
    "        language_threshold: float = 0.65,\n",
    "        exclusion_writer: DiskWriter = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        filters if the predicted language is not among given language or if the language score is below language\n",
    "        language_threshold\n",
    "\n",
    "        Args:\n",
    "            languages: list of languages to keep\n",
    "            language_threshold: language_threshold minimum score to accept a document\n",
    "            exclusion_writer:\n",
    "        \"\"\"\n",
    "        super().__init__(exclusion_writer)\n",
    "        self.language_threshold = language_threshold\n",
    "        self.languages = languages\n",
    "        self._model = None\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        if not self._model:\n",
    "            from fasttext.FastText import _FastText\n",
    "\n",
    "            model_file = cached_asset_path_or_download(\n",
    "                LANGUAGE_ID_MODEL_URL,\n",
    "                namespace=\"filters\",\n",
    "                subfolder=\"language_filter\",\n",
    "                desc=\"fast-text language identifier model\",\n",
    "            )\n",
    "            self._model = _FastText(model_file)\n",
    "        return self._model\n",
    "\n",
    "    def filter(self, doc: Document) -> bool:\n",
    "        \"\"\"Args:\n",
    "            doc: document\n",
    "\n",
    "        Returns:\n",
    "            is_filter\n",
    "        \"\"\"\n",
    "\n",
    "        language, score = self.model.predict(doc.text.replace(\"\\n\", \"\"))\n",
    "        # language label is given in the form __label__<language_id>\n",
    "        language = language[0].split(\"__\")[2]\n",
    "        doc.metadata[\"language\"] = language\n",
    "        doc.metadata[\"language_score\"] = score[0]\n",
    "        return score > self.language_threshold and language in self.languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¿®æ­£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# Constants and helper functions\n",
    "LANGUAGE_ID_MODEL_URL = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n",
    "MODEL_PATH = \"lid.176.bin\"  # Assuming the model is downloaded manually or by a setup script\n",
    "\n",
    "def download_language_model(url: str, file_path: str):\n",
    "    \"\"\" Download the language identification model if it's not already present. \"\"\"\n",
    "    try:\n",
    "        fasttext.load_model(file_path)\n",
    "    except ValueError:\n",
    "        print(\"Model not found locally. Downloading from URL...\")\n",
    "        urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "def language_filter(text: str, languages: tuple = ('english',), language_threshold: float = 0.65) -> bool:\n",
    "    \"\"\"\n",
    "    Filters text based on language identification. Only texts identified as specified languages with a confidence\n",
    "    above a specified threshold are kept.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to filter.\n",
    "        languages (tuple): Tuple of language codes to keep.\n",
    "        language_threshold (float): Minimum confidence score to accept a text.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the text is in the specified languages with sufficient confidence, False otherwise.\n",
    "    \"\"\"\n",
    "    # Ensure model is available and load it\n",
    "    try:\n",
    "        model = fasttext.load_model(MODEL_PATH)\n",
    "    except ValueError:\n",
    "        download_language_model(LANGUAGE_ID_MODEL_URL, MODEL_PATH)\n",
    "        model = fasttext.load_model(MODEL_PATH)\n",
    "\n",
    "    # Identify language of the text\n",
    "    labels, probabilities = model.predict(text.replace(\"\\n\", \"\"), k=1)  # Only get the top prediction\n",
    "    language = labels[0].replace(\"__label__\", \"\")\n",
    "    score = probabilities[0]\n",
    "\n",
    "    # Check if the language and score meet the required conditions\n",
    "    return (language in languages) and (score >= language_threshold)\n",
    "\n",
    "# Example usage\n",
    "example_text = \"This is an example text that should be detected as English.\"\n",
    "result = language_filter(example_text)\n",
    "print(\"Text passed filter:\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
