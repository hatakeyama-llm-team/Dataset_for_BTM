{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#きれいなテキストを分配する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cleaner.auto_cleaner import clean_text\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from src.classify.Text2Vec import Text2Vec, texts2classes\n",
    "from src.cleaner.auto_cleaner import clean_text, ml_clean_text\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "import joblib\n",
    "from src.distribute_jsonl import process_lines,make_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = True\n",
    "base_dir = \"../data/categorized\"\n",
    "length_threshold = 30  # 短い記事は捨てる\n",
    "check_length = 200  # はじめのlengthだけで分類する\n",
    "\n",
    "\n",
    "# load models\n",
    "t2v = Text2Vec(load_facebook_model('../data/model/cc.ja.300.bin'))\n",
    "kmeans = joblib.load(\"../data/model/kmeans.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def proc(docs,database_path):\n",
    "    process_lines(docs, t2v, kmeans, \n",
    "                  base_dir, \n",
    "                  database_path,\n",
    "                  check_length=check_length)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/textprocess/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/hatakeyama/miniconda3/envs/textprocess/lib/python3.12/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Filter: 100%|██████████| 1395760/1395760 [00:35<00:00, 39640.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../20integrate_texts/\")\n",
    "from loaders.loaders import *\n",
    "length_threshold = 100\n",
    "batch_size=100\n",
    "\n",
    "loader_dict={\n",
    "    \"NHK_School\":NHKSchool_loader(),\n",
    "    \"WikiQA\":wiki_qa_loader(),\n",
    "    \"Wiki\":cleaned_wiki_loader(),\n",
    "    \"NHK_News\":nhk_news_loader(),\n",
    "    \"aozora\":aozora_bunko_loader(),\n",
    "    \"j_ronbun\":j_research_loader(),\n",
    "    #\"cosmo\":cosmo_loader(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n",
      "Processing 100 documents...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for doc_name,loader in loader_dict.items():\n",
    "    docs = []\n",
    "    lines = []\n",
    "    for record in iter(loader):\n",
    "        lines.append(record[\"text\"])\n",
    "\n",
    "    cnt=0\n",
    "    for text in lines:\n",
    "        text = clean_text(text)\n",
    "        if len(text) < length_threshold:\n",
    "            continue\n",
    "\n",
    "        docs.append(text)\n",
    "        if len(docs) == batch_size:\n",
    "            # docsのコピーを作成してprocに渡す\n",
    "            proc(docs[:],doc_name+str(cnt))\n",
    "            # docsをリセット\n",
    "            docs = []\n",
    "            #cnt+=1\n",
    "\n",
    "    proc(docs[:],doc_name+str(cnt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textprocess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
