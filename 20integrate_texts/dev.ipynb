{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/textprocess/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from loaders.loaders import *\n",
    "#from dataset_dict import dataset_dict, output_path\n",
    "#from loaders.RecordDistributor import RecordDistributor\n",
    "#from loaders.CleanedEnglishDataset import WikiBookEn\n",
    "from loaders.OtherDatasets import *\n",
    "#from loaders.PilePythonDataset import PilePythonDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading many datasets. This may take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/textprocess/lib/python3.12/site-packages/datasets/load.py:1461: FutureWarning: The repository for allenai/peS2o contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/allenai/peS2o\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/hatakeyama/miniconda3/envs/textprocess/lib/python3.12/site-packages/datasets/load.py:1461: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python0000.jsonl.zst already exists.\n",
      "python0001.jsonl.zst already exists.\n",
      "python0002.jsonl.zst already exists.\n",
      "python0003.jsonl.zst already exists.\n",
      "python0004.jsonl.zst already exists.\n",
      "python0005.jsonl.zst already exists.\n",
      "python0006.jsonl.zst already exists.\n",
      "python0007.jsonl.zst already exists.\n",
      "python0008.jsonl.zst already exists.\n",
      "python0009.jsonl.zst already exists.\n",
      "python0010.jsonl.zst already exists.\n",
      "python0011.jsonl.zst already exists.\n",
      "python0012.jsonl.zst already exists.\n",
      "python0013.jsonl.zst already exists.\n",
      "python0014.jsonl.zst already exists.\n",
      "python0015.jsonl.zst already exists.\n",
      "python0016.jsonl.zst already exists.\n",
      "python0017.jsonl.zst already exists.\n",
      "python0018.jsonl.zst already exists.\n",
      "python0019.jsonl.zst already exists.\n",
      "python0020.jsonl.zst already exists.\n",
      "python0021.jsonl.zst already exists.\n",
      "python0022.jsonl.zst already exists.\n",
      "python0023.jsonl.zst already exists.\n",
      "python0024.jsonl.zst already exists.\n",
      "python0025.jsonl.zst already exists.\n",
      "python0026.jsonl.zst already exists.\n",
      "python0027.jsonl.zst already exists.\n",
      "python0028.jsonl.zst already exists.\n",
      "python0029.jsonl.zst already exists.\n",
      "python0030.jsonl.zst already exists.\n",
      "python0031.jsonl.zst already exists.\n",
      "python0032.jsonl.zst already exists.\n",
      "python0033.jsonl.zst already exists.\n",
      "python0034.jsonl.zst already exists.\n",
      "python0035.jsonl.zst already exists.\n",
      "python0036.jsonl.zst already exists.\n",
      "python0037.jsonl.zst already exists.\n",
      "python0038.jsonl.zst already exists.\n",
      "python0039.jsonl.zst already exists.\n",
      "python0040.jsonl.zst already exists.\n",
      "python0041.jsonl.zst already exists.\n",
      "All downloads completed.\n",
      "try loading.. ../data/original_dump/python/python0000.jsonl.zst\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from loaders.RandomEnglishDataset import RandomEnglishDataset\n",
    "#ds= load_dataset(\"suolyer/pile_stackexchange\",streaming=True,split=\"validation\")\n",
    "#ds = load_dataset(\"Muennighoff/flan\",streaming=True,split=\"train\")\n",
    "ds=RandomEnglishDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader=iter(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:04<00:00, 1547.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "out_path=\"../data/eng/eng.jsonl\"\n",
    "out_path=\"/data/hatakeyama/python/eng_corpus/eng.jsonl\"\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(10**5)):\n",
    "    t=next(loader)[\"text\"]\n",
    "    if t==\"\":\n",
    "        continue\n",
    "\n",
    "    with open(out_path,\"a\") as f:\n",
    "        f.write(json.dumps({\"text\":t},ensure_ascii=False)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textprocess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
